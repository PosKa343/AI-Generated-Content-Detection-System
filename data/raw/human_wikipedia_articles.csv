text_id,content,label,source,title,url,published_at,query,collection_method
human_wiki_0000,"Nanotechnology is the manipulation of matter with at least one dimension sized from 1 to 100 nanometers (nm). At this scale, commonly known as the nanoscale, surface area and quantum mechanical effects become important in describing properties of matter. This definition of nanotechnology includes all types of research and technologies that deal with these special properties. It is common to see the plural form ""nanotechnologies"" as well as ""nanoscale technologies"" to refer to research and applications whose common trait is scale. An earlier understanding of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabricating macroscale products, now referred to as molecular nanotechnology. Nanotechnology defined by scale includes fields of science such as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage, engineering, microfabrication, and molecular engineering. The associated research and applications range from extensions of conventional device physics to molecular self-assembly, from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale. Nanotechnology may be able to create new materials and devices with diverse applications, such as in nanomedicine, nanoelectronics, agricultural sectors, biomaterials energy production, and consumer products. However, nanotechnology raises issues, including concerns about the toxicity and environmental impact of nanomaterials, and their potential effects on global economics, as well as various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.",0,Wikipedia,Nanotechnology,https://en.wikipedia.org/wiki/Nanotechnology,,Nanotechnology,wikipedia_api
human_wiki_0001,"Origins The concepts that seeded nanotechnology were first discussed in 1959 by physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms.",0,Wikipedia,Nanotechnology,https://en.wikipedia.org/wiki/Nanotechnology,,Nanotechnology,wikipedia_api
human_wiki_0002,"The term ""nano-technology"" was first used by Norio Taniguchi in 1974, though it was not widely known. Inspired by Feynman's concepts, K. Eric Drexler used the term ""nanotechnology"" in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which achieved popular success and helped thrust nanotechnology into the public sphere.  In it he proposed the idea of a nanoscale ""assembler"" that would be able to build a copy of itself and of other items of arbitrary complexity with atom-level control. Also in 1986, Drexler co-founded The Foresight Institute to increase public awareness and understanding of nanotechnology concepts and implications. The emergence of nanotechnology as a field in the 1980s occurred through the convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework, and experimental advances that drew additional attention to the prospects. In the 1980s, two breakthroughs helped to spark the growth of nanotechnology. First, the invention of the scanning tunneling microscope in 1981 enabled visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986. Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.",0,Wikipedia,Nanotechnology,https://en.wikipedia.org/wiki/Nanotechnology,,Nanotechnology,wikipedia_api
human_wiki_0003,"Second, fullerenes (buckyballs) were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry. C60 was not initially described as nanotechnology; the term was used regarding subsequent work with related carbon nanotubes (sometimes called graphene tubes or Bucky tubes) which suggested potential applications for nanoscale electronics and devices. The discovery of carbon nanotubes is attributed to Sumio Iijima of NEC in 1991, for which Iijima won the inaugural 2008 Kavli Prize in Nanoscience. In the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology. Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003. Meanwhile, commercial products based on advancements in nanoscale technologies began emerging. These products were limited to bulk applications of nanomaterials and did not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles. Governments moved to promote and fund research into nanotechnology, such as American the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established research funding, and in Europe via the European Framework Programmes for Research and Technological Development. By the mid-2000s scientific attention began to flourish. Nanotechnology roadmaps centered on atomically precise manipulation of matter and discussed existing and projected capabilities, goals, and applications.",0,Wikipedia,Nanotechnology,https://en.wikipedia.org/wiki/Nanotechnology,,Nanotechnology,wikipedia_api
human_wiki_0004,"Fundamental concepts Nanotechnology is the science and engineering of functional systems at the molecular scale. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up making complete, high-performance products. One nanometer (nm) is one billionth, or 10−9, of a meter. By comparison, typical carbon–carbon bond lengths, or the spacing between these atoms in a molecule, are in the range 0.12–0.15 nm, and DNA's diameter is around 2 nm. On the other hand, the smallest cellular life forms, the bacteria of the genus Mycoplasma, are around 200 nm in length. By convention, nanotechnology is taken as the scale range 1 to 100 nm, following the definition used by the American National Nanotechnology Initiative. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which have an approximately ,25 nm kinetic diameter). The upper limit is more or less arbitrary, but is around the size below which phenomena not observed in larger structures start to become apparent and can be made use of. These phenomena make nanotechnology distinct from devices that are merely miniaturized versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology. To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth. Two main approaches are used in nanotechnology. In the ""bottom-up"" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition. In the ""top-down"" approach, nano-objects are constructed from larger entities without atomic-level control. Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved to provide nanotechnology's scientific foundation.",0,Wikipedia,Nanotechnology,https://en.wikipedia.org/wiki/Nanotechnology,,Nanotechnology,wikipedia_api
human_wiki_0005,"The theory of relativity usually encompasses two interrelated physics theories by Albert Einstein: special relativity and general relativity, proposed and published in 1905 and 1915, respectively. Special relativity applies to all physical phenomena in the absence of gravity. General relativity explains the law of gravitation and its relation to the forces of nature. It applies to the cosmological and astrophysical realm, including astronomy. The theory transformed theoretical physics and astronomy during the 20th century, superseding a 200-year-old theory of mechanics created primarily by Isaac Newton. It introduced concepts including 4-dimensional spacetime as a unified entity of space and time, relativity of simultaneity, kinematic and gravitational time dilation, and length contraction. In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology and astrophysics predicted extraordinary astronomical phenomena such as neutron stars, black holes, and gravitational waves.",0,Wikipedia,Theory of relativity,https://en.wikipedia.org/wiki/Theory_of_relativity,,Theory_of_relativity,wikipedia_api
human_wiki_0006,"Development and acceptance Albert Einstein published the theory of special relativity in 1905, building on many theoretical results and empirical findings obtained by Albert A. Michelson, Hendrik Lorentz, Henri Poincaré and others. Max Planck, Hermann Minkowski and others did subsequent work. Einstein developed general relativity between 1907 and 1915, with contributions by many others after 1915. The final form of general relativity was published in 1916. The term ""theory of relativity"" was based on the expression ""relative theory"" (German: Relativtheorie) used in 1906 by Planck, who emphasized how the theory uses the principle of relativity. In the discussion section of the same paper, Alfred Bucherer used for the first time the expression ""theory of relativity"" (German: Relativitätstheorie). By the 1920s, the physics community understood and accepted special relativity. It rapidly became a significant and necessary tool for theorists and experimentalists in the new fields of atomic physics, nuclear physics, and quantum mechanics. By comparison, general relativity did not appear to be as useful, beyond making minor corrections to predictions of Newtonian gravitation theory. It seemed to offer little potential for experimental test, as most of its assertions were on an astronomical scale. Its mathematics seemed difficult and fully understandable only by a small number of people. Around 1960, general relativity became central to physics and astronomy. New mathematical techniques to apply to general relativity streamlined calculations and made its concepts more easily visualized. As astronomical phenomena were discovered, such as quasars (1963), the 3-kelvin microwave background radiation (1965), pulsars (1967), and the first black hole candidates (1981), the theory explained their attributes, and measurement of them further confirmed the theory.",0,Wikipedia,Theory of relativity,https://en.wikipedia.org/wiki/Theory_of_relativity,,Theory_of_relativity,wikipedia_api
human_wiki_0007,"Special relativity Special relativity is a theory of the structure of spacetime. It was introduced in Einstein's 1905 paper ""On the Electrodynamics of Moving Bodies"" (for the contributions of many other physicists and mathematicians, see History of special relativity). Special relativity is based on two postulates which are contradictory in classical mechanics:",0,Wikipedia,Theory of relativity,https://en.wikipedia.org/wiki/Theory_of_relativity,,Theory_of_relativity,wikipedia_api
human_wiki_0008,"The laws of physics are the same for all observers in any inertial frame of reference relative to one another (principle of relativity). The speed of light in vacuum is the same for all observers, regardless of their relative motion or of the motion of the light source. The resultant theory copes with experiment better than classical mechanics. For instance, postulate 2 explains the results of the Michelson–Morley experiment. Moreover, the theory has many surprising and counterintuitive consequences. Some of these are:",0,Wikipedia,Theory of relativity,https://en.wikipedia.org/wiki/Theory_of_relativity,,Theory_of_relativity,wikipedia_api
human_wiki_0009,"Relativity of simultaneity: Two events, simultaneous for one observer, may not be simultaneous for another observer if the observers are in relative motion. Time dilation: Moving clocks are measured to tick more slowly than an observer's ""stationary"" clock. Length contraction: Objects are measured to be shortened in the direction that they are moving with respect to the observer. Maximum speed is finite: No physical object, message or field line can travel faster than the speed of light in vacuum. The effect of gravity can only travel through space at the speed of light, not faster or instantaneously. Mass–energy equivalence: E = mc2, energy and mass are equivalent and transmutable. Relativistic mass, idea used by some researchers. The defining feature of special relativity is the replacement of the Galilean transformations of classical mechanics by the Lorentz transformations. (See Maxwell's equations of electromagnetism.)",0,Wikipedia,Theory of relativity,https://en.wikipedia.org/wiki/Theory_of_relativity,,Theory_of_relativity,wikipedia_api
human_wiki_0010,"Sociology is the scientific study of human society that focuses on society, human social behavior, patterns of social relationships, social interaction, and aspects of culture associated with everyday life. The term sociology was coined in the late 18th century to describe the scientific study of society. Regarded as a part of both the social sciences and humanities, sociology uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order and social change. Sociological subject matter ranges from micro-level analyses of individual interaction and agency to macro-level analyses of social systems and social structure. Applied sociological research may be applied directly to social policy and welfare, whereas theoretical approaches may focus on the understanding of social processes and phenomenological method. Traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. Recent studies have added socio-technical aspects of the digital divide as a new focus.  Digital sociology examines the impact of digital technologies on social behavior and institutions, encompassing professional, analytical, critical, and public dimensions. The internet has reshaped social networks and power relations, illustrating the growing importance of digital sociology. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects and institutions, such as health and the institution of medicine; economy; military; punishment and systems of control; the Internet; sociology of education; social capital; and the role of social activity in the development of scientific knowledge. The range of social scientific methods has also expanded, as social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century, especially, have led to increasingly interpretative, hermeneutic, and philosophical approaches towards the analysis of society. Conversely, the turn of the 21st century has seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis. Social research has influence throughout various industries and sectors of life, such as among politicians, policy makers, and legislators; educators; planners; administrators; developers; business magnates and managers; social workers; non-governmental organizations; and non-profit organizations, as well as individuals interested in resolving social issues in general.",0,Wikipedia,Sociology,https://en.wikipedia.org/wiki/Sociology,,Sociology,wikipedia_api
human_wiki_0011,"History Sociological reasoning predates the foundation of the discipline itself. Social analysis has origins in the common stock of universal, global knowledge and philosophy, having been carried out as far back as the time of old comic poetry which features social and political criticism, and ancient Greek philosophers Socrates, Plato, and Aristotle. For instance, the origin of the survey can be traced back to at least the Domesday Book in 1086, while ancient philosophers such as Confucius wrote about the importance of social roles. Medieval Arabic writings encompass a rich tradition that unveils early insights into the field of sociology. Some sources consider Ibn Khaldun, a 14th-century Muslim scholar from Tunisia, to have been the father of sociology, although there is no reference to his work in the writings of European contributors to modern sociology. Khaldun's Muqaddimah was considered to be amongst the first works to advance social-scientific reasoning on social cohesion and social conflict.",0,Wikipedia,Sociology,https://en.wikipedia.org/wiki/Sociology,,Sociology,wikipedia_api
human_wiki_0012,"Etymology The word sociology derives part of its name from the Latin word socius ('companion' or 'fellowship'). The suffix -logy ('the study of') comes from that of the Greek -λογία, derived from λόγος (lógos, 'word' or 'knowledge'). The term sociology was first coined in 1780 by the French essayist Emmanuel-Joseph Sieyès in an unpublished manuscript. Sociology was later defined independently by French philosopher of science Auguste Comte (1798–1857) in 1838 as a new way of looking at society. Comte had earlier used the term social physics, but it had been subsequently appropriated by others, most notably the Belgian statistician Adolphe Quetelet. Comte endeavored to unify history, psychology, and economics through the scientific understanding of social life. Writing shortly after the malaise of the French Revolution, he proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in the Course in Positive Philosophy (1830–1842), later included in A General View of Positivism (1848). Comte believed a positivist stage would mark the final era in the progression of human understanding, after conjectural theological and metaphysical phases. In observing the circular dependence of theory and observation in science, and having classified the sciences, Comte may be regarded as the first philosopher of science in the modern sense of the term.",0,Wikipedia,Sociology,https://en.wikipedia.org/wiki/Sociology,,Sociology,wikipedia_api
human_wiki_0013,"Comte gave a powerful impetus to the development of sociology, an impetus that bore fruit in the later decades of the nineteenth century. To say this is certainly not to claim that French sociologists such as Durkheim were devoted disciples of the high priest of positivism. But by insisting on the irreducibility of each of his basic sciences to the particular science of sciences which it presupposed in the hierarchy and by emphasizing the nature of sociology as the scientific study of social phenomena Comte put sociology on the map. To be sure, [its] beginnings can be traced back well beyond Montesquieu, for example, and to Condorcet, not to speak of Saint-Simon, Comte's immediate predecessor. But Comte's clear recognition of sociology as a particular science, with a character of its own, justified Durkheim in regarding him as the father or founder of this science, even though Durkheim did not accept the idea of the three states and criticized Comte's approach to sociology.",0,Wikipedia,Sociology,https://en.wikipedia.org/wiki/Sociology,,Sociology,wikipedia_api
human_wiki_0014,"Marx Both Comte and Karl Marx set out to develop scientifically justified systems in the wake of European industrialization and secularization, informed by various key movements in the philosophies of history and science. Marx rejected Comtean positivism but in attempting to develop a ""science of society"" nevertheless came to be recognized as a founder of sociology as the word gained wider meaning. For Isaiah Berlin, even though Marx did not consider himself to be a sociologist, he may be regarded as the ""true father"" of modern sociology, ""in so far as anyone can claim the title.""To have given clear and unified answers in familiar empirical terms to those theoretical questions which most occupied men's minds at the time, and to have deduced from them clear practical directives without creating obviously artificial links between the two, was the principal achievement of Marx's theory. The sociological treatment of historical and moral problems, which Comte and after him, Spencer and Taine, had discussed and mapped, became a precise and concrete study only when the attack of militant Marxism made its conclusions a burning issue, and so made the search for evidence more zealous and the attention to method more intense.",0,Wikipedia,Sociology,https://en.wikipedia.org/wiki/Sociology,,Sociology,wikipedia_api
human_wiki_0015,"Natural environment, refers to  respectively to all living things and non-living things occurring naturally and the physical and biological factors along with their chemical interactions that impact  on any organism or a group of organisms",0,Wikipedia,Environment,https://en.wikipedia.org/wiki/Environment,,Environment,wikipedia_api
human_wiki_0016,"Other physical and cultural environments Ecology, the study of the relations of organisms to one another and to their physical surroundings Environment (systems), the surroundings of a physical system that may interact with the system by exchanging mass, energy, or other properties. Built environment, constructed surroundings that provide the settings for human activity, ranging from the large-scale civic surroundings to the personal places Social environment, the culture that an individual lives in, and the people and institutions with whom they interact Market environment, business term",0,Wikipedia,Environment,https://en.wikipedia.org/wiki/Environment,,Environment,wikipedia_api
human_wiki_0017,"Arts, entertainment and publishing Environment (magazine), a peer-reviewed, popular environmental science publication founded in 1958 Environment (1917 film), 1917 American silent film Environment (1922 film), 1922 American silent film Environment (1927 film), 1927 Australian silent film environments (album series), a series of LPs, cassettes and CDs depicting natural sounds Environments (album), a 2007 album by The Future Sound of London ""Environment"", a song by Dave from Psychodrama Environments (journal), a scientific journal",0,Wikipedia,Environment,https://en.wikipedia.org/wiki/Environment,,Environment,wikipedia_api
human_wiki_0018,"In computing Environment (type theory), the association between variable names and data types in type theory Deployment environment, in software deployment, a computer system in which a computer program or software component is deployed and executed Runtime environment, a virtual machine state which provides software services for processes or programs while a computer is running Environment variable, a variable capable of affecting the way processes behave on a computer",0,Wikipedia,Environment,https://en.wikipedia.org/wiki/Environment,,Environment,wikipedia_api
human_wiki_0019,"See also Environmentalism, a broad philosophy, ideology, and social movement regarding concerns for environmental protection Environmental disease Environmental health Environmental science Environmental history of the United States Environmental Issues are disruptions in the usual function of ecosystems.",0,Wikipedia,Environment,https://en.wikipedia.org/wiki/Environment,,Environment,wikipedia_api
human_wiki_0020,"Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in a defined population, and application of this knowledge to prevent diseases. It is a cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences. Major areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, environmental epidemiology, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment. Epidemiology, literally meaning ""the study of what is upon the people"", is derived from Greek  epi 'upon, among'  demos 'people, district' and  logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term ""epizoology"" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology). The distinction between ""epidemic"" and ""endemic"" was first drawn by Hippocrates, The term ""epidemiology"" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Joaquín de Villalba in Epidemiología Española. Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic. The term epidemiology is now widely applied to cover the description and causation of not only epidemic, infectious disease, but of disease in general, including related conditions and, especially since the 20th century, chronic diseases such as diabetes, cardiovascular disease, and cancer. Some examples of topics examined through epidemiology include as high blood pressure, mental illness and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.",0,Wikipedia,Epidemiology,https://en.wikipedia.org/wiki/Epidemiology,,Epidemiology,wikipedia_api
human_wiki_0021,"History The Greek physician Hippocrates, taught by Democritus, was known as the father of medicine, sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences. Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (black bile, yellow bile, blood, and phlegm). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine. He coined the terms endemic (for diseases usually found in some places but not in others) and epidemic (for diseases that are seen at some times but not others).",0,Wikipedia,Epidemiology,https://en.wikipedia.org/wiki/Epidemiology,,Epidemiology,wikipedia_api
human_wiki_0022,"Modern era In the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that the very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen's miasma theory (poison gas in sick people). In 1543 he wrote a book De contagione et contagiosis morbis, in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Antonie van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease. During the Ming dynasty, Wu Youke (1582–1652) developed the idea that some diseases were caused by transmissible agents, which he called Li Qi (戾气 or pestilential factors) when he observed various epidemics rage around him between 1641 and 1644. His book Wen Yi Lun (瘟疫论, Treatise on Pestilence/Treatise of Epidemic Diseases) can be regarded as the main etiological work that brought forward the concept. His concepts were still being considered in analysing SARS outbreak by WHO in 2004 in the context of traditional Chinese medicine. Another pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated. John Graunt, a haberdasher and amateur statistician, published Natural and Political Observations ... upon the Bills of Mortality in 1662. In it, he analysed the mortality rolls in London before the Great Plague, presented one of the first life tables, and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.",0,Wikipedia,Epidemiology,https://en.wikipedia.org/wiki/Epidemiology,,Epidemiology,wikipedia_api
human_wiki_0023,"John Snow is famous for his investigations into the causes of the 19th-century cholera epidemics, and is also known as the father of (modern) Epidemiology. He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world. However, Snow's research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death due to the prevailing Miasma Theory of the time, a model of disease in which poor air quality was blamed for illness. This was used to rationalize high rates of infection in impoverished areas instead of addressing the underlying issues of poor nutrition and sanitation, and was proven false by his work. Other pioneers include Danish physician Peter Anton Schleisner, who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland. Another important pioneer was Hungarian physician Ignaz Semmelweis, who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill-received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister, aided by his college, chemist Thomas Anderson, was able to ""discover"" antiseptics in 1865 based on the earlier work of Louis Pasteur. In the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross, Janet Lane-Claypon, Anderson Gray McKendrick, and others. In a parallel development during the 1920s, German-Swiss pathologist Max Askanazy and others founded the International Society for Geographical Pathology to systematically investigate the geographical pathology of cancer and other non-infectious diseases across populations in different regions. After World War II, Richard Doll and other non-pathologists joined the field and advanced methods to study cancer, a disease with patterns and mode of occurrences that could not be suitably studied with the methods developed for epidemics of infectious diseases. Geography pathology eventually combined with infectious disease epidemiology to make the field that is epidemiology today. Another breakthrough was the 1954 publication of the results of a British Doctors Study, led by Richard Doll and Austin Bradford Hill, which lent very strong statistical support to the link between tobacco smoking and lung cancer. In the late 20th century, with the advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level and disease was broadly named ""molecular epidemiology"". Specifically, ""genetic epidemiology"" has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes.",0,Wikipedia,Epidemiology,https://en.wikipedia.org/wiki/Epidemiology,,Epidemiology,wikipedia_api
human_wiki_0024,"21st century Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions. While most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease progression represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual (""the unique disease principle""), considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges, including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs. Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine, ""molecular pathology"" and ""epidemiology"" was integrated to create a new interdisciplinary field of ""molecular pathological epidemiology"" (MPE), defined as ""epidemiology of molecular pathology and heterogeneity of disease"". In MPE, investigators analyze the relationships between (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases. The concept and paradigm of MPE have become widespread in the 2010s. By 2012, it was recognized that many pathogens' evolution is rapid enough to be highly relevant to epidemiology, and that therefore much could be gained from an interdisciplinary approach to infectious disease integrating epidemiology and molecular evolution to ""inform control strategies, or even patient treatment."" Modern epidemiological studies can use advanced statistics and machine learning to create predictive models as well as to define treatment effects. There is increasing recognition that a wide range of modern data sources, many not originating from healthcare or epidemiology, can be used for epidemiological study. Such digital epidemiology can include data from internet searching, mobile phone records and retail sales of drugs.",0,Wikipedia,Epidemiology,https://en.wikipedia.org/wiki/Epidemiology,,Epidemiology,wikipedia_api
human_wiki_0025,"Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs. The terms programmer and coder overlap software engineer, but they imply only the construction aspect of a typical software engineer workload. A software engineer applies a software development process, which involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.",0,Wikipedia,Software engineering,https://en.wikipedia.org/wiki/Software_engineering,,Software_engineering,wikipedia_api
human_wiki_0026,"History Beginning in the 1960s, software engineering was recognized as a separate field of engineering. The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968, NATO organized the first conference on software engineering, which addressed emerging challenges in software development. The event played a key role in formalizing guidelines and best practices for creating reliable and maintainable software. The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of ""Computers and Automation"" and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in ""President's Letter to the ACM Membership"" by Anthony A. Oettinger. It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer. Margaret Hamilton described the discipline of ""software engineering"" during the Apollo missions to give what they were doing legitimacy. At the time, there was perceived to be a ""software crisis"". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of ""Software Engineering"" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton. In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team. Modern, generally accepted best practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of the major computing disciplines. In modern systems, where concepts such as Edge Computing, Internet of Things and Cyber-physical Systems are prevalent, software is a critical factor. Thus, software engineering is closely related to the Systems Engineering discipline. The Systems Engineering Body of Knowledge claims:",0,Wikipedia,Software engineering,https://en.wikipedia.org/wiki/Software_engineering,,Software_engineering,wikipedia_api
human_wiki_0027,Software is prominent in most modern systems architectures and is often the primary means for integrating complex system components. Software engineering and systems engineering are not merely related disciplines; they are intimately intertwined....Good systems engineering is a key factor in enabling good software engineering.,0,Wikipedia,Software engineering,https://en.wikipedia.org/wiki/Software_engineering,,Software_engineering,wikipedia_api
human_wiki_0028,"""The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software.""—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary ""The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software.""—IEEE Standard Glossary of Software Engineering Terminology ""An engineering discipline concerned with all aspects of software production."" — Ian Sommerville",0,Wikipedia,Software engineering,https://en.wikipedia.org/wiki/Software_engineering,,Software_engineering,wikipedia_api
human_wiki_0029,"""The establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines.""—Fritz Bauer ""A branch of computer science that deals with the design, implementation, and maintenance of complex computer programs.""—Merriam-Webster ""'Software engineering' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as 'programming integrated over time.'""—Software Engineering at Google The term has also been used less formally:",0,Wikipedia,Software engineering,https://en.wikipedia.org/wiki/Software_engineering,,Software_engineering,wikipedia_api
human_wiki_0030,"Language is a structured system of communication that consists of grammar and vocabulary. It is the primary means by which humans convey meaning, both in spoken and signed forms, and may also be conveyed through writing. Human language is characterized by its cultural and historical diversity, with significant variations observed between cultures and across time. Human languages possess the properties of productivity and displacement, which enable the creation of an infinite number of sentences, and the ability to refer to objects, events, and ideas that are not immediately present in the discourse. The use of human language relies on social convention and is acquired through learning. Estimates of the number of human languages in the world vary between 5,000 and 7,000. Precise estimates depend on an arbitrary distinction (dichotomy) established between languages and dialects. Natural languages are spoken, signed, or both; however, any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, writing, whistling, signing, or braille. In other words, human language is modality-independent, but written or signed language is the way to inscribe or encode the natural human speech or gestures. Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, ""language"" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances. The scientific study of language is called linguistics. Critical examinations of languages, such as philosophy of language, the relationships between language and thought, how words represent experience, etc., have been debated at least since Gorgias and Plato in ancient Greek civilization. Thinkers such as Jean-Jacques Rousseau (1712–1778) have argued that language originated from emotions, while others like Immanuel Kant (1724–1804) have argued that languages originated from rational and logical thought. Twentieth century philosophers such as Ludwig Wittgenstein (1889–1951) argued that philosophy is really the study of language itself. Major figures in contemporary linguistics include Ferdinand de Saussure and Noam Chomsky. Language is thought to have gradually diverged from earlier primate communication systems when early hominins acquired the ability to form a theory of mind and shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. Language and culture are codependent. Therefore, in addition to its strictly communicative uses, language has social uses such as signifying group identity, social stratification, as well as use for social grooming and entertainment. Languages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family; in contrast, a language that has been demonstrated not to have any living or non-living relationship with another language is called a language isolate. There are also many unclassified languages whose relationships have not been established, and spurious languages may have not existed at all. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.",0,Wikipedia,Language,https://en.wikipedia.org/wiki/Language,,Language,wikipedia_api
human_wiki_0031,"Definitions The English word language derives ultimately from Proto-Indo-European *dn̥ǵʰwéh₂s ""tongue, speech, language"" through Latin lingua, ""language; tongue"", and Old French language. The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics. As an object of linguistic study, ""language"" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. ""French"". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word langage for language as a concept, langue as a specific instance of a language system, and parole for the concrete use of speech in a particular language. When speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language. During the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Johann Gottfried Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and René Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world – asking whether language simply reflects the objective structure of the world, or whether it creates concepts that in turn impose structure on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.",0,Wikipedia,Language,https://en.wikipedia.org/wiki/Language,,Language,wikipedia_api
human_wiki_0032,"Mental faculty, organ or instinct One definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of universal grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.",0,Wikipedia,Language,https://en.wikipedia.org/wiki/Language,,Language,wikipedia_api
human_wiki_0033,"Formal symbolic system Another definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language. Some proponents of Saussure's view of language have advocated a formal approach that studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used in formal logic, in formal linguistics, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.",0,Wikipedia,Language,https://en.wikipedia.org/wiki/Language,,Language,wikipedia_api
human_wiki_0034,"Tool for communication Yet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was ""tailored"" to serve the communicative needs of its users. This view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J. L. Austin, Paul Grice, John Searle, and W.O. Quine.",0,Wikipedia,Language,https://en.wikipedia.org/wiki/Language,,Language,wikipedia_api
human_wiki_0035,"In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term ""database"" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data have become widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.",0,Wikipedia,Database,https://en.wikipedia.org/wiki/Database,,Database,wikipedia_api
human_wiki_0036,"Terminology and overview Formally, a ""database"" refers to a set of related data accessed through the use of a ""database management system"" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term ""database"" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system. Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:",0,Wikipedia,Database,https://en.wikipedia.org/wiki/Database,,Database,wikipedia_api
human_wiki_0037,"Data definition – Creation, modification and removal of definitions that detail how the data is to be organized. Update – Insertion, modification, and deletion of the data itself. Retrieval – Selecting data according to specified criteria (e.g., a query, a position in a hierarchy, or a position in relation to other data) and providing that data either directly to the user, or making it available for further processing by the database itself or by other applications. The retrieved data may be made available in a more or less direct form without modification, as it is stored in the database, or in a new form obtained by altering it or combining it with existing data from the database. Administration – Registering and monitoring users, enforcing data security, monitoring performance, maintaining data integrity, dealing with concurrency control, and recovering information that has been corrupted by some event such as an unexpected system failure. Both a database and its DBMS conform to the principles of a particular database model. ""Database system"" refers collectively to the database model, database management system, and database. Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions. Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans. Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.",0,Wikipedia,Database,https://en.wikipedia.org/wiki/Database,,Database,wikipedia_api
human_wiki_0038,"History The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational. The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another. The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018 they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS. The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models. Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term ""post-relational"" and also the development of hybrid object–relational databases. The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing ""next generation"" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.",0,Wikipedia,Database,https://en.wikipedia.org/wiki/Database,,Database,wikipedia_api
human_wiki_0039,"1960s, navigational DBMS The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term ""data-base"" in a specific technical sense. As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market. The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods:",0,Wikipedia,Database,https://en.wikipedia.org/wiki/Database,,Database,wikipedia_api
human_wiki_0040,"Astronomy is a natural science that studies celestial objects and the phenomena that occur in the cosmos. It uses mathematics, physics, and chemistry to explain their origin and their overall evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, meteoroids, asteroids, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates beyond Earth's atmosphere. Cosmology is the branch of astronomy that studies the universe as a whole. Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Egyptians, Babylonians, Greeks, Indians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars. Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical astronomy seeks to explain observational results and observations are used to confirm theoretical results. Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets.",0,Wikipedia,Astronomy,https://en.wikipedia.org/wiki/Astronomy,,Astronomy,wikipedia_api
human_wiki_0041,"Etymology Astronomy (from the Greek ἀστρονομία from ἄστρον astron, ""star"" and -νομία -nomia from νόμος nomos, ""law"" or ""rule"") means study of celestial objects. Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. The two fields share a common origin but became distinct, astronomy being supported by physics while astrology is not.",0,Wikipedia,Astronomy,https://en.wikipedia.org/wiki/Astronomy,,Astronomy,wikipedia_api
human_wiki_0042,"Use of terms ""astronomy"" and ""astrophysics"" ""Astronomy"" and ""astrophysics"" are broadly synonymous in modern usage. In dictionary definitions, ""astronomy"" is ""the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties"", while ""astrophysics"" is the branch of astronomy dealing with ""the behavior, physical properties, and dynamic processes of celestial objects and phenomena"". Sometimes, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, ""astronomy"" means the qualitative study of the subject, whereas ""astrophysics"" is the physics-oriented version of the subject. Some fields, such as astrometry, are in this sense purely astronomy rather than also astrophysics. Research departments may use ""astronomy"" and ""astrophysics"" according to whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Thus, in modern use, the two terms are often used interchangeably.",0,Wikipedia,Astronomy,https://en.wikipedia.org/wiki/Astronomy,,Astronomy,wikipedia_api
human_wiki_0043,"History Pre-historic The initial development of astronomy was driven by practical needs like agricultural calendars. Before recorded history archeological sites such as Stonehenge provide evidence of ancient interest in astronomical observations.  Evidence also comes from artefacts such as the Nebra sky disc which serves as an astronomical calendar, defining a year as twelve lunar months, 354 days, with intercalary months to make up the solar year. The disc is inlaid with symbols interpreted as a sun, moon, and stars including a cluster of seven stars.",0,Wikipedia,Astronomy,https://en.wikipedia.org/wiki/Astronomy,,Astronomy,wikipedia_api
human_wiki_0044,"Classical Civilizations such as Egypt, Mesopotamia, Greece, India, China together – with cross-cultural influences – created astronomical observatories and developed ideas on the nature of the Universe, along with calendars and astronomical instruments. A key early development was the beginning of mathematical and scientific astronomy among the Babylonians, laying the foundations for astronomical traditions in other civilizations. The Babylonians discovered that lunar eclipses recurred in the saros cycle of 223 synodic months. Following the Babylonians, significant advances were made in ancient Greece and the Hellenistic world. Greek astronomy sought a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model. In the 2nd century BC, Hipparchus calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. He also observed the small drift in the positions of the equinoxes and solstices with respect to the fixed stars that we now know is caused by precession. Hipparchus also created a catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe. After the classical Greek era, astronomy was dominated by the geocentric model of the Universe, or the Ptolemaic system, named after Claudius Ptolemy. His 13-volume astronomy work, named the Almagest in its Arabic translation, became the primary reference for over a thousand years.  In this system, the Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. While the system would eventually be discredited it gave the most accurate predictions for the positions of astronomical bodies available at that time.",0,Wikipedia,Astronomy,https://en.wikipedia.org/wiki/Astronomy,,Astronomy,wikipedia_api
human_wiki_0045,"A university (from Latin  universitas 'a whole') is an institution of tertiary education and research which awards academic degrees in several academic disciplines. University is derived from the Latin phrase universitas magistrorum et scholarium, which roughly means ""community of teachers and scholars"". Universities typically offer both undergraduate and postgraduate programs. The first universities in Europe developed from schools that had been maintained by the Church for the purpose of educating priests. The University of Bologna (Università di Bologna), Italy, which was founded in 1088, is the first university in the sense of:",0,Wikipedia,University,https://en.wikipedia.org/wiki/University,,University,wikipedia_api
human_wiki_0046,"being a high degree-awarding institute. using the word universitas (which was coined at its foundation). having independence from the ecclesiastic schools and issuing secular as well as non-secular degrees (with teaching conducted by both clergy and non-clergy): grammar, rhetoric, logic, theology, canon law and notarial law.",0,Wikipedia,University,https://en.wikipedia.org/wiki/University,,University,wikipedia_api
human_wiki_0047,"History Definition The original Latin word universitas refers in general to ""a number of persons associated into one body, a society, company, community, guild, corporation, etc"". As urban town life and medieval guilds developed, specialized associations of students and teachers with collective legal rights (these rights were usually guaranteed by charters issued by princes, prelates, or their towns) became denominated by this general term. Like other guilds, they were self-regulating and determined the qualifications of their members. In modern usage, the word has come to mean ""an institution of higher education offering tuition in mainly non-vocational subjects and typically having the power to confer degrees"". The earlier emphasis on its corporate organization is no longer the primary feature by which a modern university is recognized. The original Latin word referred to degree-awarding institutions of learning in Western and Central Europe, where this form of legal organisation was prevalent and from where the institution spread around the world.",0,Wikipedia,University,https://en.wikipedia.org/wiki/University,,University,wikipedia_api
human_wiki_0048,"Academic freedom An important idea in the definition of a university is the notion of academic freedom. The first documentary evidence of this comes from early in the life of the University of Bologna, which adopted an academic charter, the Constitutio Habita, in 1155 or 1158, which guaranteed the right of a traveling scholar to unhindered passage in the interests of education. Today, this is claimed as the origin of ""academic freedom"".  This is now a widely accepted concept in international research. On 18 September 1988, 430 university rectors signed the Magna Charta Universitatum, marking the 900th anniversary of Bologna's foundation. The number of universities signing the Magna Charta Universitatum continues to grow, drawing from all parts of the world.",0,Wikipedia,University,https://en.wikipedia.org/wiki/University,,University,wikipedia_api
human_wiki_0049,"Antecedents Nalanda University was established by emperor Kumaragupta I of the Gupta Empire around 427 CE, became a major Buddhist learning hub, attracting scholars like Xuanzang. It was destroyed in 1202 CE by Bakhtiyar Khilji. The characterization of Nalanda as a ""university"" in the modern sense has been challenged by scholars. They argue that while it was undoubtedly a major center of learning, comparing it directly to a modern university is historically imprecise. An early institution, often called a university, is the Harran University, founded in the late 8th century. Scholars occasionally call the University of al-Qarawiyyin (name given in 1963), founded as a mosque by Fatima al-Fihri in 859 CE, a university, although Jacques Verger writes that this is done out of scholarly convenience. Several scholars consider that al-Qarawiyyin was founded and run as a madrasa until after World War II. They date the transformation of the madrasa of al-Qarawiyyin into a university to its modern reorganization in 1963. In the wake of these reforms, al-Qarawiyyin was officially renamed ""University of Al Quaraouiyine"" two years later. Some scholars, including George Makdisi, have argued that early medieval universities were influenced by the madrasas in Al-Andalus, the Emirate of Sicily, and the Middle East during the Crusades. Norman Daniel, however, views this argument as overstated. Universities and madrasas differed in several major respects. In 2013, Roy Lowe and Yoshihito Yasuhara claimed that the influences of scholarship from the Islamic world on the universities of Western Europe requires a reconsideration of the development of higher education, turning away from a concern with local institutional structures to a broader consideration within a global context.",0,Wikipedia,University,https://en.wikipedia.org/wiki/University,,University,wikipedia_api
human_wiki_0050,"Aerospace refers to the technology and industry involved with the atmosphere and outer space collectively. Aerospace activity is very diverse, with a multitude of commercial, industrial, and military applications. Aerospace engineering consists of aeronautics and astronautics. Aerospace organizations research, design, manufacture, operate, maintain, and repair both aircraft and spacecraft. The border between space and the atmosphere has been proposed as 100 kilometres (62.1 mi) above the ground according to the physical explanation that the air density is too low for a lifting body to generate meaningful lift force without exceeding orbital velocity. This border has been called the Kármán line.",0,Wikipedia,Aerospace,https://en.wikipedia.org/wiki/Aerospace,,Aerospace,wikipedia_api
human_wiki_0051,"Overview In most industrial countries, the aerospace industry is a co-operation of the public and private sectors. For example, several states have a civilian space program funded by the government, such as National Aeronautics and Space Administration in the United States, European Space Agency in Europe, the Canadian Space Agency in Canada, Indian Space Research Organisation in India, Japan Aerospace Exploration Agency in Japan, Roscosmos State Corporation for Space Activities in Russia, China National Space Administration in China, SUPARCO in Pakistan, Iranian Space Agency in Iran, and Korea Aerospace Research Institute in South Korea. Along with these public space programs, many companies produce technical tools and components such as spacecraft and satellites. Some known companies involved in space programs include Boeing, Cobham, Airbus, SpaceX, Lockheed Martin, RTX Corporation, MDA and Northrop Grumman. These companies are also involved in other areas of aerospace, such as the construction of aircraft.",0,Wikipedia,Aerospace,https://en.wikipedia.org/wiki/Aerospace,,Aerospace,wikipedia_api
human_wiki_0052,"History Modern aerospace began with Engineer George Cayley in 1799. Cayley proposed an aircraft with a ""fixed wing and a horizontal and vertical tail,"" defining characteristics of the modern aeroplane. The 19th century saw the creation of the Aeronautical Society of Great Britain (1866), the American Rocketry Society, and the Institute of Aeronautical Sciences, all of which made aeronautics a more serious scientific discipline. Airmen like Otto Lilienthal, who introduced cambered airfoils in 1891, used gliders to analyze aerodynamic forces. The Wright brothers were interested in Lilienthal's work and read several of his publications. They also found inspiration in Octave Chanute, an airman and the author of Progress in Flying Machines (1894). It was the preliminary work of Cayley, Lilienthal, Chanute, and other early aerospace engineers that brought about the first powered sustained flight at Kitty Hawk, North Carolina on December 17, 1903, by the Wright brothers. War and science fiction inspired scientists and engineers like Konstantin Tsiolkovsky and Wernher von Braun to achieve flight beyond the atmosphere. World War II inspired Wernher von Braun to create the V1 and V2 rockets. The launch of Sputnik 1 in October 1957 started the Space Age, and on July 20, 1969 Apollo 11 achieved the first crewed Moon landing. In April 1981, the Space Shuttle Columbia launched, the start of regular crewed access to orbital space. A sustained human presence in orbital space started with ""Mir"" in 1986 and is continued by the ""International Space Station"". Space commercialization and space tourism are more recent features of aerospace.",0,Wikipedia,Aerospace,https://en.wikipedia.org/wiki/Aerospace,,Aerospace,wikipedia_api
human_wiki_0053,"Manufacturing Aerospace manufacturing is a high-technology industry that produces ""aircraft, guided missiles, space vehicles, aircraft engines, propulsion units, and related parts"". Most of the industry is geared toward governmental work.  For each original equipment manufacturer (OEM), the US government has assigned a Commercial and Government Entity (CAGE) code. These codes help to identify each manufacturer, repair facilities, and other critical aftermarket vendors in the aerospace industry. In the United States, the Department of Defense and the National Aeronautics and Space Administration (NASA) are the two largest consumers of aerospace technology and products. Others include the very large airline industry. The aerospace industry employed 472,000 wage and salary workers in 2006. Most of those jobs were in Washington state and in California, with Missouri, New York and Texas also being important. The leading aerospace manufacturers in the U.S. are Boeing, United Technologies Corporation, SpaceX, Northrop Grumman and Lockheed Martin. As talented American employees age and retire, these manufacturers face an expanding labor shortfall. In order to supply the industrial sector with fresh workers, apprenticeship programs like the Aerospace Joint Apprenticeship Council (AJAC) collaborate with community colleges and aerospace firms in Washington state. Important locations of the civilian aerospace industry worldwide include Washington state (Boeing), California (Boeing, Lockheed Martin, etc.) and Montreal, Quebec, Canada (Bombardier, Pratt & Whitney Canada) in North America; Toulouse, France (Airbus) and Hamburg, Germany (Airbus) in Europe; as well as São José dos Campos, Brazil (Embraer), Querétaro, Mexico (Bombardier Aerospace, General Electric Aviation) and Mexicali, Mexico (United Technologies Corporation, Gulfstream Aerospace) in Latin America. In the European Union, aerospace companies such as Airbus, Safran, Thales, Dassault Aviation, Leonardo and Saab AB account for a large share of the global aerospace industry and research effort, with the European Space Agency as one of the largest consumers of aerospace technology and products. In India, Bangalore is a major center of the aerospace industry, where Hindustan Aeronautics Limited, the National Aerospace Laboratories and the Indian Space Research Organisation are headquartered. The Indian Space Research Organisation (ISRO) launched India's first Moon orbiter, Chandrayaan-1, in October 2008. In Russia, large aerospace companies like Oboronprom and the United Aircraft Building Corporation (encompassing Mikoyan, Sukhoi, Ilyushin, Tupolev, Yakovlev, and Irkut which includes Beriev) are among the major global players in this industry. The historic Soviet Union was also the home of a major aerospace industry. The United Kingdom formerly attempted to maintain its own large aerospace industry, making its own airliners and warplanes, but it has largely turned its lot over to cooperative efforts with continental companies, and it has turned into a large import customer, too, from countries such as the United States. However, the United Kingdom has a very active aerospace sector, with major companies such as BAE Systems, supplying fully assembled aircraft, aircraft components, sub-assemblies and sub-systems to other manufacturers, both in Europe and all over the world. Canada has formerly manufactured some of its own designs for jet warplanes, etc. (e.g. the CF-100 fighter), but for some decades, it has relied on imports from the United States and Europe to fill these needs. However Canada still manufactures some military aircraft although they are generally not combat capable. Another notable example was the late 1950s development of the Avro Canada CF-105 Arrow, a supersonic fighter-interceptor whose 1959 cancellation was considered highly controversial. France has continued to make its own warplanes for its air force and navy, and Sweden continues to make its own warplanes for the Swedish Air Force—especially in support of its position as a neutral country. (See Saab AB.) Other European countries either team up in making fighters (such as the Panavia Tornado and the Eurofighter Typhoon), or else to import them from the United States. Pakistan has a developing aerospace engineering industry. The National Engineering and Scientific Commission, Khan Research Laboratories and Pakistan Aeronautical Complex are among the premier organizations involved in research and development in this sector. Pakistan has the capability of designing and manufacturing guided rockets, missiles and space vehicles. The city of Kamra is home to the Pakistan Aeronautical Complex which contains several factories. This facility is responsible for manufacturing the MFI-17, MFI-395, K-8 and JF-17 Thunder aircraft. Pakistan also has the capability to design and manufacture both armed and unarmed unmanned aerial vehicles. In the People's Republic of China, Beijing, Xi'an, Chengdu, Shanghai, Shenyang and Nanchang are major research and manufacture centers of the aerospace industry. China has developed an extensive capability to design, test and produce military aircraft, missiles and space vehicles. Despite the cancellation in 1983 of the experimental Shanghai Y-10, China is still developing its civil aerospace industry. The aircraft parts industry was born out of the sale of second-hand or used aircraft parts from the aerospace manufacture sector. Within the United States there is a specific process that parts brokers or resellers must follow.  This includes leveraging a certified repair station to overhaul and ""tag"" a part. This certification guarantees that a part was repaired or overhauled to meet OEM specifications. Once a part is overhauled its value is determined from the supply and demand of the aerospace market. When an airline has an aircraft on the ground, the part that the airline requires to get the plane back into service becomes invaluable. This can drive the market for specific parts.  There are several online marketplaces that assist with the commodity selling of aircraft parts. In the aerospace and defense industry, much consolidation occurred at the end of the 20th century and in the early 21st century. Between 1988 and 2011, more than 6,068 mergers and acquisitions with a total known value of US$678 billion were announced worldwide. The largest transactions have been:",0,Wikipedia,Aerospace,https://en.wikipedia.org/wiki/Aerospace,,Aerospace,wikipedia_api
human_wiki_0054,"The acquisition of Rockwell Collins by United Technologies Corporation for US$30.0 billion in 2018 The acquisition of Goodrich Corporation by United Technologies Corporation for US$16.2 billion in 2011 The merger of Allied Signal with Honeywell in a stock swap valued at US$15.6 billion in 1999 The merger of Boeing with McDonnell valued at US$13.4 billion in 1996 The acquisition of Marconi Electronic Systems, a subsidiary of GEC, by British Aerospace for US$12.9 billion in 1999 (now called: BAE Systems) The acquisition of Hughes Aircraft by Raytheon for US$9.5 billion in 1997",0,Wikipedia,Aerospace,https://en.wikipedia.org/wiki/Aerospace,,Aerospace,wikipedia_api
human_wiki_0055,"Quantum mechanics is the fundamental physical theory that describes the behavior of matter and of light; its unusual characteristics typically occur at and below the scale of atoms. It is the foundation of all quantum physics, which includes quantum chemistry, quantum biology, quantum field theory, quantum technology, and quantum information science. Quantum mechanics can describe many systems that classical physics cannot. Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales. Classical mechanics can be derived from quantum mechanics as an approximation that is valid at ordinary scales. Quantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle). Quantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the ""old quantum theory"", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.",0,Wikipedia,Quantum mechanics,https://en.wikipedia.org/wiki/Quantum_mechanics,,Quantum_mechanics,wikipedia_api
human_wiki_0056,"Overview and fundamental concepts Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and subatomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms, but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative. Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been shown to agree with experiment to within 1 part in 1012 when predicting the magnetic properties of an electron. A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only gives probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schrödinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another. One consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.",0,Wikipedia,Quantum mechanics,https://en.wikipedia.org/wiki/Quantum_mechanics,,Quantum_mechanics,wikipedia_api
human_wiki_0057,"Another consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known as wave–particle duality. In addition to light, electrons, atoms, and molecules are all found to exhibit the same dual behavior when fired towards a double slit.",0,Wikipedia,Quantum mechanics,https://en.wikipedia.org/wiki/Quantum_mechanics,,Quantum_mechanics,wikipedia_api
human_wiki_0058,"Another non-classical phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential. In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy, tunnel diode and tunnel field-effect transistor. When quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement ""...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought"". Quantum entanglement enables quantum computing and is part of quantum communication protocols, such as quantum key distribution and superdense coding. Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem. Another possibility opened by entanglement is testing for ""hidden variables"", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory provides. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables. It is not possible to present these concepts in more than a superficial way without introducing the mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects. Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.",0,Wikipedia,Quantum mechanics,https://en.wikipedia.org/wiki/Quantum_mechanics,,Quantum_mechanics,wikipedia_api
human_wiki_0059,"Mathematical formulation In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector                         ψ                 {\displaystyle \psi }     belonging to a (separable) complex Hilbert space                                                 H                                     {\displaystyle {\mathcal {H}}}    . This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys                         ⟨         ψ         ,         ψ         ⟩         =         1                 {\displaystyle \langle \psi ,\psi \rangle =1}    , and it is well-defined up to a complex number of modulus 1 (the global phase), that is,                         ψ                 {\displaystyle \psi }     and                                    e                        i             α                             ψ                 {\displaystyle e^{i\alpha }\psi }     represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions                                    L                        2                             (                    C                  )                 {\displaystyle L^{2}(\mathbb {C} )}    , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors                                                 C                                   2                                     {\displaystyle \mathbb {C} ^{2}}     with the usual inner product. Physical quantities of interest – position, momentum, energy, spin – are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue                         λ                 {\displaystyle \lambda }     is non-degenerate and the probability is given by                                    |                  ⟨                                                λ               →                                          ,         ψ         ⟩                                 |                                   2                                     {\displaystyle |\langle {\vec {\lambda }},\psi \rangle |^{2}}    , where                                                                λ               →                                                  {\displaystyle {\vec {\lambda }}}     is its associated unit-length eigenvector. More generally, the eigenvalue is degenerate and the probability is given by                         ⟨         ψ         ,                    P                        λ                             ψ         ⟩                 {\displaystyle \langle \psi ,P_{\lambda }\psi \rangle }    , where                                    P                        λ                                     {\displaystyle P_{\lambda }}     is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density. After the measurement, if result                         λ                 {\displaystyle \lambda }     was obtained, the quantum state is postulated to collapse to                                                                λ               →                                                  {\displaystyle {\vec {\lambda }}}    , in the non-degenerate case, or to                                    P                        λ                             ψ                                 /                                                              ⟨             ψ             ,                            P                                λ                                         ψ             ⟩                                     {\textstyle P_{\lambda }\psi {\big /}\!{\sqrt {\langle \psi ,P_{\lambda }\psi \rangle }}}    , in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a ""measurement"" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of ""wave function collapse"" (see, for example, the many-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled so that the original quantum system ceases to exist as an independent entity (see Measurement in quantum mechanics).",0,Wikipedia,Quantum mechanics,https://en.wikipedia.org/wiki/Quantum_mechanics,,Quantum_mechanics,wikipedia_api
human_wiki_0060,"Exercise or working out is physical activity that enhances or maintains fitness and overall health. It is performed for various reasons, including weight loss or maintenance, to aid growth and improve strength, develop muscles and the cardiovascular system, hone athletic skills, improve health, or simply for enjoyment. Many people choose to exercise outdoors where they can congregate in groups, socialize, and improve well-being as well as mental health. In terms of health benefits, usually, 150 minutes (2 hours and 30 minutes) of moderate-intensity exercise per week is recommended for reducing the risk of health problems. At the same time, even doing a small amount of exercise is healthier than doing none. Only doing an hour and a quarter (11 minutes/day) of exercise could reduce the risk of early death, cardiovascular disease, stroke, and cancer.",0,Wikipedia,Exercise,https://en.wikipedia.org/wiki/Exercise,,Exercise,wikipedia_api
human_wiki_0061,"Aerobic exercise is any physical activity that uses large muscle groups and causes the body to use more oxygen than it would while resting. The goal of aerobic exercise is to increase cardiovascular endurance. Examples of aerobic exercise include running, cycling, swimming, brisk walking, skipping rope, rowing, hiking, dancing, playing tennis, continuous training, and long distance running. Anaerobic exercise, which includes strength and resistance training, can firm, strengthen, and increase muscle mass, as well as improve bone density, balance, and coordination. Examples of strength exercises are push-ups, pull-ups, lunges, squats, bench press. Anaerobic exercise also includes weight training, functional training, Eccentric Training, interval training, sprinting, and high-intensity interval training which increase short-term muscle strength. Flexibility exercises stretch and lengthen muscles. Activities such as stretching help to improve joint flexibility and keep muscles limber. The goal is to improve the range of motion which can reduce the chance of injury. Physical exercise can also include training that focuses on accuracy, agility, power, and speed. Types of exercise can also be classified as dynamic or static. 'Dynamic' exercises such as steady running, tend to produce a lowering of the diastolic blood pressure during exercise, due to the improved blood flow. Conversely, static exercise (such as weight-lifting) can cause the systolic pressure to rise significantly, albeit transiently, during the performance of the exercise.",0,Wikipedia,Exercise,https://en.wikipedia.org/wiki/Exercise,,Exercise,wikipedia_api
human_wiki_0062,"Health effects Physical exercise is important for maintaining physical fitness and can contribute to maintaining a healthy weight, regulating the digestive system, building and maintaining healthy bone density, muscle strength, and joint mobility, promoting physiological well-being, reducing surgical risks, and strengthening the immune system. Some studies indicate that exercise may increase life expectancy and the overall quality of life. People who participate in moderate to high levels of physical exercise have a lower mortality rate compared to individuals who by comparison are not physically active. Moderate levels of exercise have been correlated with preventing aging by reducing inflammatory potential. The majority of the benefits from exercise are achieved with around 3500 metabolic equivalent (MET) minutes per week, with diminishing returns at higher levels of activity. For example, climbing stairs 10 minutes, vacuuming 15 minutes, gardening 20 minutes, running 20 minutes, and walking or bicycling for transportation 25 minutes on a daily basis would together achieve about 3000 MET minutes a week. A lack of physical activity causes approximately 6% of the burden of disease from coronary heart disease, 7% of type 2 diabetes, 10% of breast cancer, and 10% of colon cancer worldwide. Overall, physical inactivity causes 9% of premature mortality worldwide. The American-British writer Bill Bryson wrote: ""If someone invented a pill that could do for us all that a moderate amount of exercise achieves, it would instantly become the most successful drug in history.""",0,Wikipedia,Exercise,https://en.wikipedia.org/wiki/Exercise,,Exercise,wikipedia_api
human_wiki_0063,"Fitness Most people can increase fitness by increasing physical activity levels. Increases in muscle size from resistance training are primarily determined by diet and testosterone. This genetic variation in improvement from training is one of the key physiological differences between elite athletes and the larger population. There is evidence that exercising in middle age may lead to better physical ability later in life. Early motor skills and development is also related to physical activity and performance later in life. Children who are more proficient with motor skills early on are more inclined to be physically active, and thus tend to perform well in sports and have better fitness levels. Early motor proficiency has a positive correlation to childhood physical activity and fitness levels, while less proficiency in motor skills results in a more sedentary lifestyle. The type and intensity of physical activity performed may have an effect on a person's fitness level. There is some weak evidence that high-intensity interval training may improve a person's VO2 max slightly more than lower intensity endurance training. However, unscientific fitness methods could lead to sports injuries.",0,Wikipedia,Exercise,https://en.wikipedia.org/wiki/Exercise,,Exercise,wikipedia_api
human_wiki_0064,"Cardiovascular system The beneficial effect of exercise on the cardiovascular system is well documented. There is a direct correlation between physical inactivity and cardiovascular disease, and physical inactivity is an independent risk factor for the development of coronary artery disease. Low levels of physical exercise increase the risk of cardiovascular diseases mortality. Children who participate in physical exercise experience greater loss of body fat and increased cardiovascular fitness. Studies have shown that academic stress in youth increases the risk of cardiovascular disease in later years; however, these risks can be greatly decreased with regular physical exercise. There is a dose-response relationship between the amount of exercise performed from approximately 700–2000 kcal of energy expenditure per week and all-cause mortality and cardiovascular disease mortality in middle-aged and elderly men. The greatest potential for reduced mortality is seen in sedentary individuals who become moderately active. Studies have shown that since heart disease is the leading cause of death in women, regular exercise in aging women leads to healthier cardiovascular profiles. The most beneficial effects of physical activity on cardiovascular disease mortality can be attained through moderate-intensity activity (40–60% of maximal oxygen uptake, depending on age). After a myocardial infarction, survivors who changed their lifestyle to include regular exercise had higher survival rates. Sedentary people are most at risk for mortality from cardiovascular and all other causes. According to the American Heart Association, exercise reduces the risk of cardiovascular diseases, including heart attack and stroke. Some have suggested that increases in physical exercise might decrease healthcare costs, increase the rate of job attendance, as well as increase the amount of effort women put into their jobs.",0,Wikipedia,Exercise,https://en.wikipedia.org/wiki/Exercise,,Exercise,wikipedia_api
human_wiki_0065,"Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary gas driving global warming, has increased in concentration by about 50% since the pre-industrial era to levels not seen for millions of years. Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common. Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline. Higher temperatures are also causing more intense storms, droughts, and other weather extremes. Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct. Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise. Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss. Human migration and conflict can also be a result. The World Health Organization calls climate change one of the biggest threats to global health in the 21st century. Societies and ecosystems will experience more severe risks without action to limit warming. Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached. Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change.",0,Wikipedia,Climate change,https://en.wikipedia.org/wiki/Climate_change,,Climate_change,wikipedia_api
human_wiki_0066,"Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60 °C (2.88 °F) since regular tracking began in 1850. Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet. Under the 2015 Paris Agreement, nations collectively agreed to keep warming ""well under 2 °C"". However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century.  There is widespread support for climate action worldwide, and most countries aim to stop emitting carbon dioxide. Fossil fuels can be phased out by stopping subsidising them, conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power. Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes. Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that store carbon in soil.",0,Wikipedia,Climate change,https://en.wikipedia.org/wiki/Climate_change,,Climate_change,wikipedia_api
human_wiki_0067,"Terminology Before the 1980s, it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time. In the 1980s, the terms global warming and climate change became more common, often being used interchangeably. Scientifically, global warming refers only to increased global average surface temperature, while climate change describes both global warming and its effects on Earth's climate system, such as precipitation changes. Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history. Global warming—used as early as 1975—became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate. Since the 2000s, usage of climate change has increased. Various scientists, politicians and media may use the terms climate crisis or climate emergency to talk about climate change, and may use the term global heating instead of global warming.",0,Wikipedia,Climate change,https://en.wikipedia.org/wiki/Climate_change,,Climate_change,wikipedia_api
human_wiki_0068,"Global temperature rise Temperatures prior to present-day global warming Over the last few million years the climate cycled through ice ages. One of the hotter periods was the Last Interglacial, around 125,000 years ago, where temperatures were between 0.5 °C and 1.5 °C warmer than before the start of global warming. This period saw sea levels 5 to 10 metres higher than today. The most recent glacial maximum 20,000 years ago was some 5–7 °C colder. This period has sea levels that were over 125 metres (410 ft) lower than today. Temperatures stabilized in the current interglacial period beginning 11,700 years ago. This period also saw the start of agriculture. Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions. Climate information for that period comes from climate proxies, such as trees and ice cores.",0,Wikipedia,Climate change,https://en.wikipedia.org/wiki/Climate_change,,Climate_change,wikipedia_api
human_wiki_0069,"Warming since the Industrial Revolution Around 1850 thermometer records began to provide global coverage. Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain, but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause global dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature.",0,Wikipedia,Climate change,https://en.wikipedia.org/wiki/Climate_change,,Climate_change,wikipedia_api
human_wiki_0070,"Transport (in British English) or transportation (in American English) is the intentional movement of humans, animals, and goods from one location to another. Modes of transport include air, land (rail and road), water, cable, pipelines, and space. The field can be divided into infrastructure, vehicles, and operations. Transport enables human trade, which is essential for the development of civilizations. Transport infrastructure consists of fixed installations, including roads, railways, airways, waterways, canals, and pipelines, as well as terminals such as airports, railway stations, bus stations, warehouses, trucking terminals, refueling depots (including fuel docks and fuel stations), and seaports. Terminals may be used both for the interchange of passengers and cargo and for maintenance. Means of transport are any of the different kinds of transport facilities used to carry people or cargo. They may include vehicles, riding animals, and pack animals. Vehicles may include wagons, automobiles, bicycles, buses, trains, trucks, helicopters, watercraft, spacecraft, and aircraft.",0,Wikipedia,Transportation,https://en.wikipedia.org/wiki/Transport,,Transportation,wikipedia_api
human_wiki_0071,"Modes A mode of transport is a solution that makes use of a certain type of vehicle, infrastructure, and operation. The transport of a person or of cargo may involve one mode or several of the modes, with the latter case being called inter-modal or multi-modal transport. Each mode has its own advantages and disadvantages, and will be chosen on the basis of cost, capability, and route. Governments regulate the way the vehicles are operated, and the procedures set for this purpose, including financing, legalities, and policies. In the transport industry, operations and ownership of infrastructure can be public, private, or a partnership between the two, depending on the country and mode. Transport modes can be a mix of the two ownership systems, such as privately owned cars and government-owned urban transport in cities. Many international airlines have a mixed public-private ownership. Passenger transport may be public, where operators provide scheduled services, or private. Freight transport has become focused on containerization, although bulk transport is used for large volumes of durable items. Transport plays an important part in economic growth and globalization, but machine-propelled forms cause air pollution and use large amounts of land. While it is heavily subsidized by governments, good planning of transport is essential to make traffic flow and restrain urban sprawl.",0,Wikipedia,Transportation,https://en.wikipedia.org/wiki/Transport,,Transportation,wikipedia_api
human_wiki_0072,"Human-powered Human-powered transport, a form of sustainable transport, is the transport of people or goods using human muscle-power, in the form of walking, running, and swimming. Technology has allowed machines to improve the energy efficiency of human mobility on relatively smooth terrain. Human-powered transport remains popular for reasons of cost-saving, leisure, physical exercise, and environmentalism; it is sometimes the only type available, especially in underdeveloped or inaccessible regions. Although humans are able to walk without infrastructure, the accessibility can be enhanced through the use of roads, sidewalks, and shared-use paths, especially when using the human power with vehicles, such as bicycles, inline skates, and wheelchairs. Human-powered vehicles have been developed for difficult environments, such as snow and water, by watercraft rowing and skiing; even the air can be flown through with human-powered aircraft. Personal transporters, a form of hybrid human-electric powered vehicle, have emerged in the 21st century as a form of multi-model urban transport.",0,Wikipedia,Transportation,https://en.wikipedia.org/wiki/Transport,,Transportation,wikipedia_api
human_wiki_0073,"Animal-powered Animal-powered transport is the use of working animals for the movement of people and commodities. Humans may ride some of the animals directly, use them as pack animals for carrying goods, or harness them, alone or in teams, to pull sleds or wheeled vehicles. They remain useful in rough terrain that is not readily accessible by automotive-based transportation.",0,Wikipedia,Transportation,https://en.wikipedia.org/wiki/Transport,,Transportation,wikipedia_api
human_wiki_0074,"Air A fixed-wing aircraft, commonly called an airplane, is a heavier-than-air craft where movement of the air in relation to the wings is used to generate lift. The term is used to distinguish this from rotary-wing aircraft, where the movement of the lift surfaces relative to the air generates lift. A gyroplane is both fixed-wing and rotary wing. Fixed-wing aircraft range from small trainers and recreational aircraft to large airliners and military cargo aircraft. Two things necessary for aircraft are air flow over the wings for lift and an apparatus for landing. The majority of aircraft require an airport with the infrastructure for maintenance, restocking, and refueling and for the loading and unloading of crew, cargo, and passengers. Many aerodromes have takeoff and landing restrictions on weight and runway length, and so are not able to handle all types of aircraft. While the vast majority of fixed-wing aircraft land and take off on land, some are capable of take-off and landing on ice, snow, and calm water. Autonomous or remotely-piloted airplanes are known as unmanned aerial vehicles, or UAV. These drones can range in size from less than a metre across to a full-sized airplane. They are capable of carrying a payload, and are being used for package delivery. The aircraft is the second fastest method of transport, after the rocket. Commercial jets can reach up to 955 kilometres per hour (593 mph), single-engine aircraft 555 kilometres per hour (345 mph). Aviation is able to quickly transport people and limited amounts of cargo over longer distances, but incurs high costs and energy use; for short distances or in inaccessible places, helicopters can be used. As of April 28, 2009, The Guardian article notes that ""the WHO estimates that up to 500,000 people are on planes at any time."" An aerostat is a class of lighter-than-air aircraft that gains its lift by containing a volume of gas that has a lower density than the surrounding atmosphere. These include balloons and rigid, semi-rigid, or non-rigid airships; the last is called a blimp. The lifting gas is typically helium, as hydrogen is highly flammable. Alternatively, heated air is used in hot air balloons and thermal airships. Aerostats can transport passengers and a payload over long distances. For example, zeppelins were used on long-ranged bombing raids during World War I.",0,Wikipedia,Transportation,https://en.wikipedia.org/wiki/Transport,,Transportation,wikipedia_api
human_wiki_0075,"Health care, or healthcare, is the improvement or maintenance of health via the prevention, diagnosis, treatment, amelioration or cure of disease, illness, injury, and other physical and mental impairments in people. Health care is delivered by health professionals and allied health fields. Medicine, dentistry, pharmacy, midwifery, nursing, optometry, audiology, psychology, occupational therapy, physical therapy, athletic training, and other health professions all constitute health care. The term includes work done in providing primary care, secondary care, tertiary care, and public health. Access to health care may vary across countries, communities, and individuals, influenced by social and economic conditions and health policies. Providing health care services means ""the timely use of personal health services to achieve the best possible health outcomes"". Factors to consider in terms of health care access include financial limitations (such as insurance coverage), geographical and logistical barriers (such as additional transportation costs and the ability to take paid time off work to use such services), sociocultural expectations, and personal limitations (lack of ability to communicate with health care providers, poor health literacy, low income). Limitations to health care services affect negatively the use of medical services, the efficacy of treatments, and overall outcome (well-being, mortality rates). Health systems are the organizations established to meet the health needs of targeted populations. According to the World Health Organization (WHO), a well-functioning health care system requires a financing mechanism, a well-trained and adequately paid workforce, reliable information on which to base decisions and policies, and well-maintained health facilities to deliver quality medicines and technologies. An efficient health care system can contribute to a significant part of a country's economy, development, and industrialization. Health care is an important determinant in promoting the general physical and mental health and well-being of people around the world. An example of this was the worldwide eradication of smallpox in 1980, declared by the WHO, as the first disease in human history to be eliminated by deliberate health care interventions.",0,Wikipedia,Healthcare,https://en.wikipedia.org/wiki/Health_care,,Healthcare,wikipedia_api
human_wiki_0076,"Delivery The delivery of modern health care depends on groups of trained professionals and paraprofessionals coming together as interdisciplinary teams. This includes professionals in medicine, psychology, physiotherapy, nursing, dentistry, midwifery and allied health, along with many others such as public health practitioners, community health workers and assistive personnel. These professionals systematically provide personal and population-based preventive, curative and rehabilitative care services. While the definitions of the various types of health care vary based on the different cultural, political, organizational, and disciplinary perspectives, there is general consensus that primary care constitutes the first element of a continuous health care process and may also include the provision of secondary and tertiary levels of care. Health care can be defined as either public or private.",0,Wikipedia,Healthcare,https://en.wikipedia.org/wiki/Health_care,,Healthcare,wikipedia_api
human_wiki_0077,"Primary care Primary care refers to the work of health professionals who act as a first point of consultation for all patients within the health care system. The primary care model supports first-contact, accessible, continuous, comprehensive and coordinated person-focused care. Such a professional would usually be a primary care physician, such as a general practitioner or family physician. Another professional would be a licensed independent practitioner such as a physiotherapist, or a non-physician primary care provider such as a physician assistant or nurse practitioner. Depending on the locality and health system organization, the patient may see another health care professional first, such as a pharmacist or nurse. Depending on the nature of the health condition, patients may be referred for secondary or tertiary care. Primary care is often used as the term for the health care services that play a role in the local community. It can be provided in different settings, such as Urgent care centers that provide same-day appointments or services on a walk-in basis. Primary care involves the widest scope of health care, including all ages of patients, patients of all socioeconomic and geographic origins, patients seeking to maintain optimal health, and patients with all types of acute and chronic physical, mental and social health issues, including multiple chronic diseases. Consequently, a primary care practitioner must possess a wide breadth of knowledge in many areas. Continuity is a key characteristic of primary care, as patients usually prefer to consult the same practitioner for routine check-ups and preventive care, health education, and every time they require an initial consultation about a new health problem. The International Classification of Primary Care (ICPC) is a standardized tool for understanding and analyzing information on interventions in primary care based on the reason for the patient's visit. Common chronic illnesses usually treated in primary care may include, for example, hypertension, diabetes, asthma, COPD, depression and anxiety, back pain, arthritis or thyroid dysfunction. Primary care also includes many basic maternal and child health care services, such as family planning services and vaccinations. In the United States, the 2013 National Health Interview Survey found that skin disorders (42.7%), osteoarthritis and joint disorders (33.6%), back problems (23.9%), disorders of lipid metabolism (22.4%), and upper respiratory tract disease (22.1%, excluding asthma) were the most common reasons for accessing a physician. In the United States, primary care physicians have begun to deliver primary care outside of the managed care (insurance-billing) system through direct primary care which is a subset of the more familiar concierge medicine. Physicians in this model bill patients directly for services, either on a pre-paid monthly, quarterly, or annual basis, or bill for each service in the office. Examples of direct primary care practices include Foundation Health in Colorado and Qliance in Washington. In the context of global population aging, with increasing numbers of older adults at greater risk of chronic non-communicable diseases, rapidly increasing demand for primary care services is expected in both developed and developing countries. The World Health Organization attributes the provision of essential primary care as an integral component of an inclusive primary health care strategy.",0,Wikipedia,Healthcare,https://en.wikipedia.org/wiki/Health_care,,Healthcare,wikipedia_api
human_wiki_0078,"Secondary care Secondary care includes acute care: necessary treatment for a short period of time for a brief but serious illness, injury, or other health condition. This care is often found in a hospital emergency department. Secondary care also includes skilled attendance during childbirth, intensive care, and medical imaging services. The term ""secondary care"" is sometimes used synonymously with ""hospital care"". However, many secondary care providers, such as psychiatrists, clinical psychologists, occupational therapists, most dental specialties or physiotherapists, do not necessarily work in hospitals. Some primary care services are delivered within hospitals. Depending on the organization and policies of the national health system, patients may be required to see a primary care provider for a referral before they can access secondary care. In countries that operate under a mixed market health care system, some physicians limit their practice to secondary care by requiring patients to see a primary care provider first. This restriction may be imposed under the terms of the payment agreements in private or group health insurance plans. In other cases, medical specialists may see patients without a referral, and patients may decide whether self-referral is preferred. In other countries patient self-referral to a medical specialist for secondary care is rare as prior referral from another physician (either a primary care physician or another specialist) is considered necessary, regardless of whether the funding is from private insurance schemes or national health insurance. Allied health professionals, such as physical therapists, respiratory therapists, occupational therapists, speech therapists, and dietitians, also generally work in secondary care, accessed through either patient self-referral or through physician referral.",0,Wikipedia,Healthcare,https://en.wikipedia.org/wiki/Health_care,,Healthcare,wikipedia_api
human_wiki_0079,"Tertiary care Tertiary care is specialized consultative health care, usually for inpatients and on referral from a primary or secondary health professional, in a facility that has personnel and facilities for advanced medical investigation and treatment, such as a tertiary referral hospital. Examples of tertiary care services are cancer management, neurosurgery, cardiac surgery, plastic surgery, treatment for severe burns, advanced neonatology services, palliative, and other complex medical and surgical interventions.",0,Wikipedia,Healthcare,https://en.wikipedia.org/wiki/Health_care,,Healthcare,wikipedia_api
human_wiki_0080,"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots. Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes. A roboticist is someone who specializes in robotics.",0,Wikipedia,Robotics,https://en.wikipedia.org/wiki/Robotics,,Robotics,wikipedia_api
human_wiki_0081,"Mechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments. The mechanical aspect of the robot is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function. Electrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol-powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status), and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations) Software. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not be able to go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly structured, its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence, and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. A hybrid is a form of programming that incorporates both AI and RC functions in them.",0,Wikipedia,Robotics,https://en.wikipedia.org/wiki/Robotics,,Robotics,wikipedia_api
human_wiki_0082,"Applied robotics As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed ""assembly robots"". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a ""welding robot"" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as ""heavy-duty robots"". Current and potential applications include:",0,Wikipedia,Robotics,https://en.wikipedia.org/wiki/Robotics,,Robotics,wikipedia_api
human_wiki_0083,"Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales. In the auto industry, they can amount for more than half of the ""labor"". There are even ""lights off"" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003. Autonomous transport including airplane autopilot and self-driving cars Domestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading and flatbread baking. Construction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton. Automated mining. Space exploration, including Mars rovers. Energy applications including cleanup of nuclear contaminated areas; and cleaning solar panel arrays. Medical robots and Robot-assisted surgery designed and used in clinics. Agricultural robots. The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage. Food processing. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts), Sally (salads), salad or food bowl robots manufactured by Dexai (a Draper Laboratory spinoff, operating on military bases), and integrated food bowl assembly systems manufactured by Spyce Kitchen (acquired by Sweetgreen) and Silicon Valley startup Hyphen. Other examples may include manufacturing technologies based on 3D Food Printing. Military robots. Robot sports for entertainment and education, including Robot combat, Autonomous racing, drone racing, and FIRST Robotics.",0,Wikipedia,Robotics,https://en.wikipedia.org/wiki/Robotics,,Robotics,wikipedia_api
human_wiki_0084,"Mechanical robotics areas Power source At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.  Potential power sources could be:",0,Wikipedia,Robotics,https://en.wikipedia.org/wiki/Robotics,,Robotics,wikipedia_api
human_wiki_0085,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways. Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to Fortune Global 500 companies.",0,Wikipedia,Civil engineering,https://en.wikipedia.org/wiki/Civil_engineering,,Civil_engineering,wikipedia_api
human_wiki_0086,"History Civil engineering as a discipline Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields. Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in craft guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental. One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.",0,Wikipedia,Civil engineering,https://en.wikipedia.org/wiki/Civil_engineering,,Civil_engineering,wikipedia_api
human_wiki_0087,"Civil engineering profession Engineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt, the Indus Valley civilization, and Mesopotamia (ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing. Until modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The constructions of pyramids in Egypt (c. 2700–2500 BC) constitute some of the first instances of large structure constructions in history. Other ancient historic civil engineering constructions include the Qanat water management system in modern-day Iran (the oldest is older than 3000 years and longer than 71 kilometres (44 mi)), the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.",0,Wikipedia,Civil engineering,https://en.wikipedia.org/wiki/Civil_engineering,,Civil_engineering,wikipedia_api
human_wiki_0088,"In the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. In 1747, the first institution for the teaching of civil engineering, the École Nationale des Ponts et Chaussées, was established in France; and more examples followed in other European countries, like Spain (Escuela Técnica Superior de Ingenieros de Caminos, Canales y Puertos). The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.",0,Wikipedia,Civil engineering,https://en.wikipedia.org/wiki/Civil_engineering,,Civil_engineering,wikipedia_api
human_wiki_0089,"In 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:the art of directing the great sources of power in nature for the use and convenience of man, as the means of production and of traffic in states, both for external and internal trade, as applied in the construction of roads, bridges, aqueducts, canals, river navigation and docks for internal intercourse and exchange, and in the construction of ports, harbours, moles, breakwaters and lighthouses, and in the art of navigation by artificial power for the purposes of commerce, and in the construction and application of machinery, and in the drainage of cities and towns.",0,Wikipedia,Civil engineering,https://en.wikipedia.org/wiki/Civil_engineering,,Civil_engineering,wikipedia_api
human_wiki_0090,"Communication is commonly defined as the transmission of information. Its precise definition is disputed and there are disagreements about whether unintentional or failed transmissions are included and whether communication not only transmits meaning but also creates it. Models of communication are simplified overviews of its main components and their interactions. Many models include the idea that a source uses a coding system to express information in the form of a message. The message is sent through a channel to a receiver who has to decode it to understand it. The main field of inquiry investigating communication is called communication studies. A common way to classify communication is by whether information is exchanged between humans, members of other species, or non-living entities such as computers. For human communication, a central contrast is between verbal and non-verbal communication. Verbal communication involves the exchange of messages in linguistic form, including spoken and written messages as well as sign language. Non-verbal communication happens without the use of a linguistic system, for example, using body language, touch, and facial expressions. Another distinction is between interpersonal communication, which happens between distinct persons, and intrapersonal communication, which is communication with oneself. Communicative competence is the ability to communicate well and applies to the skills of formulating messages and understanding them. Non-human forms of communication include animal and plant communication. Researchers in this field often refine their definition of communicative behavior by including the criteria that observable responses are present and that the participants benefit from the exchange. Animal communication is used in areas like courtship and mating, parent–offspring relations, navigation, and self-defense. Communication through chemicals is particularly important for the relatively immobile plants. For example, maple trees release so-called volatile organic compounds into the air to warn other plants of a herbivore attack. Most communication takes place between members of the same species. The reason is that its purpose is usually some form of cooperation, which is not as common between different species. Interspecies communication happens mainly in cases of symbiotic relationships. For instance, many flowers use symmetrical shapes and distinctive colors to signal to insects where nectar is located. Humans engage in interspecies communication when interacting with pets and working animals. Human communication has a long history and how people exchange information has evolved over time. These changes were usually triggered by the development of new communication technologies. Examples are the invention of writing systems, the development of mass printing, the use of radio and television, and the invention of the internet. The technological advances also led to new forms of communication, such as the exchange of data between computers.",0,Wikipedia,Communication,https://en.wikipedia.org/wiki/Communication,,Communication,wikipedia_api
human_wiki_0091,"Definitions The word communication has its root in the Latin verb communicare, which means 'to share' or 'to make common'. Communication is usually understood as the transmission of information: a message is conveyed from a sender to a receiver using some medium, such as sound, written signs, bodily movements, or electricity. Sender and receiver are often distinct individuals but it is also possible for an individual to communicate with themselves. In some cases, sender and receiver are not individuals but groups like organizations, social classes, or nations. In a different sense, the term communication refers to the message that is being communicated or to the field of inquiry studying communicational phenomena. The precise characterization of communication is disputed. Many scholars have raised doubts that any single definition can capture the term accurately. These difficulties come from the fact that the term is applied to diverse phenomena in different contexts, often with slightly different meanings. The issue of the right definition affects the research process on many levels. This includes issues like which empirical phenomena are observed, how they are categorized, which hypotheses and laws are formulated as well as how systematic theories based on these steps are articulated. Some definitions are broad and encompass unconscious and non-human behavior. Under a broad definition, many animals communicate within their own species and flowers communicate by signaling the location of nectar to bees through their colors and shapes. Other definitions restrict communication to conscious interactions among human beings. Some approaches focus on the use of symbols and signs while others stress the role of understanding, interaction, power, or transmission of ideas. Various characterizations see the communicator's intent to send a message as a central component. In this view, the transmission of information is not sufficient for communication if it happens unintentionally. A version of this view is given by philosopher Paul Grice, who identifies communication with actions that aim to make the recipient aware of the communicator's intention. One question in this regard is whether only successful transmissions of information should be regarded as communication. For example, distortion may interfere with and change the actual message from what was originally intended. A closely related problem is whether acts of deliberate deception constitute communication. According to a broad definition by literary critic I. A. Richards, communication happens when one mind acts upon its environment to transmit its own experience to another mind. Another interpretation is given by communication theorists Claude Shannon and Warren Weaver, who characterize communication as a transmission of information brought about by the interaction of several components, such as a source, a message, an encoder, a channel, a decoder, and a receiver. The transmission view is rejected by transactional and constitutive views, which hold that communication is not just about the transmission of information but also about the creation of meaning. Transactional and constitutive perspectives hold that communication shapes the participant's experience by conceptualizing the world and making sense of their environment and themselves. Researchers studying animal and plant communication focus less on meaning-making. Instead, they often define communicative behavior as having other features, such as playing a beneficial role in survival and reproduction, or having an observable response.",0,Wikipedia,Communication,https://en.wikipedia.org/wiki/Communication,,Communication,wikipedia_api
human_wiki_0092,"Models of communication Models of communication are conceptual representations of the process of communication. Their goal is to provide a simplified overview of its main components. This makes it easier for researchers to formulate hypotheses, apply communication-related concepts to real-world cases, and test predictions. Due to their simplified presentation, they may lack the conceptual complexity needed for a comprehensive understanding of all the essential aspects of communication. They are usually presented visually in the form of diagrams showing the basic components and their interaction. Models of communication are often categorized based on their intended applications and how they conceptualize communication. Some models are general in the sense that they are intended for all forms of communication. Specialized models aim to describe specific forms, such as models of mass communication. One influential way to classify communication is to distinguish between linear transmission, interaction, and transaction models. Linear transmission models focus on how a sender transmits information to a receiver. They are linear because this flow of information only goes in a single direction. This view is rejected by interaction models, which include a feedback loop. Feedback is needed to describe many forms of communication, such as a conversation, where the listener may respond to a speaker by expressing their opinion or by asking for clarification. Interaction models represent the process as a form of two-way communication in which the communicators take turns sending and receiving messages. Transaction models further refine this picture by allowing representations of sending and responding at the same time. This modification is needed to describe how the listener can give feedback in a face-to-face conversation while the other person is talking. Examples are non-verbal feedback through body posture and facial expression. Transaction models also hold that meaning is produced during communication and does not exist independently of it.",0,Wikipedia,Communication,https://en.wikipedia.org/wiki/Communication,,Communication,wikipedia_api
human_wiki_0093,"All the early models, developed in the middle of the 20th century, are linear transmission models. Lasswell's model, for example, is based on five fundamental questions: ""Who?"", ""Says what?"", ""In which channel?"", ""To whom?"", and ""With what effect?"". The goal of these questions is to identify the basic components involved in the communicative process: the sender, the message, the channel, the receiver, and the effect. Lasswell's model was initially only conceived as a model of mass communication, but it has been applied to other fields as well. Some communication theorists, like Richard Braddock, have expanded it by including additional questions, like ""Under what circumstances?"" and ""For what purpose?"".",0,Wikipedia,Communication,https://en.wikipedia.org/wiki/Communication,,Communication,wikipedia_api
human_wiki_0094,"The Shannon–Weaver model is another influential linear transmission model. It is based on the idea that a source creates a message, which is then translated into a signal by a transmitter. Noise may interfere with and distort the signal. Once the signal reaches the receiver, it is translated back into a message and made available to the destination. For a landline telephone call, the person calling is the source and their telephone is the transmitter. The transmitter translates the message into an electrical signal that travels through the wire, which acts as the channel. The person taking the call is the destination and their telephone is the receiver. The Shannon–Weaver model includes an in-depth discussion of how noise can distort the signal and how successful communication can be achieved despite noise. This can happen by making the message partially redundant so that decoding is possible nonetheless. Other influential linear transmission models include Gerbner's model and Berlo's model.",0,Wikipedia,Communication,https://en.wikipedia.org/wiki/Communication,,Communication,wikipedia_api
human_wiki_0095,"Photosynthesis ( FOH-tə-SINTH-ə-sis) is a system of biological processes by which photopigment-bearing autotrophic organisms, such as most plants, algae and cyanobacteria, convert light energy — typically from sunlight — into the chemical energy necessary to fuel their metabolism. The term photosynthesis usually refers to oxygenic photosynthesis, a process that releases oxygen as a byproduct of water splitting. Photosynthetic organisms store the converted chemical energy within the bonds of intracellular organic compounds (complex compounds containing carbon), typically carbohydrates like sugars (mainly glucose, fructose and sucrose), starches, phytoglycogen and cellulose. When needing to use this stored energy, an organism's cells then metabolize the organic compounds through cellular respiration. Photosynthesis plays a critical role in producing and maintaining the oxygen content of the Earth's atmosphere, and it supplies most of the biological energy necessary for complex life on Earth. Some organisms also perform anoxygenic photosynthesis, which does not produce oxygen. Some bacteria (e.g. purple bacteria) uses bacteriochlorophyll to split hydrogen sulfide as a reductant instead of water, releasing sulfur instead of oxygen, which was a dominant form of photosynthesis in the euxinic Canfield oceans during the Boring Billion. Archaea such as Halobacterium also perform a type of non-carbon-fixing anoxygenic photosynthesis, where the simpler photopigment retinal and its microbial rhodopsin derivatives are used to absorb green light and produce a proton (hydron) gradient across the cell membrane, and the subsequent ion movement powers transmembrane proton pumps to directly synthesize adenosine triphosphate (ATP), the ""energy currency"" of cells. Such archaeal photosynthesis might have been the earliest form of photosynthesis that evolved on Earth, as far back as the Paleoarchean, preceding that of cyanobacteria (see Purple Earth hypothesis). While the details may differ between species, the process always begins when light energy is absorbed by the reaction centers, proteins that contain photosynthetic pigments or chromophores. In plants, these pigments are chlorophylls (a porphyrin derivative that absorbs the red and blue spectra of light, thus reflecting green) held inside chloroplasts, abundant in leaf cells. In cyanobacteria, they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two important molecules that participate in energetic processes: reduced nicotinamide adenine dinucleotide phosphate (NADPH) and ATP. In plants, algae, and cyanobacteria, sugars are synthesized by a subsequent sequence of light-independent reactions called the Calvin cycle. In this process, atmospheric carbon dioxide is incorporated into already existing organic compounds, such as ribulose bisphosphate (RuBP). Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose. In other bacteria, different mechanisms like the reverse Krebs cycle are used to achieve the same end. The first photosynthetic organisms probably evolved early in the evolutionary history of life using reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons. Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth, which rendered the evolution of complex life possible. The average rate of energy captured by global photosynthesis is approximately 130 terawatts, which is about eight times the total power consumption of human civilization. Photosynthetic organisms also convert around 100–115 billion tons (91–104 Pg petagrams, or billions of metric tons), of carbon into biomass per year. Photosynthesis was discovered in 1779 by Jan Ingenhousz who showed that plants need light, not just soil and water.",0,Wikipedia,Photosynthesis,https://en.wikipedia.org/wiki/Photosynthesis,,Photosynthesis,wikipedia_api
human_wiki_0096,"Overview Most photosynthetic organisms are photoautotrophs, which means that they are able to synthesize food directly from carbon dioxide and water using energy from light. However, not all organisms use carbon dioxide as a source of carbon atoms to carry out photosynthesis; photoheterotrophs use organic compounds, rather than carbon dioxide, as a source of carbon. In plants, algae, and cyanobacteria, photosynthesis releases oxygen. This oxygenic photosynthesis is by far the most common type of photosynthesis used by living organisms. Some shade-loving plants (sciophytes) produce such low levels of oxygen during photosynthesis that they use all of it themselves instead of releasing it to the atmosphere. Although there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. There are also many varieties of anoxygenic photosynthesis, used mostly by bacteria, which consume carbon dioxide but do not release oxygen or which produce elemental sulfur instead of molecular oxygen. Carbon dioxide is converted into sugars in a process called carbon fixation; photosynthesis captures energy from sunlight to convert carbon dioxide into carbohydrates. Carbon fixation is an endothermic redox reaction. In general outline, photosynthesis is the opposite of cellular respiration: while photosynthesis is a process of reduction of carbon dioxide to carbohydrates, cellular respiration is the oxidation of carbohydrates or other nutrients to carbon dioxide. Nutrients used in cellular respiration include carbohydrates, amino acids and fatty acids. These nutrients are oxidized to produce carbon dioxide and water, and to release chemical energy to drive the organism's metabolism. Photosynthesis and cellular respiration are distinct processes, as they take place through different sequences of chemical reactions and in different cellular compartments (cellular respiration in mitochondria). The general equation for photosynthesis as first proposed by Cornelis van Niel is:",0,Wikipedia,Photosynthesis,https://en.wikipedia.org/wiki/Photosynthesis,,Photosynthesis,wikipedia_api
human_wiki_0097,"CO2carbondioxide + 2H2Aelectron donor + photonslight energy → [CH2O]carbohydrate + 2Aoxidizedelectrondonor + H2Owater Since water is used as the electron donor in oxygenic photosynthesis, the equation for this process is:",0,Wikipedia,Photosynthesis,https://en.wikipedia.org/wiki/Photosynthesis,,Photosynthesis,wikipedia_api
human_wiki_0098,"CO2carbondioxide + 2H2Owater + photonslight energy → [CH2O]carbohydrate + O2oxygen + H2Owater This equation emphasizes that water is both a reactant in the light-dependent reaction and a product of the light-independent reaction, but canceling n water molecules from each side gives the net equation:",0,Wikipedia,Photosynthesis,https://en.wikipedia.org/wiki/Photosynthesis,,Photosynthesis,wikipedia_api
human_wiki_0099,CO2carbondioxide + H2O water  + photonslight energy → [CH2O]carbohydrate + O2 oxygen  Other processes substitute other compounds (such as arsenite) for water in the electron-supply role; for example some microbes use sunlight to oxidize arsenite to arsenate: The equation for this reaction is:,0,Wikipedia,Photosynthesis,https://en.wikipedia.org/wiki/Photosynthesis,,Photosynthesis,wikipedia_api
human_wiki_0100,"A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks.",0,Wikipedia,Neural network,https://en.wikipedia.org/wiki/Neural_network,,Neural_network,wikipedia_api
human_wiki_0101,"In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.",0,Wikipedia,Neural network,https://en.wikipedia.org/wiki/Neural_network,,Neural_network,wikipedia_api
human_wiki_0102,"In biology In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead. Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.",0,Wikipedia,Neural network,https://en.wikipedia.org/wiki/Neural_network,,Neural_network,wikipedia_api
human_wiki_0103,"In machine learning In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset. The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers. Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.",0,Wikipedia,Neural network,https://en.wikipedia.org/wiki/Neural_network,,Neural_network,wikipedia_api
human_wiki_0104,"History The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. In 1956, Svaetichin discovered the functioning of second order retinal cells (Horizontal Cells), which were fundamental for the understanding of neural networks. Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957, artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.",0,Wikipedia,Neural network,https://en.wikipedia.org/wiki/Neural_network,,Neural_network,wikipedia_api
human_wiki_0105,"A vaccine is a biological preparation that provides active acquired immunity to a particular infectious or malignant disease. The safety and effectiveness of vaccines has been widely studied and verified. A vaccine typically contains an agent that resembles a disease-causing microorganism and is often made from weakened or killed forms of the microbe, its toxins, or one of its surface proteins. The agent stimulates the immune system to recognize the agent as a threat, destroy it, and recognize further and destroy any of the microorganisms associated with that agent that it may encounter in the future. Vaccines can be prophylactic (to prevent or alleviate the effects of a future infection by a natural or ""wild"" pathogen), or therapeutic (to fight a disease that has already occurred, such as cancer). Some vaccines offer full sterilizing immunity, in which infection is prevented. The administration of vaccines is called vaccination. Vaccination is the most effective method of preventing infectious diseases; widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the restriction of diseases such as polio, measles, and tetanus from much of the world. The World Health Organization (WHO) reports that licensed vaccines are available for twenty-five different preventable infections. The first recorded use of inoculation to prevent smallpox (see variolation) occurred in the 16th century in China, with the earliest hints of the practice in China coming during the 10th century. It was also the first disease for which a vaccine was produced. The folk practice of inoculation against smallpox was brought from Turkey to Britain in 1721 by Lady Mary Wortley Montagu. The terms vaccine and vaccination are derived from Variolae vaccinae (smallpox of the cow), the term devised by Edward Jenner (who both developed the concept of vaccines and created the first vaccine) to denote cowpox. He used the phrase in 1798 for the long title of his Inquiry into the Variolae vaccinae Known as the Cow Pox, in which he described the protective effect of cowpox against smallpox. In 1881, to honor Jenner, Louis Pasteur proposed that the terms should be extended to cover the new protective inoculations then being developed. The science of vaccine development and production is termed vaccinology.",0,Wikipedia,Vaccine,https://en.wikipedia.org/wiki/Vaccine,,Vaccine,wikipedia_api
human_wiki_0106,"Effectiveness There is overwhelming scientific consensus that vaccines are a very safe and effective way to fight and eradicate infectious diseases. The immune system recognizes vaccine agents as foreign, destroys them, and ""remembers"" them. When the virulent version of an agent is encountered, the body recognizes the protein coat on the agent, and thus is prepared to respond, by first neutralizing the target agent before it can enter cells, and secondly by recognizing and destroying infected cells before that agent can multiply to vast numbers. In 1958, there were 763,094 cases of measles in the United States; 552 deaths resulted. After the introduction of new vaccines, the number of cases dropped to fewer than 150 per year (median of 56). In early 2008, there were 64 suspected cases of measles. Fifty-four of those infections were associated with importation from another country, although only thirteen percent were actually acquired outside the United States; 63 of the 64 individuals either had never been vaccinated against measles or were uncertain whether they had been vaccinated. The measles vaccine is estimated to prevent a million deaths every year. Vaccines led to the eradication of smallpox, one of the most contagious and deadly diseases in humans. Other diseases such as rubella, polio, measles, mumps, chickenpox, and typhoid are nowhere near as common as they were a hundred years ago thanks to widespread vaccination programs. As long as the vast majority of people are vaccinated, it is much more difficult for an outbreak of disease to occur, let alone spread. This effect is called herd immunity. Polio, which is transmitted only among humans, is targeted by an extensive eradication campaign that has seen endemic polio restricted to only parts of three countries (Afghanistan, Nigeria, and Pakistan). However, the difficulty of reaching all children, cultural misunderstandings, and disinformation have caused the anticipated eradication date to be missed several times. Vaccines also help prevent the development of antibiotic resistance. For example, by greatly reducing the incidence of pneumonia caused by Streptococcus pneumoniae, vaccine programs have greatly reduced the prevalence of infections resistant to penicillin or other first-line antibiotics.",0,Wikipedia,Vaccine,https://en.wikipedia.org/wiki/Vaccine,,Vaccine,wikipedia_api
human_wiki_0107,"Limitations Limitations to their effectiveness, nevertheless, exist. Sometimes, protection fails for vaccine-related reasons such as failures in vaccine attenuation, vaccination regimens or administration. Failure may also occur for host-related reasons if the host's immune system does not respond adequately or at all. Host-related lack of response occurs in an estimated 2–10% of individuals, due to factors including genetics, immune status, age, health and nutritional status. One type of primary immunodeficiency disorder resulting in genetic failure is X-linked agammaglobulinemia, in which the absence of an enzyme essential for B cell development prevents the host's immune system from generating antibodies to a pathogen. Host–pathogen interactions and responses to infection are dynamic processes involving multiple pathways in the immune system. A host does not develop antibodies instantaneously: while the body's innate immunity may be activated in as little as twelve hours, adaptive immunity can take 1–2 weeks to fully develop. During that time, the host can still become infected. Once antibodies are produced, they may promote immunity in any of several ways, depending on the class of antibodies involved. Their success in clearing or inactivating a pathogen will depend on the amount of antibodies produced and on the extent to which those antibodies are effective at countering the strain of the pathogen involved, since different strains may be differently susceptible to a given immune reaction.  In some cases vaccines may result in partial immune protection (in which immunity is less than 100% effective but still reduces risk of infection) or in temporary immune protection (in which immunity wanes over time) rather than full or permanent immunity. They can still raise the reinfection threshold for the population as a whole and make a substantial impact. They can also mitigate the severity of infection, resulting in a lower mortality rate, lower morbidity, faster recovery from illness, and a wide range of other effects. Those who are older often display less of a response than those who are younger, a pattern known as Immunosenescence. Adjuvants commonly are used to boost immune response, particularly for older people whose immune response to a simple vaccine may have weakened. The efficacy or performance of the vaccine is dependent on several factors:",0,Wikipedia,Vaccine,https://en.wikipedia.org/wiki/Vaccine,,Vaccine,wikipedia_api
human_wiki_0108,"the disease itself (for some diseases vaccination performs better than for others) the strain of vaccine (some vaccines are specific to, or at least most effective against, particular strains of the disease) whether the vaccination schedule has been properly observed. idiosyncratic response to vaccination; some individuals are ""non-responders"" to certain vaccines, meaning that they do not generate antibodies even after being vaccinated correctly. assorted factors such as ethnicity, age, or genetic predisposition. If a vaccinated individual does develop the disease vaccinated against (breakthrough infection), the disease is likely to be less severe and less transmissible than in unvaccinated cases. Important considerations in an effective vaccination program:",0,Wikipedia,Vaccine,https://en.wikipedia.org/wiki/Vaccine,,Vaccine,wikipedia_api
human_wiki_0109,"careful modeling to anticipate the effect that an immunization campaign will have on the epidemiology of the disease in the medium to long term ongoing surveillance for the relevant disease following introduction of a new vaccine maintenance of high immunization rates, even when a disease has become rare",0,Wikipedia,Vaccine,https://en.wikipedia.org/wiki/Vaccine,,Vaccine,wikipedia_api
human_wiki_0110,"Democracy (from Ancient Greek: δημοκρατία, romanized: dēmokratía, from dēmos 'people' and krátos 'rule') is a form of government in which political power is vested in the people or the population of a state. Under a minimalist definition of democracy, rulers are elected through competitive elections while more expansive or maximalist definitions link democracy to guarantees of civil liberties and human rights in addition to competitive elections. In a direct democracy, the people have the direct authority to deliberate and decide legislation. In a representative democracy, the people choose governing officials through elections to do so. The definition of ""the people"" and the ways authority is shared among them or delegated by them have changed over time and at varying rates in different countries. Features of democracy often include freedom of assembly, association, personal property, freedom of religion and speech, citizenship, consent of the governed, voting rights, freedom from unwarranted governmental deprivation of the right to life and liberty, and minority rights. The notion of democracy has evolved considerably over time. Throughout history, one can find evidence of direct democracy, in which communities make decisions through popular assembly. Today, the dominant form of democracy is representative democracy, where citizens elect government officials to govern on their behalf such as in a parliamentary or presidential democracy. In the common variant of liberal democracy, the powers of the majority are exercised within the framework of a representative democracy, but a constitution and supreme court limit the majority and protect the minority—usually through securing the enjoyment by all of certain individual rights, such as freedom of speech or freedom of association. The term appeared in the 5th century BC in Greek city-states, notably Classical Athens, to mean ""rule of the people"", in contrast to aristocracy (ἀριστοκρατία, aristokratía), meaning ""rule of an elite"". In virtually all democratic governments throughout ancient and modern history, democratic citizenship was initially restricted to an elite class, which was later extended to all adult citizens. In most modern democracies, this was achieved through the suffrage movements of the 19th and 20th centuries. Democracy contrasts with forms of government where power is not vested in the general population of a state, such as authoritarian systems. Historically a rare and vulnerable form of government, democratic systems of government have become more prevalent since the 19th century, in particular with various waves of democratization. Democracy garners considerable legitimacy in the modern world, as public opinion across regions tends to strongly favor democratic systems of government relative to alternatives, and as even authoritarian states try to present themselves as democratic. Democracy more consistently results in improved health, education and economic outcomes. According to the V-Dem Democracy indices and The Economist Democracy Index, less than half the world's population lives in a democracy as of 2022.",0,Wikipedia,Democracy,https://en.wikipedia.org/wiki/Democracy,,Democracy,wikipedia_api
human_wiki_0111,"Characteristics Although democracy is generally understood to be defined by voting, no consensus exists on a precise definition of democracy. Karl Popper says that the ""classical"" view of democracy is, ""in brief, the theory that democracy is the rule of the people and that the people have a right to rule"". One study identified 2,234 adjectives used to describe democracy in the English language. Democratic principles are reflected in all eligible citizens being equal before the law and having equal access to legislative processes. For example, in a representative democracy, every vote has (in theory) equal weight, and the freedom of eligible citizens is secured by legitimised rights and liberties which are typically enshrined in a constitution, while other uses of ""democracy"" may encompass direct democracy, in which citizens vote on issues directly. According to the United Nations, democracy ""provides an environment that respects human rights and fundamental freedoms, and in which the freely expressed will of people is exercised."" One theory holds that democracy requires three fundamental principles: upward control (sovereignty residing at the lowest levels of authority), political equality, and social norms by which individuals and institutions only consider acceptable acts that reflect the first two principles of upward control and political equality. Legal equality, political freedom and rule of law are often identified by commentators as foundational characteristics for a well-functioning democracy. In some countries, notably in the United Kingdom (which originated the Westminster system), the dominant principle is that of parliamentary sovereignty, while maintaining judicial independence. In India, parliamentary sovereignty is subject to the Constitution of India which includes judicial review. Though the term ""democracy"" is typically used in the context of a political state, the principles also are potentially applicable to private organisations, such as clubs, societies and firms. Democracies may use many different decision-making methods, but majority rule is the dominant form. Without compensation, like legal protections of individual or group rights, political minorities can be oppressed by the ""tyranny of the majority"". Majority rule involves a competitive approach, opposed to consensus democracy, creating the need that elections, and generally deliberation, be substantively and procedurally ""fair"","" i.e. just and equitable. In some countries, freedom of political expression, freedom of speech, and freedom of the press are considered important to ensure that voters are well informed, enabling them to vote according to their own interests and beliefs. It has also been suggested that a basic feature of democracy is the capacity of all voters to participate freely and fully in the life of their society. With its emphasis on notions of social contract and the collective will of all the voters, democracy can also be characterised as a form of political collectivism because it is defined as a form of government in which all eligible citizens have an equal say in lawmaking. Republics, though often popularly associated with democracy because of the shared principle of rule by consent of the governed, are not necessarily democracies, as republicanism does not specify how the people are to rule. Classically the term ""republic"" encompassed both democracies and aristocracies. In a modern sense the republican form of government is a form of government without a monarch. Because of this, democracies can be republics or constitutional monarchies, such as the United Kingdom, where the monarch is not a ruler.",0,Wikipedia,Democracy,https://en.wikipedia.org/wiki/Democracy,,Democracy,wikipedia_api
human_wiki_0112,"History Democratic assemblies are as old as the human species and are found throughout human history, but up until the nineteenth century, major political figures have largely opposed democracy. Republican theorists linked democracy to small size: as political units grew in size, the likelihood increased that the government would turn despotic. At the same time, small political units were vulnerable to conquest. Montesquieu wrote, ""If a republic be small, it is destroyed by a foreign force; if it is large, it is ruined by an internal imperfection."" According to Johns Hopkins University political scientist Daniel Deudney, the creation of the United States, with its large size and its system of checks and balances, was a solution to the dual problems of size. Forms of democracy occurred organically in societies around the world that had no contact with each other.",0,Wikipedia,Democracy,https://en.wikipedia.org/wiki/Democracy,,Democracy,wikipedia_api
human_wiki_0113,"Origins Greece and Rome The term democracy first appeared in ancient Greek political and philosophical thought in the city-state of Athens during classical antiquity. The word comes from dêmos '(common) people' and krátos 'force/might'. Under Cleisthenes, what is generally held as the first example of a type of democracy in the sixth-century BC (508–507 BC) was established in Athens. Cleisthenes is referred to as ""the father of Athenian democracy"". The first attested use of the word democracy is found in prose works of the 430s BC, such as Herodotus' Histories, but its usage was older by several decades, as two Athenians born in the 470s were named Democrates, a new political name—likely in support of democracy—given at a time of debates over constitutional issues in Athens. Aeschylus also strongly alludes to the word in his play The Suppliants, staged in c.463 BC, where he mentions ""the demos's ruling hand"" [demou kratousa cheir]. Before that time, the word used to define the new political system of Cleisthenes was probably isonomia, meaning political equality.",0,Wikipedia,Democracy,https://en.wikipedia.org/wiki/Democracy,,Democracy,wikipedia_api
human_wiki_0114,"Athenian democracy took the form of direct democracy, and it had two distinguishing features: the random selection of ordinary citizens to fill the few existing government administrative and judicial offices, and a legislative assembly consisting of all Athenian citizens. All eligible citizens were allowed to speak and vote in the assembly, which set the laws of the city-state. However, Athenian citizenship excluded women, slaves, foreigners (μέτοικοι / métoikoi), and youths below the age of military service. Effectively, only 1 in 4 residents in Athens qualified as citizens. Owning land was not a requirement for citizenship. The exclusion of large parts of the population from the citizen body is closely related to the ancient understanding of citizenship. In most of antiquity the benefit of citizenship was tied to the obligation to fight war campaigns. Athenian democracy was not only direct in the sense that decisions were made by the assembled people, but also the most direct in the sense that the people through the assembly, boule and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business. Even though the rights of the individual were not secured by the Athenian constitution in the modern sense (the ancient Greeks had no word for ""rights""), those who were citizens of Athens enjoyed their liberties not in opposition to the government but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person. Range voting appeared in Sparta as early as 700 BC. The Spartan ecclesia was an assembly of the people, held once a month, in which every male citizen of at least 20 years of age could participate. In the assembly, Spartans elected leaders and cast votes by range voting and shouting (the vote is then decided on how loudly the crowd shouts). Aristotle called this ""childish"", as compared with the stone voting ballots used by the Athenian citizenry. Sparta adopted it because of its simplicity, and to prevent any biased voting, buying, or cheating that was predominant in the early democratic elections.",0,Wikipedia,Democracy,https://en.wikipedia.org/wiki/Democracy,,Democracy,wikipedia_api
human_wiki_0115,"The World Wide Web (also known as WWW, W3, or simply the Web) is an information system that enables content sharing over the Internet through user-friendly ways meant to appeal to users beyond IT specialists and hobbyists. It allows documents and other web resources to be accessed over the Internet according to specific rules of the Hypertext Transfer Protocol (HTTP). The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1993. It was conceived as a ""universal linked information system"". Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through a character string called uniform resource locator (URL). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs), which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information on the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organisations, government agencies, and individual users; and comprises an enormous amount of educational, entertainment, commercial, and government information. The Web has become the world's dominant information systems platform. It is the primary tool that billions of people worldwide use to interact with the Internet.",0,Wikipedia,World Wide Web,https://en.wikipedia.org/wiki/World_Wide_Web,,World_Wide_Web,wikipedia_api
human_wiki_0116,"History The Web was invented by English computer scientist Tim Berners-Lee while working at CERN. He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organisation, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common tree structure approach, used for instance in the existing CERNDOC documentation system and in the Unix filesystem, as well as approaches that relied on tagging files with keywords, as in the VAX/NOTES system. Instead, he adopted concepts he had put into practice with his private ENQUIRE system (1980), built at CERN. When he became aware of Ted Nelson's hypertext model (1965), in which documents can be linked in unconstrained ways through hyperlinks associated with ""hot spots"" embedded in the text, it helped to confirm the validity of his concept.",0,Wikipedia,World Wide Web,https://en.wikipedia.org/wiki/World_Wide_Web,,World_Wide_Web,wikipedia_api
human_wiki_0117,"The model was later popularised by Apple's HyperCard system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to mutable data files, or even fire up programs on their server computer. He also conceived ""gateways"" that would allow access through the new system to documents organised in other ways (such as traditional computer file systems or the Usenet). Finally, he insisted that the system should be decentralised, without any central control or coordination over the creation of links. Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. He got a working system implemented by the end of 1990, including a browser called  WorldWideWeb (which became the name of the project and of the network) and an HTTP server running at CERN. As part of that development, he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN and began to spread to other scientific and academic institutions. Within the next two years, there were 50 websites created. CERN made the Web protocol and code available royalty free on 30 April 1993, enabling its widespread use. After the NCSA released the Mosaic web browser later that year, the Web's popularity grew rapidly as thousands of websites sprang up in less than a year. Mosaic was a graphical browser that could display inline images and submit forms that  were processed by the HTTPd server. Marc Andreessen and Jim Clark founded Netscape the following year and released the Navigator browser, which introduced Java and JavaScript to the Web. It quickly became the dominant browser. Netscape became a public company in 1995, which triggered a frenzy for the Web and started the dot-com bubble. Microsoft responded by developing its own browser, Internet Explorer, starting the browser wars. By bundling it with Windows, it became the dominant browser for 14 years. Berners-Lee founded the World Wide Web Consortium (W3C) which created XML in 1996 and recommended replacing HTML with stricter XHTML. In the meantime, developers began exploiting an IE feature called XMLHttpRequest to make Ajax applications and launched the Web 2.0 revolution. Mozilla, Opera, and Apple rejected XHTML and created the WHATWG which developed HTML5. In 2009, the W3C conceded and abandoned XHTML. In 2019, it ceded control of the HTML specification to the WHATWG. The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet.",0,Wikipedia,World Wide Web,https://en.wikipedia.org/wiki/World_Wide_Web,,World_Wide_Web,wikipedia_api
human_wiki_0118,"Nomenclature Tim Berners-Lee states that World Wide Web is officially spelled as three separate words, each capitalised, with no intervening hyphens. Use of the www prefix has been declining, especially when web applications sought to brand their domain names and make them easily pronounceable. As the mobile web grew in popularity, services like Gmail.com, Outlook.com, Myspace.com, Facebook.com and Twitter.com are most often mentioned without adding ""www."" (or, indeed, "".com"") to the domain. In English, www is usually read as double-u double-u double-u. Some users pronounce it dub-dub-dub, particularly in New Zealand. Stephen Fry, in his ""Podgrams"" series of podcasts, pronounces it wuh wuh wuh. The English writer Douglas Adams once quipped in The Independent on Sunday (1999): ""The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for"".",0,Wikipedia,World Wide Web,https://en.wikipedia.org/wiki/World_Wide_Web,,World_Wide_Web,wikipedia_api
human_wiki_0119,"Function The terms Internet and World Wide Web are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of computer networks interconnected through telecommunications and optical networking. In contrast, the World Wide Web is a global collection of documents and other resources, linked by hyperlinks and URIs. Web resources are accessed using HTTP or HTTPS, which are application-level Internet protocols that use the Internet transport protocols. Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after channel surfing), or 'navigating the Web'. Early studies of this new behaviour investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation, and targeted navigation. The following example demonstrates the functioning of a web browser when accessing a page at the URL http://example.org/home.html. The browser resolves the server name of the URL (example.org) into an Internet Protocol address using the globally distributed Domain Name System (DNS). This lookup returns an IP address such as 203.0.113.4 or 2001:db8:2e::7334. The browser then requests the resource by sending an HTTP request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service, so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses port number 80 and, for HTTPS, it normally uses port number 443. The content of the HTTP request can be as simple as two lines of text:",0,Wikipedia,World Wide Web,https://en.wikipedia.org/wiki/World_Wide_Web,,World_Wide_Web,wikipedia_api
human_wiki_0120,"Economics () is a social science that studies the production, distribution, and consumption of goods and services. Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyses what is viewed as basic elements within economies, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyses economies as systems where production, distribution, consumption, savings, and investment expenditure interact; and the factors of production affecting them, such as: labour, capital, land, and enterprise, inflation, economic growth, and public policies that impact these elements. It also seeks to analyse and describe the global economy. Other broad distinctions within economics include those between positive economics, describing ""what is"", and normative economics, advocating ""what ought to be""; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics. Economic analysis can be applied throughout society, including business, finance, cybersecurity, health care, engineering and government. It is also applied to such diverse subjects as crime, education, the family, feminism, law, philosophy, politics, religion, social institutions, war, science, and the environment.",0,Wikipedia,Economics,https://en.wikipedia.org/wiki/Economics,,Economics,wikipedia_api
human_wiki_0121,"Definitions of economics The earlier term for the discipline was ""political economy"", but since the late 19th century, it has commonly been called ""economics"". The term is ultimately derived from Ancient Greek οἰκονομία (oikonomia) which is a term for the ""way (nomos) to run a household (oikos)"", or in other words the know-how of an οἰκονομικός (oikonomikos), or ""household or homestead manager"". Derived terms such as ""economy"" can therefore often mean ""frugal"" or ""thrifty"". By extension then, ""political economy"" was the way to manage a polis or state. There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as ""an inquiry into the nature and causes of the wealth of nations"", in particular as:",0,Wikipedia,Economics,https://en.wikipedia.org/wiki/Economics,,Economics,wikipedia_api
human_wiki_0122,"a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the public services. Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined ""the dismal science"" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) delimited the subject matter further:",0,Wikipedia,Economics,https://en.wikipedia.org/wiki/Economics,,Economics,wikipedia_api
human_wiki_0123,"The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object. Alfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level:",0,Wikipedia,Economics,https://en.wikipedia.org/wiki/Economics,,Economics,wikipedia_api
human_wiki_0124,"Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man. Lionel Robbins (1932) developed implications of what has been termed ""[p]erhaps the most commonly accepted current definition of the subject"":",0,Wikipedia,Economics,https://en.wikipedia.org/wiki/Economics,,Economics,wikipedia_api
human_wiki_0125,"Cloud computing is defined by the ISO as ""a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on demand"". It is commonly referred to as ""the cloud"".",0,Wikipedia,Cloud computing,https://en.wikipedia.org/wiki/Cloud_computing,,Cloud_computing,wikipedia_api
human_wiki_0126,"On-demand self-service: ""A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider."" Broad network access: ""Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations)."" Resource pooling: "" The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand."" Rapid elasticity: ""Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time."" Measured service: ""Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list.",0,Wikipedia,Cloud computing,https://en.wikipedia.org/wiki/Cloud_computing,,Cloud_computing,wikipedia_api
human_wiki_0127,"History The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The ""data center"" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The ""cloud"" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of ""places"" that mobile agents in the Telescript environment could ""go"". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with ""cloud computing-enabled applications"". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers. In the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2) were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds. The following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.",0,Wikipedia,Cloud computing,https://en.wikipedia.org/wiki/Cloud_computing,,Cloud_computing,wikipedia_api
human_wiki_0128,"Value proposition Cloud computing can shorten time to market by offering pre-configured tools, scalable resources, and managed services, allowing users to focus on core business value rather than maintaining infrastructure. Cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model, where costs scale with usage. Cloud platforms also offer managed services and tools, such as artificial intelligence, data analytics, and machine learning, which might otherwise require significant in-house expertise and infrastructure investment. While cloud computing can offer cost advantages through effective resource optimization, organizations often face challenges such as unused resources, inefficient configurations, and hidden costs without proper oversight and governance. Many cloud platforms provide cost management tools, such as AWS Cost Explorer and Azure Cost Management, and frameworks like FinOps have emerged to standardize financial operations in the cloud. Cloud computing also facilitates collaboration, remote work, and global service delivery by enabling secure access to data and applications from any location with an internet connection. Cloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or licensing fees. Cloud environments operate under a shared responsibility model, where providers are typically responsible for infrastructure security, physical hardware, and software updates, while customers are accountable for data encryption, identity and access management (IAM), and application-level security. These responsibilities vary depending on the cloud service model—Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS)—with customers typically having more control and responsibility in IaaS environments and progressively less in PaaS and SaaS models, often trading control for convenience and managed services.",0,Wikipedia,Cloud computing,https://en.wikipedia.org/wiki/Cloud_computing,,Cloud_computing,wikipedia_api
human_wiki_0129,"Adoption and suitability The decision to adopt cloud computing or maintain on-premises infrastructure depends on factors such as scalability, cost structure, latency requirements, regulatory constraints, and infrastructure customization. Organizations with variable or unpredictable workloads, limited capital for upfront investments, or a focus on rapid scalability benefit from cloud adoption. Startups, SaaS companies, and e-commerce platforms often prefer the pay-as-you-go operational expenditure (OpEx) model of cloud infrastructure. Additionally, companies prioritizing global accessibility, remote workforce enablement, disaster recovery, and leveraging advanced services such as AI/ML and analytics are well-suited for the cloud. In recent years, some cloud providers have started offering specialized services for high-performance computing and low-latency applications, addressing some use cases previously exclusive to on-premises setups. On the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control and data sovereignty. Additionally, companies with ultra-low latency requirements, such as high-frequency trading (HFT) firms, rely on custom hardware (e.g., FPGAs) and physical proximity to exchanges, which most cloud providers cannot fully replicate despite recent advancements. Similarly, tech giants like Google, Meta, and Amazon build their own data centers due to economies of scale, predictable workloads, and the ability to customize hardware and network infrastructure for optimal efficiency. However, these companies also use cloud services selectively for certain workloads and applications where it aligns with their operational needs. In practice, many organizations are increasingly adopting hybrid cloud architectures, combining on-premises infrastructure with cloud services. This approach allows businesses to balance scalability, cost-effectiveness, and control, offering the benefits of both deployment models while mitigating their respective limitations.",0,Wikipedia,Cloud computing,https://en.wikipedia.org/wiki/Cloud_computing,,Cloud_computing,wikipedia_api
human_wiki_0130,"Augmented reality (AR), also known as mixed reality (MR), is a technology that overlays real-time 3D-rendered computer graphics onto a portion of the real world through a display, such as a handheld device or head-mounted display. This experience is seamlessly interwoven with the physical world such that it is perceived as an immersive aspect of the real environment. In this way, augmented reality alters one's ongoing perception of a real-world environment, compared to virtual reality, which aims to completely replace the user's real-world environment with a simulated one. Augmented reality is typically visual, but can span multiple sensory modalities, including auditory, haptic, and somatosensory. The primary value of augmented reality is the manner in which components of a digital world blend into a person's perception of the real world, through the integration of immersive sensations, which are perceived as real in the user's environment. The earliest functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992. Commercial augmented reality experiences were first introduced in entertainment and gaming businesses. Subsequently, augmented reality applications have spanned industries such as education, communications, medicine, and entertainment. Augmented reality can be used to enhance natural environments or situations and offers perceptually enriched experiences. With the help of advanced AR technologies (e.g. adding computer vision, incorporating AR cameras into smartphone applications, and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulated. Information about the environment and its objects is overlaid on the real world. This information can be virtual or real, e.g. seeing other real sensed or measured information such as electromagnetic radio waves overlaid in exact alignment with where they actually are in space. Augmented reality also has a lot of potential in the gathering and sharing of tacit knowledge. Immersive perceptual information is sometimes combined with supplemental information like scores over a live video feed of a sporting event. This combines the benefits of both augmented reality technology and heads up display technology (HUD). Augmented reality frameworks include ARKit and ARCore. Commercial augmented reality headsets include the Magic Leap 1 and HoloLens. A number of companies have promoted the concept of smartglasses that have augmented reality capability. Augmented reality can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects. The overlaid sensory information can be constructive (i.e. additive to the natural environment), or destructive (i.e. masking of the natural environment). As such, it is one of the key technologies in the reality-virtuality continuum. Augmented reality refers to experiences that are artificial and that add to the already existing reality.",0,Wikipedia,Augmented reality,https://en.wikipedia.org/wiki/Augmented_reality,,Augmented_reality,wikipedia_api
human_wiki_0131,"Comparison with mixed reality/virtual reality Augmented reality (AR) is largely synonymous with mixed reality (MR). There is also overlap in terminology with extended reality and computer-mediated reality. However, In the 2020s, the differences between AR and MR began to be emphasized.",0,Wikipedia,Augmented reality,https://en.wikipedia.org/wiki/Augmented_reality,,Augmented_reality,wikipedia_api
human_wiki_0132,"Mixed reality (MR) is an advanced technology that extends beyond augmented reality (AR) by seamlessly integrating the physical and virtual worlds. In MR, users are not only able to view digital content within their real environment but can also interact with it as if it were a tangible part of the physical world. This is made possible through devices such as Meta Quest 3S and Apple Vision Pro, which utilize multiple cameras and sensors to enable real-time interaction between virtual and physical elements. Mixed reality that incorporates haptics has sometimes been referred to as visuo-haptic mixed reality. In virtual reality (VR), the users' perception is completely computer-generated, whereas with augmented reality (AR), it is partially generated and partially from the real world. For example, in architecture, VR can be used to create a walk-through simulation of the inside of a new building; and AR can be used to show a building's structures and systems super-imposed on a real-life view. Another example is through the use of utility applications. Some AR applications, such as Augment, enable users to apply digital objects into real environments, allowing businesses to use augmented reality devices as a way to preview their products in the real world. Similarly, it can also be used to demo what products may look like in an environment for customers, as demonstrated by companies such as Mountain Equipment Co-op or Lowe's who use augmented reality to allow customers to preview what their products might look like at home. Augmented reality (AR) differs from virtual reality (VR) in the sense that in AR, the surrounding environment is 'real' and AR is just adding virtual objects to the real environment. On the other hand, in VR, the surrounding environment is completely virtual and computer generated. A demonstration of how AR layers objects onto the real world can be seen with augmented reality games. WallaMe is an augmented reality game application that allows users to hide messages in real environments, utilizing geolocation technology in order to enable users to hide messages wherever they may wish in the world. In a physics context, the term ""interreality system"" refers to a virtual reality system coupled with its real-world counterpart. A 2007 paper describes an interreality system comprising a real physical pendulum coupled to a pendulum that only exists in virtual reality. This system has two stable states of motion: a ""dual reality"" state in which the motion of the two pendula are uncorrelated, and a ""mixed reality"" state in which the pendula exhibit stable phase-locked motion, which is highly correlated. The use of the terms ""mixed reality"" and ""interreality"" is clearly defined in the context of physics and may be slightly different in other fields, however, it is generally seen as, ""bridging the physical and virtual world"". Recent improvements in AR and VR headsets have made the display quality, field of view, and motion tracking more accurate, which makes augmented experiences more immersive. Improvements in sensor calibration, lightweight optics, and wireless connectivity have also made it easier for users to move around and be comfortable. According to a market analysis, the global market for AR and VR headsets was valued $10.3 billion in 2024 and will be worth more than $105 billion by 2035, with a CAGR of more than 25%. More and more people are using these devices in gaming, healthcare, education, and industrial training because the cost of hardware is going down and the number of content ecosystems is expanding.",0,Wikipedia,Augmented reality,https://en.wikipedia.org/wiki/Augmented_reality,,Augmented_reality,wikipedia_api
human_wiki_0133,"History 1901: Author L. Frank Baum, in his science-fiction novel The Master Key, first mentions the idea of an electronic display/spectacles that overlays data onto real life (in this case 'people'). It is named a 'character marker'. Heads-up displays (HUDs), a precursor technology to augmented reality, were first developed for pilots in the 1950s, projecting simple flight data into their line of sight, thereby enabling them to keep their ""heads up"" and not look down at the instruments. It is a transparent display. 1968: Ivan Sutherland creates the first head-mounted display that has graphics rendered by a computer. 1975: Myron Krueger creates Videoplace to allow users to interact with virtual objects. 1980: The research by Gavan Lintern of the University of Illinois is the first published work to show the value of a heads up display for teaching real-world flight skills. 1980: Steve Mann creates the first wearable computer, a computer vision system with text and graphical overlays on a photographically mediated scene. 1986: Within IBM, Ron Feigenblatt describes the most widely experienced form of AR today (viz. ""magic window,"" e.g. smartphone-based Pokémon Go), use of a small, ""smart"" flat panel display positioned and oriented by hand. 1987: Douglas George and Robert Morris create a working prototype of an astronomical telescope-based ""heads-up display"" system (a precursor concept to augmented reality) which superimposed in the telescope eyepiece, over the actual sky images, multi-intensity star, and celestial body images, and other relevant information. 1990: The term augmented reality is attributed to Thomas P. Caudell, a former Boeing researcher. 1992: Louis Rosenberg developed one of the first functioning AR systems, called Virtual Fixtures, at the United States Air Force Research Laboratory—Armstrong, that demonstrated benefit to human perception. 1992: Steven Feiner, Blair MacIntyre and Doree Seligmann present an early paper on an AR system prototype, KARMA, at the Graphics Interface conference. 1993: Mike Abernathy, et al., report the first use of augmented reality in identifying space debris using Rockwell WorldView by overlaying satellite geographic trajectories on live telescope video. 1993: A widely cited version of the paper above is published in Communications of the ACM – Special issue on computer augmented environments, edited by Pierre Wellner, Wendy Mackay, and Rich Gold. 1993: Loral WDL, with sponsorship from STRICOM, performed the first demonstration combining live AR-equipped vehicles and manned simulators. Unpublished paper, J. Barrilleaux, ""Experiences and Observations in Applying Augmented Reality to Live Training"", 1999. 1995: S. Ravela et al. at University of Massachusetts introduce a vision-based system using monocular cameras to track objects (engine blocks) across views for augmented reality. 1996: General Electric develops system for projecting information from 3D CAD models onto real-world instances of those models. 1998: Spatial augmented reality introduced at University of North Carolina at Chapel Hill by Ramesh Raskar, Greg Welch, Henry Fuchs. 1999: Frank Delgado, Mike Abernathy et al. report successful flight test of LandForm software video map overlay from a helicopter at Army Yuma Proving Ground overlaying video with runways, taxiways, roads and road names. 1999: The US Naval Research Laboratory engages on a decade-long research program called the Battlefield Augmented Reality System (BARS) to prototype some of the early wearable systems for dismounted soldier operating in urban environment for situation awareness and training. 1999: NASA X-38 flown using LandForm software video map overlays at Dryden Flight Research Center. 2000: Rockwell International Science Center demonstrates tetherless wearable augmented reality systems receiving analog video and 3D audio over radio-frequency wireless channels. The systems incorporate outdoor navigation capabilities, with digital horizon silhouettes from a terrain database overlain in real time on the live outdoor scene, allowing visualization of terrain made invisible by clouds and fog. 2004: An outdoor helmet-mounted AR system was demonstrated by Trimble Navigation and the Human Interface Technology Laboratory (HIT lab). 2006: Outland Research develops AR media player that overlays virtual content onto a users view of the real world synchronously with playing music, thereby providing an immersive AR entertainment experience. 2008: Wikitude AR Travel Guide launches on 20 Oct 2008 with the G1 Android phone. 2009: ARToolkit was ported to Adobe Flash (FLARToolkit) by Saqoosha, bringing augmented reality to the web browser. 2012: Launch of Lyteshot, an interactive AR gaming platform that utilizes smart glasses for game data 2013: Niantic releases ""Ingress"", an augmented reality mobile game for iOS and Android operating systems (and a predecessor of Pokémon Go). 2015: Microsoft announced the HoloLens augmented reality headset, which uses various sensors and a processing unit to display virtual imagery over the real world. 2016: Niantic released Pokémon Go for iOS and Android in July 2016. The game quickly became one of the most popular smartphone applications and in turn spikes the popularity of augmented reality games. 2018: Magic Leap launched the Magic Leap One augmented reality headset. Leap Motion announced the Project North Star augmented reality headset, and later released it under an open source license. 2019: Microsoft announced HoloLens 2 with significant improvements in terms of field of view and ergonomics. 2022: Magic Leap launched the Magic Leap 2 headset. 2023: Meta Quest 3, a mixed reality VR headset was developed by Reality Labs, a division of Meta Platforms. In the same year, Apple Vision Pro was released. 2024: Meta Platforms revealed the Orion AR glasses prototype. 2025: Meta Platforms released their Meta Ray-Ban Display glasses, featuring a small AR HUD on the right eye.",0,Wikipedia,Augmented reality,https://en.wikipedia.org/wiki/Augmented_reality,,Augmented_reality,wikipedia_api
human_wiki_0134,"Hardware and displays AR visuals appear on handheld devices (video passthrough) and head-mounted displays (optical see-through or video passthrough). Systems pair a display with sensors (e.g., cameras and IMUs) to register virtual content to the environment; research also explores near-eye optics, projection-based AR, and experimental concepts such as contact-lens or retinal-scanned displays.",0,Wikipedia,Augmented reality,https://en.wikipedia.org/wiki/Augmented_reality,,Augmented_reality,wikipedia_api
human_wiki_0135,"Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis). Biology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science. Life on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss.",0,Wikipedia,Biology,https://en.wikipedia.org/wiki/Biology,,Biology,wikipedia_api
human_wiki_0136,"Etymology From Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to live) and λογία (logia) 'study of'. The compound appears in the title of Volume 3 of Michael Christoph Hanow's Philosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologia, published in 1766. The term biology in its modern sense appears to have been introduced independently by Thomas Beddoes (in 1799), Karl Friedrich Burdach (in 1800), Gottfried Reinhold Treviranus (Biologie oder Philosophie der lebenden Natur, 1802) and Jean-Baptiste Lamarck (Hydrogéologie, 1802).",0,Wikipedia,Biology,https://en.wikipedia.org/wiki/Biology,,Biology,wikipedia_api
human_wiki_0137,"History The earliest of roots of science, which included medicine, can be traced to ancient Egypt and Mesopotamia in around 3000 to 1200 BCE. Their contributions shaped ancient Greek natural philosophy. Ancient Greek philosophers such as Aristotle (384–322 BCE) contributed extensively to the development of biological knowledge. He explored biological causation and the diversity of life. His successor, Theophrastus, began the scientific study of plants. Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought. Biology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining. Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory. Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735, and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent.",0,Wikipedia,Biology,https://en.wikipedia.org/wiki/Biology,,Biology,wikipedia_api
human_wiki_0138,"Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution. The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions. The basis for modern genetics began with the work of Gregor Mendel in 1865. This outlined the principles of biological inheritance. However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics. In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome.",0,Wikipedia,Biology,https://en.wikipedia.org/wiki/Biology,,Biology,wikipedia_api
human_wiki_0139,"Chemical basis Atoms and molecules All organisms are made up of chemical elements; oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life. Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions.",0,Wikipedia,Biology,https://en.wikipedia.org/wiki/Biology,,Biology,wikipedia_api
human_wiki_0140,"Nuclear power is the use of nuclear reactions to produce electricity. Nuclear power can be obtained from nuclear fission, nuclear decay and nuclear fusion reactions. Presently, the vast majority of electricity from nuclear power is produced by nuclear fission of uranium and plutonium in nuclear power plants. Nuclear decay processes are used in niche applications such as radioisotope thermoelectric generators in some space probes such as Voyager 2. Reactors producing controlled fusion power have been operated since 1958 but have yet to generate net power and are not expected to be commercially available in the near future. The first nuclear power plant was built in the 1950s. The global installed nuclear capacity grew to 100 GW in the late 1970s, and then expanded during the 1980s, reaching 300 GW by 1990. The 1979 Three Mile Island accident in the United States and the 1986 Chernobyl disaster in the Soviet Union resulted in increased regulation and public opposition to nuclear power plants. Nuclear power plants supplied 2,602 terawatt hours (TWh) of electricity in 2023, equivalent to about 9% of global electricity generation, and were the second largest low-carbon power source after hydroelectricity. As of November 2025, there are 416 civilian fission reactors in the world, with overall capacity of 376 GW, 63 under construction and 87 planned, with a combined capacity of 66 GW and 84 GW, respectively. The United States has the largest fleet of nuclear reactors, generating almost 800 TWh per year with an average capacity factor of 92%. The average global capacity factor is 89%. Most new reactors under construction are generation III reactors in Asia.",0,Wikipedia,Nuclear power,https://en.wikipedia.org/wiki/Nuclear_power,,Nuclear_power,wikipedia_api
human_wiki_0141,"Nuclear power is a safe, sustainable energy source that reduces carbon emissions. This is because nuclear power generation causes one of the lowest levels of fatalities per unit of energy generated compared to other energy sources. ""Economists estimate that each nuclear plant built could save more than 800,000 life years."" Coal, petroleum, natural gas and hydroelectricity have each caused more fatalities per unit of energy due to air pollution and accidents. Nuclear power plants also emit no greenhouse gases and result in less life-cycle carbon emissions than common sources of renewable energy. The radiological hazards associated with nuclear power are the primary motivations of the anti-nuclear movement, which contends that nuclear power poses threats to people and the environment, citing the potential for accidents like the Fukushima nuclear disaster in Japan in 2011, and is too expensive to deploy when compared to alternative sustainable energy sources.",0,Wikipedia,Nuclear power,https://en.wikipedia.org/wiki/Nuclear_power,,Nuclear_power,wikipedia_api
human_wiki_0142,"History Origins The process of nuclear fission was discovered in 1938 after over four decades of work on the science of radioactivity and the elaboration of new nuclear physics that described the components of atoms. Soon after the discovery of the fission process, it was realized that neutrons released by a fissioning nucleus could, under the right conditions, induce fissions in nearby nuclei, thus initiating a self-sustaining chain reaction. Once this was experimentally confirmed in 1939, scientists in many countries petitioned their governments for support for nuclear fission research, just on the cusp of World War II, in order to develop a nuclear weapon. In the United States, these research efforts led to the creation of the first human-made nuclear reactor, the Chicago Pile-1 under the Stagg Field stadium at the University of Chicago, which achieved criticality on December 2, 1942. The reactor's development was part of the Manhattan Project, the Allied effort to create atomic bombs during World War II. It led to the building of larger single-purpose production reactors for the production of weapons-grade plutonium for use in the first nuclear weapons. The United States tested the first nuclear weapon in July 1945, the Trinity test, and the atomic bombings of Hiroshima and Nagasaki happened one month later.",0,Wikipedia,Nuclear power,https://en.wikipedia.org/wiki/Nuclear_power,,Nuclear_power,wikipedia_api
human_wiki_0143,"Despite the military nature of the first nuclear devices, there was strong optimism in the 1940s and 1950s that nuclear power could provide cheap and endless energy. Electricity was generated for the first time by a nuclear reactor on December 20, 1951, at the EBR-I experimental station near Arco, Idaho, which initially produced about 100 kW. In 1953, American President Dwight Eisenhower gave his ""Atoms for Peace"" speech at the United Nations, emphasizing the need to develop ""peaceful"" uses of nuclear power quickly. This was followed by the Atomic Energy Act of 1954 which allowed rapid declassification of U.S. reactor technology and encouraged development by the private sector.",0,Wikipedia,Nuclear power,https://en.wikipedia.org/wiki/Nuclear_power,,Nuclear_power,wikipedia_api
human_wiki_0144,"First power generation The first organization to develop practical nuclear power was the U.S. Navy, with the S1W reactor for the purpose of propelling submarines and aircraft carriers. The first nuclear-powered submarine, USS Nautilus, was put to sea in January 1954. The S1W reactor was a pressurized water reactor. This design was chosen because it was simpler, more compact, and easier to operate compared to alternative designs, thus more suitable to be used in submarines. This decision would result in the PWR being the reactor of choice also for power generation, thus having a lasting impact on the civilian electricity market in the years to come. On June 27, 1954, the Obninsk Nuclear Power Plant in the USSR became the world's first nuclear power plant to generate electricity for a power grid, producing around 5 megawatts of electric power. The world's first commercial nuclear power station, Calder Hall at Windscale, England was connected to the national power grid on 27 August 1956. In common with a number of other generation I reactors, the plant had the dual purpose of producing electricity and plutonium-239, the latter for the nascent nuclear weapons program in Britain.",0,Wikipedia,Nuclear power,https://en.wikipedia.org/wiki/Nuclear_power,,Nuclear_power,wikipedia_api
human_wiki_0145,"The cell is the basic structural and functional unit of all forms of life or organisms. The term comes from the Latin word cellula meaning 'small room'. A biological cell basically consists of a semipermeable cell membrane enclosing cytoplasm that contains genetic material. Most cells are only visible under a microscope. Except for highly-differentiated cell types (examples include red blood cells and gametes) most cells are capable of replication, and protein synthesis. Some types of cell are motile. Cells emerged on Earth about four billion years ago.  All organisms are grouped into prokaryotes, and eukaryotes. Prokaryotes are single-celled, and include archaea, and bacteria. Eukaryotes can be single-celled or multicellular, and include protists, plants, animals, most types of fungi, and some species of algae. All multicellular organisms are made up of many different types of cell. The diploid cells that make up the body of a plant or animal are known as somatic cells, and in animals excludes the haploid gametes. Prokaryotic cells lack the membrane-bound nucleus present in eukaryotic cells, and instead have a nucleoid region. In eukaryotic cells the nucleus is enclosed in the nuclear membrane. Eukaryotic cells contain other membrane-bound organelles such as mitochondria, which provide energy for cell functions, and chloroplasts, in plants that create sugars by photosynthesis. Other non-membrane-bound organelles may be proteinaceous such as the ribosomes present (though different) in both groups. A unique membrane-bound prokaryotic organelle the magnetosome has been discovered in magnetotactic bacteria. Cells were discovered by Robert Hooke in 1665, who named them after their resemblance to cells in a monastery. Cell theory, developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all organisms, and that all cells come from pre-existing cells.",0,Wikipedia,Cell (biology),https://en.wikipedia.org/wiki/Cell_(biology),,Cell_(biology),wikipedia_api
human_wiki_0146,"Types Organisms are broadly grouped into eukaryotes, and prokaryotes. Eukaryotic cells possess a membrane-bound nucleus, and prokaryotic cells lack a nucleus but have a nucleoid region. Prokaryotes are single-celled organisms, whereas eukaryotes can be either single-celled or multicellular. Single-celled eukaryotes include microalgae such as diatoms. Multicellular eukaryotes include all animals, and plants, most fungi, and some species of algae.",0,Wikipedia,Cell (biology),https://en.wikipedia.org/wiki/Cell_(biology),,Cell_(biology),wikipedia_api
human_wiki_0147,"Prokaryotes All prokaryotes are single-celled and include bacteria and archaea, two of the three domains of life. Prokaryotic cells were likely the first form of life on Earth, characterized by having vital biological processes including cell signaling. They are simpler and smaller than eukaryotic cells, lack a nucleus, and the other usually present membrane-bound organelles. Prokaryotic organelles are less complex, and are typically non-membrane-bound. All prokaryotic cells  secrete different substances from their membranes, including exoenzymes, and extracellular polymeric substances. Most prokaryotes are the smallest of all organisms, ranging from 0.5 to 2.0 μm in diameter. The largest bacterium known, Thiomargarita magnifica, is visible to the naked eye with an average length of 1 cm, but can be as much as 2 cm",0,Wikipedia,Cell (biology),https://en.wikipedia.org/wiki/Cell_(biology),,Cell_(biology),wikipedia_api
human_wiki_0148,"Bacteria Bacteria are enclosed in a cell envelope, that protects the interior from the exterior. It generally consists of a plasma membrane covered by a cell wall which, for some bacteria, is covered by a third gelatinous layer called a bacterial capsule.  The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as Bacillus anthracis or hyaluronic acid as in streptococci. Mycoplasma only possess the cell membrane. The cell envelope gives rigidity to the cell and separates the interior of the cell from its environment, serving as a protective mechanical and chemical filter. The cell wall consists of peptidoglycan and acts as an additional barrier against exterior forces. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. It also prevents the cell from expanding and bursting (cytolysis) from osmotic pressure due to a hypotonic environment.  The DNA of a bacterium typically consists of a single circular chromosome that is in direct contact with the cytoplasm in a region called the nucleoid. Some bacteria contain multiple circular or even linear chromosomes. The cytoplasm also contains ribosomes and various inclusions where transcription takes place alongside translation. Extrachromosomal DNA as plasmids, are usually circular and encode additional genes, such as those of antibiotic resistance. Linear bacterial plasmids have been identified in several species of spirochete bacteria, including species of Borrelia which causes Lyme disease. The prokaryotic cytoskeleton in bacteria is involved in the maintenance of cell shape, polarity and cytokinesis. Compartmentalization is a feature of eukaryotic cells but some species of bacteria, have protein-based organelle-like microcompartments such as gas vesicles, and carboxysomes, and encapsulin nanocompartments. Certain membrane-bound prokaryotic organelles have also been discovered. They include the magnetosome of magnetotactic bacteria, and the anammoxosome of anammox bacteria.  Cell-surface appendages can include flagella, and pili, protein structures that facilitate movement and communication between cells. The flagellum stretches from the cytoplasm through the cell membrane and extrudes through the cell wall. Fimbriae are short attachment pili, the other type of pilus is the longer conjugative type.  Fimbriae are formed of an antigenic protein called pilin, and are responsible for the attachment of bacteria to specific receptors on host cells.",0,Wikipedia,Cell (biology),https://en.wikipedia.org/wiki/Cell_(biology),,Cell_(biology),wikipedia_api
human_wiki_0149,"Archaea Archaea are enclosed in a cell envelope consisting of a plasma membrane and a cell wall. An exception to this is the Thermoplasma that only has the cell membrane. The cell membranes of archaea are unique, consisting of ether-linked lipids. The prokaryotic cytoskeleton has homologues of eukaryotic actin and tubulin. A unique form of metabolism in the archaean is methanogenesis. Their cell-surface appendage equivalent of the flagella is the differently structured and unique archaellum. The DNA is contained in a circular chromosome in direct contact with the cytoplasm, in a region known as the nucleoid. Ribosomes are also found freely in the cytoplasm, or attached to the cell membrane where DNA processing takes place. The archaea are noted for their extremophile species, and many are selectively evolved to thrive in extreme heat, cold, acidic, alkaline, or high salt conditions. There are no known archaean pathogens.",0,Wikipedia,Cell (biology),https://en.wikipedia.org/wiki/Cell_(biology),,Cell_(biology),wikipedia_api
human_wiki_0150,"Mars is the fourth planet from the Sun. It is also known as the ""Red Planet"", for its orange-red appearance. Mars is a desert-like rocky planet with a tenuous atmosphere that is primarily carbon dioxide (CO2). At the average surface level the atmospheric pressure is a few thousandths of Earth's, atmospheric temperature ranges from −153 to 20 °C (−243 to 68 °F), and cosmic radiation is high. Mars retains some water, in the ground as well as thinly in the atmosphere, forming cirrus clouds, fog, frost, larger polar regions of permafrost and ice caps (with seasonal CO2 snow), but no bodies of liquid surface water. Its surface gravity is roughly a third of Earth's or double that of the Moon. Its diameter, 6,779 km (4,212 mi), is about half the Earth's, or twice the Moon's, and its surface area is the size of all the dry land of Earth. Fine dust is prevalent across the surface and the atmosphere, being picked up and spread at the low Martian gravity even by the weak wind of the tenuous atmosphere. The terrain of Mars roughly follows a north-south divide, the Martian dichotomy, with the northern hemisphere mainly consisting of relatively flat, low lying plains, and the southern hemisphere of cratered highlands. Geologically, the planet is fairly active with marsquakes trembling underneath the ground, but also hosts many enormous volcanoes that are extinct (the tallest is Olympus Mons, 21.9 km or 13.6 mi tall), as well as one of the largest canyons in the Solar System (Valles Marineris, 4,000 km or 2,500 mi long). Mars has two natural satellites that are small and irregular in shape: Phobos and Deimos. With a significant axial tilt of 25 degrees, Mars experiences seasons, like Earth (which has an axial tilt of 23.5 degrees). A Martian solar year is equal to 1.88 Earth years (687 Earth days), a Martian solar day (sol) is equal to 24.6 hours. Mars formed along with the other planets approximately 4.5 billion years ago. During the martian Noachian period (4.5 to 3.5 billion years ago), its surface was marked by meteor impacts, valley formation, erosion, the possible presence of water oceans and the loss of its magnetosphere. The Hesperian period (beginning 3.5 billion years ago and ending 3.3–2.9 billion years ago) was dominated by widespread volcanic activity and flooding that carved immense outflow channels. The Amazonian period, which continues to the present, is the currently dominating and remaining influence on geological processes. Because of Mars's geological history, the possibility of past or present life on Mars remains an area of active scientific investigation, with some possible traces needing further examination. Being visible with the naked eye in Earth's sky as a red wandering star, Mars has been observed throughout history, acquiring diverse associations in different cultures. In 1963 the first flight to Mars took place with Mars 1, but communication was lost en route. The first successful flyby exploration of Mars was conducted in 1965 with Mariner 4. In 1971 Mariner 9 entered orbit around Mars, being the first spacecraft to orbit any body other than the Moon, Sun or Earth; following in the same year were the first uncontrolled impact (Mars 2) and first successful landing (Mars 3) on Mars. Probes have been active on Mars continuously since 1997. At times, more than ten probes have simultaneously operated in orbit or on the surface, more than at any other planet beyond Earth. Mars is an often proposed target for future crewed exploration missions, though no such mission is currently planned.",0,Wikipedia,Mars,https://en.wikipedia.org/wiki/Mars,,Mars,wikipedia_api
human_wiki_0151,"Natural history Formation Scientists have theorized that during the Solar System's formation, Mars was created as the result of a random process of run-away accretion of material from the protoplanetary disk that orbited the Sun. Mars has many distinctive chemical features caused by its position in the Solar System. Elements with comparatively low boiling points, such as chlorine, phosphorus, and sulfur, are much more common on Mars than on Earth; these elements were probably pushed outward by the young Sun's energetic solar wind.",0,Wikipedia,Mars,https://en.wikipedia.org/wiki/Mars,,Mars,wikipedia_api
human_wiki_0152,"Late Heavy Bombardment After the formation of the planets, the inner Solar System may have been subjected to the so-called Late Heavy Bombardment. About 60% of the surface of Mars shows a record of impacts from that era, whereas much of the remaining surface is probably underlain by immense impact basins caused by those events. However, more recent modeling has disputed the existence of the Late Heavy Bombardment. There is evidence of an enormous impact basin in the Northern Hemisphere of Mars, spanning 10,600 by 8,500 kilometres (6,600 by 5,300 mi), or roughly four times the size of the Moon's South Pole–Aitken basin, which would be the largest impact basin yet discovered if confirmed. It has been hypothesized that the basin was formed when Mars was struck by a Pluto-sized body about four billion years ago. The event, thought to be the cause of the Martian hemispheric dichotomy, created the smooth Borealis basin that covers 40% of the planet. A 2023 study shows evidence, based on the orbital inclination of Deimos (a small moon of Mars), that Mars may once have had a ring system 3.5 billion years to 4 billion years ago. This ring system may have been formed from a moon, 20 times more massive than Phobos, orbiting Mars billions of years ago; and Phobos would be a remnant of that ring.",0,Wikipedia,Mars,https://en.wikipedia.org/wiki/Mars,,Mars,wikipedia_api
human_wiki_0153,"Noachian period: Formation of the oldest extant surfaces of Mars, 4.5 to 3.5 billion years ago. Noachian age surfaces are scarred by many large impact craters. The Tharsis bulge, a volcanic upland, is thought to have formed during this period, with extensive flooding by liquid water late in the period. Named after Noachis Terra. Hesperian period: 3.5 to between 3.3 and 2.9 billion years ago. The Hesperian period is marked by the formation of extensive lava plains. Named after Hesperia Planum. Amazonian period: between 3.3 and 2.9 billion years ago to the present. Amazonian regions have few meteorite impact craters but are otherwise quite varied. Olympus Mons formed during this period, with lava flows elsewhere on Mars. Named after Amazonis Planitia.",0,Wikipedia,Mars,https://en.wikipedia.org/wiki/Mars,,Mars,wikipedia_api
human_wiki_0154,"Recent geological activity Geological activity is still taking place on Mars. The Athabasca Valles is home to sheet-like lava flows created about 200 million years ago. Water flows in the grabens called the Cerberus Fossae occurred less than 20 million years ago, indicating equally recent volcanic intrusions. The Mars Reconnaissance Orbiter has captured images of avalanches.",0,Wikipedia,Mars,https://en.wikipedia.org/wiki/Mars,,Mars,wikipedia_api
human_wiki_0155,"Construction is the process involved in delivering buildings, infrastructure, industrial facilities, and associated activities through to the end of their life. It typically starts with planning, financing, and design that continues until the asset is built and ready for use. Construction also covers repairs and maintenance work, any works to expand, extend and improve the asset, and its eventual demolition, dismantling or decommissioning. The construction industry contributes significantly to many countries' gross domestic products (GDP). Global expenditure on construction activities was about $4 trillion in 2012. In 2022, expenditure on the construction industry exceeded $11 trillion a year, equivalent to about 13 percent of global GDP. This spending was forecasted to rise to around $14.8 trillion in 2030. The construction industry promotes economic development and brings many non-monetary benefits to many countries, but it is one of the most hazardous industries. For example, about 20% (1,061) of US industry fatalities in 2019 happened in construction.",0,Wikipedia,Construction,https://en.wikipedia.org/wiki/Construction,,Construction,wikipedia_api
human_wiki_0156,"Etymology ""Construction"" stems from the Latin word constructio (which comes from com- ""together"" and struere ""to pile up"") as well as Old French construction. ""To construct"" is a verb: the act of building. The noun is ""construction"": how something is built or the nature of its structure.",0,Wikipedia,Construction,https://en.wikipedia.org/wiki/Construction,,Construction,wikipedia_api
human_wiki_0157,"History The first huts and shelters were constructed by hand or with simple tools. As cities grew during the Bronze Age, a class of professional craftsmen, like bricklayers and carpenters, appeared. Occasionally, slaves were used for construction work. In the Middle Ages, the artisan craftsmen were organized into guilds. In the 19th century, steam-powered machinery appeared, and later, diesel- and electric-powered vehicles such as cranes, excavators and bulldozers. Fast-track construction has been increasingly popular in the 21st century. Some estimates suggest that 40% of construction projects are now fast-track construction.",0,Wikipedia,Construction,https://en.wikipedia.org/wiki/Construction,,Construction,wikipedia_api
human_wiki_0158,"Building construction is usually further divided into residential and non-residential. Infrastructure, also called 'heavy civil' or 'heavy engineering', includes large public works, dams, bridges, highways, railways, water or wastewater and utility distribution. Industrial construction includes offshore construction (mainly of energy installations), mining and quarrying, refineries, chemical processing, mills and manufacturing plants. The industry can also be classified into sectors or markets. For example, Engineering News-Record (ENR), a US-based construction trade magazine, has compiled and reported data about the size of design and construction contractors. In 2014, it split the data into nine market segments: transportation, petroleum, buildings, power, industrial, water, manufacturing, sewage/waste, telecom, hazardous waste, and a tenth category for other projects. ENR used data on transportation, sewage, hazardous waste and water to rank firms as heavy contractors. The Standard Industrial Classification and the newer North American Industry Classification System classify companies that perform or engage in construction into three subsectors: building construction, heavy and civil engineering construction, and specialty trade contractors. There are also categories for professional services firms (e.g., engineering, architecture, surveying, project management).",0,Wikipedia,Construction,https://en.wikipedia.org/wiki/Construction,,Construction,wikipedia_api
human_wiki_0159,"Building construction Building construction is the process of adding structures to areas of land, also known as real property sites. Typically, a project is instigated by or with the owner of the property (who may be an individual or an organisation); occasionally, land may be compulsorily purchased from the owner for public use.",0,Wikipedia,Construction,https://en.wikipedia.org/wiki/Construction,,Construction,wikipedia_api
human_wiki_0160,"Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.",0,Wikipedia,Artificial intelligence,https://en.wikipedia.org/wiki/Artificial_intelligence,,Artificial_intelligence,wikipedia_api
human_wiki_0161,Goals The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.,0,Wikipedia,Artificial intelligence,https://en.wikipedia.org/wiki/Artificial_intelligence,,Artificial_intelligence,wikipedia_api
human_wiki_0162,"Reasoning and problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a ""combinatorial explosion"": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.",0,Wikipedia,Artificial intelligence,https://en.wikipedia.org/wiki/Artificial_intelligence,,Artificial_intelligence,wikipedia_api
human_wiki_0163,"Knowledge representation Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining ""interesting"" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.",0,Wikipedia,Artificial intelligence,https://en.wikipedia.org/wiki/Artificial_intelligence,,Artificial_intelligence,wikipedia_api
human_wiki_0164,"Planning and decision-making An ""agent"" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the ""utility"") that measures how much the agent prefers it. For each possible action, it can calculate the ""expected utility"": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is ""unknown"" or ""unobservable"") and it may not know for certain what will happen after each possible action (it is not ""deterministic""). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.",0,Wikipedia,Artificial intelligence,https://en.wikipedia.org/wiki/Artificial_intelligence,,Artificial_intelligence,wikipedia_api
human_wiki_0165,"A black hole is an astronomical body so compact that its gravity prevents anything from escaping, even light. Albert Einstein's theory of general relativity predicts that a sufficiently compact mass will form a black hole. The boundary of no escape is called the event horizon. In general relativity, a black hole's event horizon seals an object's fate but produces no locally detectable change when crossed. In many ways, a black hole acts like an ideal black body, as it reflects no light. Quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is of the order of billionths of a kelvin for stellar black holes, making it essentially impossible to observe directly. Objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. In 1916, Karl Schwarzschild found the first modern solution of general relativity that would characterise a black hole. Due to his influential research, the Schwarzschild metric is named after him. David Finkelstein, in 1958, first published the interpretation of ""black hole"" as a region of space from which nothing can escape. Black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. The first black hole known was Cygnus X-1, identified by several researchers independently in 1971. Black holes typically form when massive stars collapse at the end of their life cycle. After a black hole has formed, it can grow by absorbing mass from its surroundings. Supermassive black holes of millions of solar masses may form by absorbing other stars and merging with other black holes, or via direct collapse of gas clouds. There is consensus that supermassive black holes exist in the centres of most galaxies. The presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter falling toward a black hole can form an accretion disk of infalling plasma, heated by friction and emitting light. In extreme cases, this creates a quasar, some of the brightest objects in the universe. Stars passing too close to a supermassive black hole can be shredded into streamers that shine very brightly before being ""swallowed"". If other stars are orbiting a black hole, their orbits can be used to determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses.",0,Wikipedia,Black hole,https://en.wikipedia.org/wiki/Black_hole,,Black_hole,wikipedia_api
human_wiki_0166,"History The idea of a body so massive that even light could not escape was briefly proposed by English astronomical pioneer and clergyman John Michell and independently by French scientist Pierre-Simon Laplace. Both scholars proposed very large stars, analogous to modern models of supermassive black holes. Michell's idea, in a short part of a letter published in 1784, calculated that a star with the same density but 500 times the radius of the sun would not let any emitted light escape; the surface escape velocity would exceed the speed of light. Michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies. In 1796, Laplace mentioned that a star could be invisible if it were sufficiently large while speculating on the origin of the Solar System in his book Exposition du Système du Monde. Franz Xaver von Zach asked Laplace for a mathematical analysis, which Laplace provided and published in a journal edited by von Zach. Scholars of the time were initially excited by the proposal that giant but invisible 'dark stars' might be hiding in plain view, but enthusiasm dampened in the early 19th century, when light was discovered to be wavelike. Since light was understood as a wave rather than a particle, it was unclear what, if any, influence gravity would have on escaping light waves.",0,Wikipedia,Black hole,https://en.wikipedia.org/wiki/Black_hole,,Black_hole,wikipedia_api
human_wiki_0167,"General relativity In 1911, Albert Einstein published a paper about the properties of acceleration in special relativity, noting that an object accelerating through space outside of a gravitational field would be physically indistinguishable from an object in a static gravitational field. The paper also predicted the deflection of light by massive bodies. In 1915, Einstein refined these ideas into his general theory of relativity, which explained how matter affects spacetime, which in turn affects the motion of other matter. This theory formed the basis for black hole physics, although Einstein himself would later try, and fail, to refute the idea that black holes could exist. Only a few months after Einstein published the field equations describing general relativity, Karl Schwarzschild found a solution describing the gravitational field of a point mass and a spherical mass. A few months after Schwarzschild, Johannes Droste, a student of Hendrik Lorentz, independently gave the same solution for the point mass and wrote more extensively about its properties. At a certain radius from the center of the mass, the Schwarzschild solution became singular, meaning that some of the terms in the Einstein equations became infinite. The nature of this radius, which later became known as the Schwarzschild radius, was not understood at the time. In 1924, Arthur Eddington showed that the singularity disappeared after a change of coordinates. In 1933, Georges Lemaître realised that this meant the singularity at the Schwarzschild radius was a non-physical coordinate singularity. Arthur Eddington commented on the possibility of a star with mass compressed to the Schwarzschild radius in a 1926 book, noting that Einstein's theory allows us to rule out overly large densities for visible stars like Betelgeuse because the extreme force of gravity would make light unable to escape from the star, redshift the star's entire light spectrum out of existence, and ""produce so much curvature of the spacetime metric that space would close up around the star, leaving us outside (i.e., nowhere)."" In 1931, using special relativity, Subrahmanyan Chandrasekhar calculated that a non-rotating body of electron-degenerate matter above a certain limiting mass (now called the Chandrasekhar limit at 1.4 M☉) has no stable solutions. His arguments were opposed by many of his contemporaries like Eddington and Lev Landau, who argued that some yet unknown mechanism would stop the collapse. They were partially correct: a white dwarf slightly more massive than the Chandrasekhar limit will collapse into a neutron star, which is itself stable. In 1939, based on Chandrasekhar's reasoning, Robert Oppenheimer and George Volkoff predicted that neutron stars above another mass limit, now known as the Tolman–Oppenheimer–Volkoff limit, would collapse further, concluding that no law of physics was likely to intervene and stop at least some stars from collapsing to black holes. Their original calculations, based on the Pauli exclusion principle, gave it as 0.7 M☉. Subsequent consideration of neutron-neutron repulsion mediated by the strong force raised the estimate to approximately 1.5 M☉ to 3.0 M☉. Observations of the neutron star merger GW170817, which is thought to have generated a black hole shortly afterward, have refined the TOV limit estimate to ~2.17 M☉. Oppenheimer and his co-authors interpreted the singularity at the boundary of the Schwarzschild radius as indicating that this was the boundary of a bubble in which time stopped. This is a valid point of view for external observers, but not for infalling observers. John Wheeler later described black holes viewed from an external reference frame as ""frozen stars"" because an outside observer would see the surface of the star frozen in time due to gravitational time dilation, never to fully collapse. Also in 1939, Einstein attempted to prove that black holes were impossible in his publication ""On a Stationary System with Spherical Symmetry Consisting of Many Gravitating Masses"", using his theory of general relativity to defend his argument. Months later, Oppenheimer and his student Hartland Snyder provided the Oppenheimer–Snyder model in their paper ""On Continued Gravitational Contraction"", which predicted the existence of black holes. In the paper, which made no reference to Einstein's recent publication, Oppenheimer and Snyder used Einstein's own theory of general relativity to show the conditions on how a black hole could develop, for the first time in contemporary physics. In 1958, David Finkelstein identified the Schwarzschild surface as an event horizon, calling it ""a perfect unidirectional membrane: causal influences can cross it in only one direction"". In this sense, events that occur inside of the black hole cannot affect events that occur outside of the black hole. Finkelstein created a new reference frame to include the point of view of infalling observers. Finkelstein's solution extended the Schwarzschild solution for the future of observers falling into a black hole. A similar concept had already been found by Martin Kruskal, but its significance had not been fully understood at the time.",0,Wikipedia,Black hole,https://en.wikipedia.org/wiki/Black_hole,,Black_hole,wikipedia_api
human_wiki_0168,"Golden age The era from the mid-1960s to the mid-1970s was the ""golden age of black hole research"", when general relativity and black holes became mainstream subjects of research. In this period, more general black hole solutions were found. In 1963, Roy Kerr found the exact solution for a rotating black hole. Two years later, Ezra Newman found the cylindrically symmetric solution for a black hole that is both rotating and electrically charged. In 1967, Werner Israel found that the Schwarzschild solution was the only possible solution for a nonspinning, uncharged black hole, and couldn't have any additional parameters. In that sense, a Schwarzschild black hole would be defined by its mass alone, and any two Schwarzschild black holes with the same mass would be identical. Israel later found that Reissner-Nordstrom black holes were only defined by their mass and electric charge, while Brandon Carter discovered that Kerr black holes only had two degrees of freedom, mass and spin. Together, these findings became known as the no-hair theorem, which states that a stationary black hole is completely described by the three parameters of the Kerr–Newman metric: mass, angular momentum, and electric charge. At first, it was suspected that the strange mathematical singularities found in each of the black hole solutions only appeared due to the assumption that a black hole would be perfectly spherically symmetric, and therefore the singularities would not appear in generic situations where black holes would not necessarily be symmetric. This view was held in particular by Vladimir Belinski, Isaak Khalatnikov, and Evgeny Lifshitz, who tried to prove that no singularities appear in generic solutions, although they would later reverse their positions. However, in 1965, Roger Penrose proved that general relativity without quantum mechanics requires that singularities appear in all black holes. Shortly afterwards, Hawking generalized Penrose's solution to find that in all but a few physically infeasible scenarios, a cosmological Big Bang singularity is inevitable unless quantum gravity intervenes. For his work, Penrose received half of the 2020 Nobel Prize in Physics, Hawking having died in 2018. Astronomical observations also made great strides during this era. In 1967, Antony Hewish and Jocelyn Bell Burnell discovered pulsars and by 1969, these were shown to be rapidly rotating neutron stars. Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities, but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse. Based on observations in Greenwich and Toronto in the early 1970s, Cygnus X-1, a galactic X-ray source discovered in 1964, became the first astronomical object commonly accepted to be a black hole. Work by James Bardeen, Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation of black hole thermodynamics. These laws describe the behaviour of a black hole in close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. The analogy was completed when Hawking, in 1974, showed that quantum field theory implies that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole, predicting the effect now known as Hawking radiation.",0,Wikipedia,Black hole,https://en.wikipedia.org/wiki/Black_hole,,Black_hole,wikipedia_api
human_wiki_0169,"Modern research and observation The first strong evidence for black holes came from combined X-ray and optical observations of Cygnus X-1 in 1972. The x-ray source, located in the Cygnus constellation, was discovered through a survey by two suborbital rockets,  as the blocking of x-rays by Earth's atmosphere makes it difficult to detect them from the ground. Unlike stars or pulsars, Cygnus X-1 was not associated with any prominent radio or optical source. In 1972, Louise Webster, Paul Murdin, and, independently, Charles Thomas Bolton, found that Cygnus X-1 was actually in a binary system with the supergiant star HDE 226868. Using the emission patterns of the visible star, both research teams found that the mass of Cygnus X-1 was likely too large to be a white dwarf or neutron star, indicating that it was probably a black hole. Further research strengthened their hypothesis. While Cygnus X-1, a stellar-mass black hole, was generally accepted by the scientific community as a black hole by the end of 1973, it would be decades before a supermassive black hole would gain the same broad recognition. Although, as early as the 1960s, physicists such as Donald Lynden-Bell and Martin Rees had suggested that powerful quasars in the center of galaxies were powered by accreting supermassive black holes, little observational proof existed at the time. However, the Hubble Space Telescope, launched decades later, found that supermassive black holes were not only present in these active galactic nuclei, but that supermassive black holes in the center of galaxies were ubiquitous: Almost every galaxy had a supermassive black hole at its center, many of which were quiescent. In 1999, David Merritt proposed the M–sigma relation, which related the dispersion of the velocity of matter in the center bulge of a galaxy to the mass of the supermassive black hole at its core. Subsequent studies confirmed this correlation. Around the same time, based on telescope observations of the velocities of stars at the center of the Milky Way galaxy, independent work groups led by Andrea Ghez and Reinhard Genzel concluded that the compact radio source in the center of the galaxy, Sagittarius A*, was likely a supermassive black hole. In 2020, Ghez and Genzel won the Nobel Prize in Physics for their prediction.",0,Wikipedia,Black hole,https://en.wikipedia.org/wiki/Black_hole,,Black_hole,wikipedia_api
human_wiki_0170,"Anthropology is the scientific study of humanity that crosses biology and sociology, concerned with human behavior, human biology, cultures, societies, and linguistics, in both the present and past, including archaic humans. Social anthropology studies patterns of behaviour, while cultural anthropology studies cultural meaning, including norms and values. The term sociocultural anthropology is commonly used today. Linguistic anthropology studies how language influences social life. Biological (or physical) anthropology studies the biology and evolution of humans and their close primate relatives. Archaeology, often referred to as the ""anthropology of the past,"" explores human activity by examining physical remains. In North America and Asia, it is generally regarded as a branch of anthropology, whereas in Europe, it is considered either an independent discipline or classified under related fields like history and palaeontology.",0,Wikipedia,Anthropology,https://en.wikipedia.org/wiki/Anthropology,,Anthropology,wikipedia_api
human_wiki_0171,"Etymology The abstract noun anthropology is first attested in reference to history. Its present use first appeared in Renaissance Germany in the works of Magnus Hundt and Otto Casmann. Their Neo-Latin anthropologia derived from the combining forms of the Greek words ánthrōpos (ἄνθρωπος, ""human"") and lógos (λόγος, ""study""). Its adjectival form appeared in the works of Aristotle. It began to be used in English, possibly via French Anthropologie, by the early 18th century.",0,Wikipedia,Anthropology,https://en.wikipedia.org/wiki/Anthropology,,Anthropology,wikipedia_api
human_wiki_0172,"Anthropology, that is to say the science that treats of man, is divided ordinarily and with reason into Anatomy, which considers the body and the parts, and Psychology, which speaks of the soul. Sporadic use of the term for some of the subject matter occurred subsequently, including its use by Étienne Serres in 1839 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the French National Museum of Natural History by Jean Louis Armand de Quatrefages de Bréau. Various short-lived organizations of anthropologists had already been formed. The Société Ethnologique de Paris, the first to use the term ethnology, was formed in 1839 and focused on methodically studying human races. After the death of its founder, William Frédéric Edwards, in 1842, it gradually declined in activity until it eventually dissolved in 1862. Meanwhile, the Ethnological Society of New York, now the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society. These anthropologists were liberal, anti-slavery, and pro-human rights. They maintained international connections. Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in diverse fields such as anatomy, linguistics, and ethnology, started making feature-by-feature comparisons of their subject matters, and were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then. For them, the publication of Charles Darwin's On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild. Darwin and Wallace unveiled evolution in the late 1850s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris in 1859. When he read Darwin, he became an immediate convert to Transformisme, as the French called evolutionism. His definition now became ""the study of the human group, considered as a whole, in its details, and in relation to the rest of nature"". Broca, being what today would be called a neurosurgeon, had gained an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled Die Anthropologie der Naturvölker, 1859–1864. The title was soon translated as ""The Anthropology of Primitive Peoples"". The last two volumes were published posthumously. Waitz defined anthropology as ""the science of the nature of man"". Following Broca's lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from ""the animals nearest to him"". He stresses that the data of comparison must be empirical, gathered by experimentation. The history of civilization, as well as ethnology, are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that ""the same laws of thought are applicable to all men"". Waitz was influential among British ethnologists. In 1863, the explorer Richard Francis Burton and the speech therapist James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French Société were present, though not Broca. In his keynote address, printed in the first volume of its new publication, The Anthropological Review, Hunt stressed the work of Waitz, adopting his definitions as a standard. Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist.  Similar organizations in other countries followed: The Anthropological Society of Madrid (1865), the American Anthropological Association in 1902, the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionists. One notable exception was the Berlin Society for Anthropology, Ethnology, and Prehistory (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwin's conclusions lacked empirical foundation. During the last three decades of the 19th century, a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association. The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898, 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology. Anthropology is considered by some to have become a tool for colonisers studying their subjects to gain a better understanding and control.",0,Wikipedia,Anthropology,https://en.wikipedia.org/wiki/Anthropology,,Anthropology,wikipedia_api
human_wiki_0173,"20th and 21st centuries Anthropology as a specialized field of academic study developed much through the end of the 19th century. Then it rapidly expanded beginning in the early 20th century to the point where many of the world's higher educational institutions typically included anthropology departments. Thousands of anthropology departments have come into existence, and anthropology has also diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. The organization has also reached a global level. For example, the World Council of Anthropological Associations (WCAA), ""a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology"", currently contains members from about three dozen nations. Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, social anthropology in Great Britain and cultural anthropology in the US have been distinguished from other social sciences by their emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance they place on participant-observation or experiential immersion in the area of research. Cultural anthropology, in particular, has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques. This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork. In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate. In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: biological or physical anthropology; social, cultural, or sociocultural anthropology; archaeological anthropology; and linguistic anthropology. These fields frequently overlap but tend to use different methodologies and techniques. European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. Kollár in 1783). It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition.",0,Wikipedia,Anthropology,https://en.wikipedia.org/wiki/Anthropology,,Anthropology,wikipedia_api
human_wiki_0174,"Fields Anthropology is a global discipline involving humanities, social sciences and natural sciences. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of Homo sapiens, human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of Homo sapiens has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc. Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity. As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies. According to Clifford Geertz,",0,Wikipedia,Anthropology,https://en.wikipedia.org/wiki/Anthropology,,Anthropology,wikipedia_api
human_wiki_0175,"Electricity is the set of physical phenomena associated with the presence and motion of matter possessing an electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others. The presence of either a positive or negative electric charge produces an electric field. The motion of electric charges is an electric current and produces a magnetic field. In most applications, Coulomb's law determines the force acting on an electric charge. Electric potential is the work done to move an electric charge from one point to another within an electric field, typically measured in volts. Electricity plays a central role in many modern technologies, serving in electric power where electric current is used to energise equipment, and in electronics dealing with electrical circuits involving active components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies. The study of electrical phenomena dates back to antiquity, with theoretical understanding progressing slowly until the 17th and 18th centuries. The development of the theory of electromagnetism in the 19th century marked significant progress, leading to electricity's industrial and residential application by electrical engineers by the century's end. This rapid expansion in electrical technology at the time was the driving force behind the Second Industrial Revolution, with electricity's versatility driving transformations in both industry and society. Electricity is integral to applications spanning transport, heating, lighting, communications, and computation, making it the foundation of modern industrial society.",0,Wikipedia,Electricity,https://en.wikipedia.org/wiki/Electricity,,Electricity,wikipedia_api
human_wiki_0176,"History Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE described them as the ""protectors"" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arab naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by electric catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients with ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artefact was electrical in nature.",0,Wikipedia,Electricity,https://en.wikipedia.org/wiki/Electricity,,Electricity,wikipedia_api
human_wiki_0177,"Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the Neo-Latin word electricus (""of amber"" or ""like amber"", from ἤλεκτρον, elektron, the Greek word for ""amber"") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words ""electric"" and ""electricity"", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646. Isaac Newton made early investigations into electricity, with an idea of his written down in his book Opticks arguably the beginning of the field theory of the electric force. Further work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.",0,Wikipedia,Electricity,https://en.wikipedia.org/wiki/Electricity,,Electricity,wikipedia_api
human_wiki_0178,"In 1775, Hugh Williamson reported a series of experiments to the Royal Society on the shocks delivered by the electric eel; that same year the surgeon and anatomist John Hunter described the structure of the fish's electric organs. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his ""On Physical Lines of Force"" in 1861 and 1862. While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life. In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for ""his discovery of the law of the photoelectric effect"". The photoelectric effect is also employed in photocells such as can be found in solar panels. The first solid-state device was the ""cat's-whisker detector"" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor. Solid-state electronics came into its own with the emergence of transistor technology. The first working transistor, a germanium-based point-contact transistor, was invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947, followed by the bipolar junction transistor in 1948.",0,Wikipedia,Electricity,https://en.wikipedia.org/wiki/Electricity,,Electricity,wikipedia_api
human_wiki_0179,"Concepts Electric charge By modern convention, the charge carried by electrons is defined as negative, and that by protons is positive. Before these particles were discovered, Benjamin Franklin had defined a positive charge as being the charge acquired by a glass rod when it is rubbed with a silk cloth. A proton by definition carries a charge of exactly 1.602176634×10−19 coulombs. This value is also defined as the elementary charge. No object can have a charge smaller than the elementary charge, and any amount of charge an object may carry is a multiple of the elementary charge. An electron has an equal negative charge, i.e. −1.602176634×10−19 coulombs. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle. The presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity. A lightweight ball suspended by a fine thread can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: like-charged objects repel and opposite-charged objects attract. The force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them. The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 1042 times that of the gravitational attraction pulling them together. Charge originates from certain types of subatomic particles, the most familiar carriers of which are the electron and proton. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact or by passing along a conducting material, such as a wire. The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other. Charge can be measured by a number of means, an early instrument being the gold-leaf electroscope, which although still in use for classroom demonstrations, has been superseded by the electronic electrometer.",0,Wikipedia,Electricity,https://en.wikipedia.org/wiki/Electricity,,Electricity,wikipedia_api
human_wiki_0180,"The heart is a muscular organ found in humans and other animals. This organ pumps blood through the blood vessels. The heart and blood vessels together make up the circulatory system. The pumped blood carries oxygen and nutrients to the tissue, while carrying metabolic waste such as carbon dioxide to the lungs. In humans, the heart is approximately the size of a closed fist and is located between the lungs, in the middle compartment of the chest, called the mediastinum. In humans, the heart is divided into four chambers: upper left and right atria and lower left and right ventricles. Commonly, the right atrium and ventricle are referred together as the right heart and their left counterparts as the left heart. In a healthy heart, blood flows one way through the heart due to heart valves, which prevent backflow. The heart is enclosed in a protective sac, the pericardium, which also contains a small amount of fluid. The wall of the heart is made up of three layers: epicardium, myocardium, and endocardium.   The heart pumps blood with a rhythm determined by a group of pacemaker cells in the sinoatrial node. These generate an electric current that causes the heart to contract, traveling through the atrioventricular node and along the conduction system of the heart. In humans, deoxygenated blood enters the heart through the right atrium from the superior and inferior venae cavae and passes to the right ventricle. From here, it is pumped into pulmonary circulation to the lungs, where it receives oxygen and gives off carbon dioxide. Oxygenated blood then returns to the left atrium, passes through the left ventricle and is pumped out through the aorta into systemic circulation, traveling through arteries, arterioles, and capillaries—where nutrients and other substances are exchanged between blood vessels and cells, losing oxygen and gaining carbon dioxide—before being returned to the heart through venules and veins. The adult heart beats at a resting rate close to 72 beats per minute. Cardiovascular diseases were the most common cause of death globally as of 2008, accounting for 30% of all human deaths. Of these more than three-quarters are a result of coronary artery disease and stroke. Risk factors include: smoking, being overweight, little exercise, high cholesterol, high blood pressure, and poorly controlled diabetes, among others. Cardiovascular diseases do not frequently have symptoms but may cause chest pain or shortness of breath. Diagnosis of heart disease is often done by the taking of a medical history, listening to the heart-sounds with a stethoscope, as well as with ECG, and echocardiogram which uses ultrasound. Specialists who focus on diseases of the heart are called cardiologists, although many specialties of medicine may be involved in treatment.",0,Wikipedia,Heart,https://en.wikipedia.org/wiki/Heart,,Heart,wikipedia_api
human_wiki_0181,"Structure Location and shape The human heart is situated in the mediastinum, at the level of thoracic vertebrae T5–T8. A double-membraned sac called the pericardium surrounds the heart and attaches to the mediastinum. The back surface of the heart lies near the vertebral column, and the front surface, known as the sternocostal surface, sits behind the sternum and rib cartilages. The upper part of the heart is the attachment point for several large blood vessels—the venae cavae, aorta and pulmonary trunk. The upper part of the heart is located at the level of the third costal cartilage. The lower tip of the heart, the apex, lies to the left of the sternum (8 to 9 cm from the midsternal line) between the junction of the fourth and fifth ribs near their articulation with the costal cartilages. The largest part of the heart is usually slightly offset to the left side of the chest (levocardia). In a rare congenital disorder (dextrocardia) the heart is offset to the right side and is felt to be on the left because the left heart is stronger and larger, since it pumps to all body parts. Because the heart is between the lungs, the left lung is smaller than the right lung and has a cardiac notch in its border to accommodate the heart. The heart is cone-shaped, with its base positioned upwards and tapering down to the apex. An adult heart has a mass of 250–350 grams (9–12 oz). The heart is often described as the size of a fist: 12 cm (5 in) in length, 8 cm (3.5 in) wide, and 6 cm (2.5 in) in thickness, although this description is disputed, as the heart is likely to be slightly larger. Well-trained athletes can have much larger hearts due to the effects of exercise on the heart muscle, similar to the response of skeletal muscle.",0,Wikipedia,Heart,https://en.wikipedia.org/wiki/Heart,,Heart,wikipedia_api
human_wiki_0182,"Chambers The heart has four chambers, two upper atria, the receiving chambers, and two lower ventricles, the discharging chambers. The atria open into the ventricles via the atrioventricular valves, present in the atrioventricular septum. This distinction is visible also on the surface of the heart as the coronary sulcus. There is an ear-shaped structure in the upper right atrium called the right atrial appendage, or auricle, and another in the upper left atrium, the left atrial appendage. The right atrium and the right ventricle together are sometimes referred to as the right heart. Similarly, the left atrium and the left ventricle together are sometimes referred to as the left heart. The ventricles are separated from each other by the interventricular septum, visible on the surface of the heart as the anterior longitudinal sulcus and the posterior interventricular sulcus. The fibrous cardiac skeleton gives structure to the heart. It forms the atrioventricular septum, which separates the atria from the ventricles, and the fibrous rings, which serve as bases for the four heart valves. The cardiac skeleton also provides an important boundary in the heart's electrical conduction system since collagen cannot conduct electricity. The interatrial septum separates the atria, and the interventricular septum separates the ventricles. The interventricular septum is much thicker than the interatrial septum since the ventricles need to generate greater pressure when they contract.",0,Wikipedia,Heart,https://en.wikipedia.org/wiki/Heart,,Heart,wikipedia_api
human_wiki_0183,"Valves The heart has four valves, which separate its chambers. One valve lies between each atrium and ventricle, and one valve rests at the exit of each ventricle. The valves between the atria and ventricles are called the atrioventricular valves. Between the right atrium and the right ventricle is the tricuspid valve. The tricuspid valve has three cusps, which connect to chordae tendinae and three papillary muscles named the anterior, posterior, and septal muscles, after their relative positions. The mitral valve lies between the left atrium and left ventricle. It is also known as the bicuspid valve due to its having two cusps, an anterior and a posterior cusp. These cusps are also attached via chordae tendinae to two papillary muscles projecting from the ventricular wall. The papillary muscles extend from the walls of the heart to valves by cartilaginous connections called chordae tendinae. These muscles prevent the valves from falling too far back when they close. During the relaxation phase of the cardiac cycle, the papillary muscles are also relaxed and the tension on the chordae tendineae is slight. As the heart chambers contract, so do the papillary muscles. This creates tension on the chordae tendineae, helping to hold the cusps of the atrioventricular valves in place and preventing them from being blown back into the atria. Two additional semilunar valves sit at the exit of each of the ventricles. The pulmonary valve is located at the base of the pulmonary artery. This has three cusps which are not attached to any papillary muscles. When the ventricle relaxes blood flows back into the ventricle from the artery and this flow of blood fills the pocket-like valve, pressing against the cusps which close to seal the valve. The semilunar aortic valve is at the base of the aorta and also is not attached to papillary muscles. This too has three cusps which close with the pressure of the blood flowing back from the aorta.",0,Wikipedia,Heart,https://en.wikipedia.org/wiki/Heart,,Heart,wikipedia_api
human_wiki_0184,"Right heart The right heart consists of two chambers, the right atrium and the right ventricle, separated by a valve, the tricuspid valve. The right atrium receives blood almost continuously from the body's two major veins, the superior and inferior venae cavae. A small amount of blood from the coronary circulation also drains into the right atrium via the coronary sinus, which is immediately above and to the middle of the opening of the inferior vena cava. In the wall of the right atrium is an oval-shaped depression known as the fossa ovalis, which is a remnant of an opening in the fetal heart known as the foramen ovale. Most of the internal surface of the right atrium is smooth, the depression of the fossa ovalis is medial, and the anterior surface has prominent ridges of pectinate muscles, which are also present in the right atrial appendage. The right atrium is connected to the right ventricle by the tricuspid valve. The walls of the right ventricle are lined with trabeculae carneae, ridges of cardiac muscle covered by endocardium. In addition to these muscular ridges, a band of cardiac muscle, also covered by endocardium, known as the moderator band reinforces the thin walls of the right ventricle and plays a crucial role in cardiac conduction. It arises from the lower part of the interventricular septum and crosses the interior space of the right ventricle to connect with the inferior papillary muscle. The right ventricle tapers into the pulmonary trunk, into which it ejects blood when contracting. The pulmonary trunk branches into the left and right pulmonary arteries that carry the blood to each lung. The pulmonary valve lies between the right heart and the pulmonary trunk.",0,Wikipedia,Heart,https://en.wikipedia.org/wiki/Heart,,Heart,wikipedia_api
human_wiki_0185,"Computer science is the study of computation, information, and automation. Included broadly in the sciences, computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). An expert in the field is known as a computer scientist.  Algorithms and data structures are central to computer science. The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data. The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.",0,Wikipedia,Computer science,https://en.wikipedia.org/wiki/Computer_science,,Computer_science,wikipedia_api
human_wiki_0186,"History The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and ""in less than two years, he had sketched out many of the salient features of the modern computer"". ""A crucial step was the adoption of a punched card system derived from the Jacquard loom"" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used punched cards and a central processing unit. When the machine was finished, some hailed it as ""Babbage's dream come true"".",0,Wikipedia,Computer science,https://en.wikipedia.org/wiki/Computer_science,,Computer_science,wikipedia_api
human_wiki_0187,"During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.",0,Wikipedia,Computer science,https://en.wikipedia.org/wiki/Computer_science,,Computer_science,wikipedia_api
human_wiki_0188,"Etymology and scope Although first proposed in 1956, the term ""computer science"" appears in a 1959 article in Communications of the ACM, in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline. This effort, and those of others such as numerical analyst George Forsythe, were successful, and universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases. In the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression ""automatic information"" (e.g. ""informazione automatica"" in Italian) or ""information and mathematics"" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). ""In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain."" A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that ""computer science is no more about computers than astronomy is about telescopes."" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic. Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter, Stephen Kleene, and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra. The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term ""software engineering"" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines. The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.",0,Wikipedia,Computer science,https://en.wikipedia.org/wiki/Computer_science,,Computer_science,wikipedia_api
human_wiki_0189,"Philosophy Epistemology of computer science Despite the word science in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena. Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.",0,Wikipedia,Computer science,https://en.wikipedia.org/wiki/Computer_science,,Computer_science,wikipedia_api
human_wiki_0190,"Electrical engineering is an engineering discipline concerned with the study, design, and application of equipment, devices, and systems that use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after the commercialization of the electric telegraph, the telephone, and electrical power generation, distribution, and use. Electrical engineering is divided into a wide range of different fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, control engineering, photovoltaic cells, electronics, and optics and photonics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics/control, and electrical materials science. Electrical engineers also study machine learning and computer science techniques due to significant overlap. Electrical engineers typically hold a degree in electrical engineering, electronic or electrical and electronic engineering. Practicing engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the National Society of Professional Engineers (NSPE), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET, formerly the IEE). Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.",0,Wikipedia,Electrical engineering,https://en.wikipedia.org/wiki/Electrical_engineering,,Electrical_engineering,wikipedia_api
human_wiki_0191,"History Electricity has been a subject of scientific interest since at least the early 17th century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term ""electricity"". He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery.",0,Wikipedia,Electrical engineering,https://en.wikipedia.org/wiki/Electrical_engineering,,Electrical_engineering,wikipedia_api
human_wiki_0192,"19th century In the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Hans Christian Ørsted, who discovered in 1820 that an electric current produces a magnetic field that will deflect a compass needle; of William Sturgeon, who in 1825 invented the electromagnet; of Joseph Henry and Edward Davy, who invented the electrical relay in 1835; of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor; of Michael Faraday, the discoverer of electromagnetic induction in 1831; and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise Electricity and Magnetism. In 1782, Georges-Louis Le Sage developed and presented in Berlin probably the world's first form of electric telegraphy, using 24 different wires, one for each letter of the alphabet. This telegraph connected two rooms. It was an electrostatic telegraph that moved gold leaf through electrical conduction. In 1795, Francisco Salva Campillo proposed an electrostatic telegraph system. Between 1803 and 1804, he worked on electrical telegraphy, and in 1804, he presented his report at the Royal Academy of Natural Sciences and Arts of Barcelona. Salva's electrolyte telegraph system was very innovative though it was greatly influenced by and based upon two discoveries made in Europe in 1800—Alessandro Volta's electric battery for generating an electric current and William Nicholson and Anthony Carlyle's electrolysis of water. Electrical telegraphy may be considered the first example of electrical engineering. Electrical engineering became a profession in the later 19th century.  Practitioners had created a global electric telegraph network, and the first professional electrical engineering institutions were founded in the UK and the US to support the new discipline. Francis Ronalds created an electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity. Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort. By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy. Practical applications and advances in such fields created an increasing need for standardized units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893. The publication of these standards formed the basis of future advances in standardization in various industries, and in many countries, the definitions were immediately recognized in relevant legislation. During these years, the study of electricity was largely considered to be a subfield of physics since early electrical technology was considered electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882 and introduced the first-degree course in electrical engineering in 1883. The first electrical engineering degree program in the United States was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, though it was Cornell University to produce the world's first electrical engineering graduates in 1885. The first course in electrical engineering was taught in 1883 in Cornell's Sibley College of Mechanical Engineering and Mechanic Arts. In about 1885, Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States. In the same year, University College London founded the first chair of electrical engineering in Great Britain. Professor Mendell P. Weinbach at University of Missouri established the electrical engineering department in 1886. Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world. During these decades the use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts—direct current (DC)—to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley Jr. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering. The spread in the use of AC set off in the United States what has been called the war of the currents between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.",0,Wikipedia,Electrical engineering,https://en.wikipedia.org/wiki/Electrical_engineering,,Electrical_engineering,wikipedia_api
human_wiki_0193,"Early 20th century During the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including the possibility of invisible airborne waves (later called ""radio waves""). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these ""Hertzian waves"" into a purpose-built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of 2,100 miles (3,400 km). Millimetre wave communication was first investigated by Jagadish Chandra Bose during 1894–1896, when he reached an extremely high frequency of up to 60 GHz in his experiments. He also introduced the use of semiconductor junctions to detect radio waves, when he patented the radio crystal detector in 1901. In 1897, Karl Ferdinand Braun introduced the cathode-ray tube as part of an oscilloscope, a crucial enabling technology for electronic television. John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode. In 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer. In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936. In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts.  In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer. In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives. In 1948, Claude Shannon published ""A Mathematical Theory of Communication"" which mathematically describes the passage of information with uncertainty (electrical noise).",0,Wikipedia,Electrical engineering,https://en.wikipedia.org/wiki/Electrical_engineering,,Electrical_engineering,wikipedia_api
human_wiki_0194,"Solid-state electronics The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain while working under William Shockley at the Bell Telephone Laboratories (BTL) in 1947. They then invented the bipolar junction transistor in 1948. While early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, they opened the door for more compact devices. The first integrated circuits were the hybrid integrated circuit invented by Jack Kilby at Texas Instruments in 1958 and the monolithic integrated circuit chip invented by Robert Noyce at Fairchild Semiconductor in 1959. The MOSFET (metal–oxide–semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at BTL in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. It revolutionized the electronics industry, becoming the most widely used electronic device in the world. The MOSFET made it possible to build high-density integrated circuit chips. The earliest experimental MOS IC chip to be fabricated was built by Fred Heiman and Steven Hofstein at RCA Laboratories in 1962. MOS technology enabled Moore's law, the doubling of transistors on an IC chip every two years, predicted by Gordon Moore in 1965. Silicon-gate MOS technology was developed by Federico Faggin at Fairchild in 1968. Since then, the MOSFET has been the basic building block of modern electronics. The mass-production of silicon MOSFETs and MOS integrated circuit chips, along with continuous MOSFET scaling miniaturization at an exponential pace (as predicted by Moore's law), has since led to revolutionary changes in technology, economy, culture and thinking. The Apollo program which culminated in landing astronauts on the Moon with Apollo 11 in 1969 was enabled by NASA's adoption of advances in semiconductor electronic technology, including MOSFETs in the Interplanetary Monitoring Platform (IMP) and silicon integrated circuit chips in the Apollo Guidance Computer (AGC). The development of MOS integrated circuit technology in the 1960s led to the invention of the microprocessor in the early 1970s. The first single-chip microprocessor was the Intel 4004, released in 1971.  The Intel 4004 was designed and realized by Federico Faggin at Intel with his silicon-gate MOS technology, along with Intel's Marcian Hoff and Stanley Mazor and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.",0,Wikipedia,Electrical engineering,https://en.wikipedia.org/wiki/Electrical_engineering,,Electrical_engineering,wikipedia_api
human_wiki_0195,"Literacy is the ability to read and write, while illiteracy refers to an inability to read and write. Some researchers suggest that the study of literacy as a concept can be divided into two periods: the period before 1950, when literacy was understood solely as alphabetical literacy (word and letter recognition); and the period after 1950, when literacy slowly began to be considered as a wider concept and process, including the social and cultural aspects of reading, writing, and functional literacy.",0,Wikipedia,Literacy,https://en.wikipedia.org/wiki/Literacy,,Literacy,wikipedia_api
human_wiki_0196,"Definition The range of definitions of literacy used by NGOs, think tanks, and advocacy groups since the 1990s suggests that this shift in understanding from ""discrete skill"" to ""social practice"" is both ongoing and uneven. Some definitions remain fairly closely aligned with the traditional ""ability to read and write"" connotation, whereas others take a broader view:",0,Wikipedia,Literacy,https://en.wikipedia.org/wiki/Literacy,,Literacy,wikipedia_api
human_wiki_0197,"The 2003 National Assessment of Adult Literacy (USA) included ""quantitative literacy"" (numeracy) in its treatment of literacy. It defined literacy as ""the ability to use printed and written information to function in society, to achieve one's goals, and to develop one's knowledge and potential."" It included three types of adult literacy: prose (e.g., a newspaper article), documents (e.g., a bus schedule), and quantitative literacy (e.g., the use of arithmetic operations in a product advertisement). In 2015, the United Nations Statistics Division defined the youth literacy rate as ""the percentage of the population aged 15–24 years who can both read and write with understanding a short simple statement on everyday life."" In 2016, the European Literacy Policy Network defined literacy as ""the ability to read and write [...] in all media (print or electronic), including digital literacy."" In 2018, UNESCO included ""printed and written materials"" and ""varying contexts"" in its definition of literacy, i.e., ""the ability to identify, understand, interpret, create, communicate and compute, using printed and written materials associated with varying contexts."" In 2019, the Organisation for Economic Co-operation and Development (OECD), in its Programme for the International Assessment of Adult Competencies (PIAAC) adult skills surveys, included ""written texts"" in its definition of literacy, i.e., ""the ability to understand, evaluate, use and engage with written texts in order to participate in society, achieve one's goals, and develop one's knowledge and potential."" Also, it treats numeracy and problem solving using technology as separate considerations. In 2021, Education Scotland and the National Literacy Trust in the UK included oral communication skills (listening and speaking) under the umbrella of literacy. As of 2021, the International Literacy Association  uses ""the ability to identify, understand, interpret, create, compute, and communicate using visual, audible, and digital materials across disciplines and in any context."" The expression ""reading literacy"" is used by the Progress in International Reading Literacy Study (PIRLS), which has monitored international trends in reading achievement at the fourth grade level since 2001. Other organizations might include numeracy skills and technology skills separately but alongside literacy skills; still others emphasize the increasing involvement of computers and other digital technologies in communication that necessitates additional skills (e.g., interfacing with web browsers and word processing programs, organizing and altering the configuration of files, etc.). Some researchers define literacy as ""particular ways of thinking about and doing reading and writing"" with the purpose of understanding or expressing thoughts or ideas in written form in some specific context of use. In this view, humans in literate societies have sets of practices for producing and consuming writing, and they also have beliefs about these practices. Reading, in this view, is always reading something for some purpose; writing is always writing something for someone for some purpose. Beliefs about reading and writing and their value for society and for the individual always influence the ways literacy is taught, learned, and practiced. The concept of multiliteracies has gained currency, particularly in English Language Arts curricula, on the grounds that reading ""is interactive and informative, and occurs in ever-increasingly technological settings where information is part of spatial, audio, and visual patterns (Rhodes & Robnolt, 2009)"". Objections have been raised that this concept downplays the importance of reading instruction that focuses on ""alphabetic representations"". However, these are not mutually exclusive, as children can become proficient in word-reading while engaging with multiliteracies. Word reading is fundamental for multiple forms of communication. Beginning in the 1940s, the term literacy has often been used to mean having knowledge or skill in a particular field, such as:",0,Wikipedia,Literacy,https://en.wikipedia.org/wiki/Literacy,,Literacy,wikipedia_api
human_wiki_0198,"Computer literacy – Skill in using computers and digital technology Scientific literacy – Ability to understand science Statistical literacy – Ability to understand and reason with statistics and data Critical literacy – Ability to find embedded discrimination in media Disaster literacy – Proposed model for the ability to understand and use life-saving information, including the ability to respond and recover from disasters effectively Ecological literacy – Ability to understand natural systems and their interactions Financial literacy – Ability to make informed choices about money Health literacy – Ability to understand healthcare information Linguistic literacy – Ability to read, write, understand, and speak any type of language Media literacy – Ability to navigate media Political literacy – Abilities needed for an effective electorate Social literacy – Literacy gained through social interactions Mathematical literacy, also called numeracy – Ability to apply numerical concepts Visual literacy – Ability to interpret information in images, e.g., body language, pictures, maps, and video Musical literacy – Refers to culturally determined systems of knowledge in music and to musical abilities. Classicist Eric Havelock developed a continuum for a culture's literacy, from pre-literate, through craft-literate, recitation-literate and script-literate to type-literate.",0,Wikipedia,Literacy,https://en.wikipedia.org/wiki/Literacy,,Literacy,wikipedia_api
human_wiki_0199,"Inability to use reading, writing, and calculation skills for their own and their community's development. Inability to read well enough to manage daily living and employment tasks that require reading skills beyond a basic level. Inability to understand complex texts despite adequate schooling, language skills, elementary reading skills, age, and IQ. Functional illiteracy is distinguished from primary illiteracy (i.e., the inability to read and write a short, simple statement concerning one's own everyday life) and learning difficulties (e.g., dyslexia). These categories have been contested—as has the concept of ""illiteracy"" itself—for being predicated on narrow assumptions, primarily derived from school-based contexts, about what counts as reading and writing (e.g., comprehending and following instructions).",0,Wikipedia,Literacy,https://en.wikipedia.org/wiki/Literacy,,Literacy,wikipedia_api
human_wiki_0200,"Geography (from Ancient Greek γεωγραφία geōgraphía; combining gê 'Earth' and gráphō 'write', literally 'Earth writing') is the study of the lands, features, inhabitants, and phenomena of Earth. Geography is an all-encompassing discipline that seeks an understanding of Earth and its human and natural complexities—not merely where objects are, but also how they have changed and come to be. While geography is specific to Earth, many concepts can be applied more broadly to other celestial bodies in the field of planetary science. Geography has been called ""a bridge between natural science and social science disciplines."" The history of geography as a discipline spans cultures and millennia, being independently developed by multiple groups, and cross-pollinated by trade between these groups. Geography as a discipline dates back to the earliest attempts to understand the world spatially, with the earliest example of an attempted world map dating to the 9th century BCE in ancient Babylon. Origins of many of the concepts in geography can be traced to Greek Eratosthenes of Cyrene, who may have coined the term ""geographia"" (c. 276 BC – c. 195/194 BC). The first recorded use of the word γεωγραφία was as the title of a book by Greek scholar Claudius Ptolemy (100 – 170 AD). During the Middle Ages, geography was influenced by Islamic scholars, like Muhammad al-Idrisi, producing detailed maps of the world. The Age of Discovery was influential in the development of geography, as European explorers mapped the New World. Modern developments include the development of geomatics and geographic information science. The core concepts of geography consistent between all approaches are a focus on space, place, time, and scale. Today, geography is an extremely broad discipline with multiple approaches and modalities. The main branches of geography are physical geography, human geography, and technical geography. Physical geography focuses on the natural environment, human geography focuses on how humans interact with the Earth, and technical geography focuses on the development of tools for understanding geography. Techniques employed can generally be broken down into quantitative and qualitative approaches, with many studies taking mixed-methods approaches. Common techniques include cartography, remote sensing, interviews, and surveying.",0,Wikipedia,Geography,https://en.wikipedia.org/wiki/Geography,,Geography,wikipedia_api
human_wiki_0201,"Fundamentals Geography is a systematic study of the Earth (other celestial bodies are specified, such as ""geography of Mars"", or given another name, such as areography in the case of Mars, or selenography in the case of the Moon, or planetography for the general case), its features, and phenomena that take place on it. For something to fall into the domain of geography, it generally needs some sort of spatial component that can be placed on a map, such as coordinates, place names, or addresses. This has led to geography being associated with cartography and place names. Although many geographers are trained in toponymy and cartology, this is not their main preoccupation. Geographers study the Earth's spatial and temporal distribution of phenomena, processes, and features as well as the interaction of humans and their environment. Because space and place affect a variety of topics, such as economics, health, climate, plants, and animals, geography is highly interdisciplinary. The interdisciplinary nature of the geographical approach depends on an attentiveness to the relationship between physical and human phenomena and their spatial patterns. While narrowing down geography to a few key concepts is extremely challenging, and subject to tremendous debate within the discipline, several sources have approached the topic. The 1st edition of the book ""Key Concepts in Geography"" broke down this into chapters focusing on ""Space,"" ""Place,"" ""Time,"" ""Scale,"" and ""Landscape."" The 2nd edition of the book expanded on these key concepts by adding ""Environmental systems,"" ""Social Systems,"" ""Nature,"" ""Globalization,"" ""Development,"" and ""Risk,"" demonstrating how challenging narrowing the field can be. Another approach used extensively in teaching geography are the Five themes of geography established by ""Guidelines for Geographic Education: Elementary and Secondary Schools,"" published jointly by the National Council for Geographic Education and the Association of American Geographers in 1984. These themes are Location, place, relationships within places (often summarized as Human-Environment Interaction), movement, and regions. The five themes of geography have shaped how American education approaches the topic in the years since.",0,Wikipedia,Geography,https://en.wikipedia.org/wiki/Geography,,Geography,wikipedia_api
human_wiki_0202,"Space Just as all phenomena exist in time and thus have a history, they also exist in space and have a geography. For something to exist in the realm of geography, it must be able to be described spatially. Thus, space is the most fundamental concept at the foundation of geography. The concept is so basic, that geographers often have difficulty defining exactly what it is. Absolute space is the exact site, or spatial coordinates, of objects, persons, places, or phenomena under investigation. We exist in space. Absolute space leads to the view of the world as a photograph, with everything frozen in place when the coordinates were recorded. Today, geographers are trained to recognize the world as a dynamic space where all processes interact and take place, rather than a static image on a map.",0,Wikipedia,Geography,https://en.wikipedia.org/wiki/Geography,,Geography,wikipedia_api
human_wiki_0203,"Place Place is one of the most complex and important terms in geography. In human geography, place is the synthesis of the coordinates on the Earth's surface, the activity and use that occurs, has occurred, and will occur at the coordinates, and the meaning ascribed to the space by human individuals and groups. This can be extraordinarily complex, as different spaces may have different uses at different times and mean different things to different people. In physical geography, a place includes all of the physical phenomena that occur in space, including the lithosphere, atmosphere, hydrosphere, and biosphere. Places do not exist in a vacuum and instead have complex spatial relationships with each other, and place is concerned how a location is situated in relation to all other locations. As a discipline then, the term place in geography includes all spatial phenomena occurring at a location, the diverse uses and meanings humans ascribe to that location, and how that location impacts and is impacted by all other locations on Earth. In one of Yi-Fu Tuan's papers, he explains that in his view, geography is the study of Earth as a home for humanity, and thus place and the complex meaning behind the term is central to the discipline of geography.",0,Wikipedia,Geography,https://en.wikipedia.org/wiki/Geography,,Geography,wikipedia_api
human_wiki_0204,"Time Time is usually thought to be within the domain of history, however, it is of significant concern in the discipline of geography. In physics, space and time are not separated, and are combined into the concept of spacetime. Geography is subject to the laws of physics, and in studying things that occur in space, time must be considered. Time in geography is more than just the historical record of events that occurred at various discrete coordinates; but also includes modeling the dynamic movement of people, organisms, and things through space. Time facilitates movement through space, ultimately allowing things to flow through a system. The amount of time an individual, or group of people, spends in a place will often shape their attachment and perspective to that place. Time constrains the possible paths that can be taken through space, given a starting point, possible routes, and rate of travel. Visualizing time over space is challenging in terms of cartography, and includes Space-Prism, advanced 3D geovisualizations, and animated maps.",0,Wikipedia,Geography,https://en.wikipedia.org/wiki/Geography,,Geography,wikipedia_api
human_wiki_0205,"Education is the transmission of knowledge and skills and the development of character traits. Formal education occurs within a structured institutional framework, such as public schools, following a curriculum. Non-formal education also follows a structured approach but occurs outside the formal schooling system, while informal education involves unstructured learning through daily experiences. Formal and non-formal education are categorized into levels, including early childhood education, primary education, secondary education, and tertiary education. Other classifications focus on teaching methods, such as teacher-centered and student-centered education, and on subjects, such as science education, language education, and physical education. Additionally, the term ""education"" can denote the mental states and qualities of educated individuals and the academic field studying educational phenomena. The precise definition of education is disputed, and there are disagreements about the aims of education and the extent to which education differs from indoctrination by fostering critical thinking. These disagreements impact how to identify, measure, and enhance various forms of education. Essentially, education socializes children into society by instilling cultural values and norms, equipping them with the skills necessary to become productive members of society. In doing so, it stimulates economic growth and raises awareness of local and global problems. Organized institutions play a significant role in education. For instance, governments establish education policies to determine the timing of school classes, the curriculum, and attendance requirements. International organizations, such as UNESCO, have been influential in promoting primary education for all children. Many factors influence the success of education. Psychological factors include motivation, intelligence, and personality. Social factors, such as socioeconomic status, ethnicity, and gender, are often associated with discrimination. Other factors encompass access to educational technology, teacher quality, and parental involvement. The primary academic field examining education is known as education studies. It delves into the nature of education, its objectives, impacts, and methods for enhancement. Education studies encompasses various subfields, including philosophy, psychology, sociology, and economics of education. Additionally, it explores topics such as comparative education, pedagogy, and the history of education. In prehistory, education primarily occurred informally through oral communication and imitation. With the emergence of ancient civilizations, the invention of writing led to an expansion of knowledge, prompting a transition from informal to formal education. Initially, formal education was largely accessible to elites and religious groups. The advent of the printing press in the 15th century facilitated widespread access to books, thus increasing general literacy. In the 18th and 19th centuries, public education gained significance, paving the way for the global movement to provide primary education to all, free of charge, and compulsory up to a certain age. Presently, over 90% of primary-school-age children worldwide attend primary school.",0,Wikipedia,Education,https://en.wikipedia.org/wiki/Education,,Education,wikipedia_api
human_wiki_0206,"Definitions The term ""education"" originates from the Latin words educare, meaning ""to bring up,"" and educere, meaning ""to bring forth."" The definition of education has been explored by theorists from various fields. Many agree that education is a purposeful activity aimed at achieving goals like the transmission of knowledge, skills, and character traits. However, extensive debate surrounds its precise nature beyond these general features. One approach views education as a process occurring during events such as schooling, teaching, and learning. Another perspective perceives education not as a process but as the mental states and dispositions of educated individuals resulting from this process. Furthermore, the term may also refer to the academic field that studies the methods, processes, and social institutions involved in teaching and learning. Having a clear understanding of the term is crucial when attempting to identify educational phenomena, measure educational success, and improve educational practices. Some theorists provide precise definitions by identifying specific features exclusive to all forms of education. Education theorist R. S. Peters, for instance, outlines three essential features of education, including imparting knowledge and understanding to the student, ensuring the process is beneficial, and conducting it in a morally appropriate manner. While such precise definitions often characterize the most typical forms of education effectively, they face criticism because less common types of education may occasionally fall outside their parameters. Dealing with counterexamples not covered by precise definitions can be challenging, which is why some theorists prefer offering less exact definitions based on family resemblance instead. This approach suggests that all forms of education are similar to each other but need not share a set of essential features common to all. Some education theorists, such as Keira Sewell and Stephen Newman, argue that the term ""education"" is context-dependent. Evaluative or thick conceptions of education assert that it is inherent in the nature of education to lead to some form of improvement. They contrast with thin conceptions, which offer a value-neutral explanation. Some theorists provide a descriptive conception of education by observing how the term is commonly used in ordinary language. Prescriptive conceptions, on the other hand, define what constitutes good education or how education should be practiced. Many thick and prescriptive conceptions view education as an endeavor that strives to achieve specific objectives, which may encompass acquiring knowledge, learning to think rationally, and cultivating character traits such as kindness and honesty. Various scholars emphasize the importance of critical thinking in distinguishing education from indoctrination. They argue that indoctrination focuses solely on instilling beliefs in students, regardless of their rationality; whereas education also encourages the rational ability to critically examine and question those beliefs. However, it is not universally accepted that these two phenomena can be clearly distinguished, as some forms of indoctrination may be necessary in the early stages of education when the child's mind is not yet fully developed. This is particularly relevant in cases where young children must learn certain things without comprehending the underlying reasons, such as specific safety rules and hygiene practices. Education can be characterized from both the teacher's and the student's perspectives. Teacher-centered definitions emphasize the perspective and role of the teacher in transmitting knowledge and skills in a morally appropriate manner. On the other hand, student-centered definitions analyze education based on the student's involvement in the learning process, suggesting that this process transforms and enriches their subsequent experiences. It is also possible to consider definitions that incorporate both perspectives. In this approach, education is seen as a process of shared experience, involving the discovery of a common world and the collaborative solving of problems.",0,Wikipedia,Education,https://en.wikipedia.org/wiki/Education,,Education,wikipedia_api
human_wiki_0207,"Types There are several classifications of education. One classification depends on the institutional framework, distinguishing between formal, non-formal, and informal education. Another classification involves different levels of education based on factors such as the student's age and the complexity of the content. Further categories focus on the topic, teaching method, medium used, and funding.",0,Wikipedia,Education,https://en.wikipedia.org/wiki/Education,,Education,wikipedia_api
human_wiki_0208,"Formal, non-formal, and informal The most common division is between formal, non-formal, and informal education. Formal education occurs within a structured institutional framework, typically with a chronological and hierarchical order. The modern schooling system organizes classes based on the student's age and progress, ranging from primary school to university. Formal education is usually overseen and regulated by the government and often mandated up to a certain age. Non-formal and informal education occur outside the formal schooling system, with non-formal education serving as a middle ground. Like formal education, non-formal education is organized, systematic, and pursued with a clear purpose, as seen in activities such as tutoring, fitness classes, and participation in the scouting movement. Informal education, on the other hand, occurs in an unsystematic manner through daily experiences and exposure to the environment. Unlike formal and non-formal education, there is typically no designated authority figure responsible for teaching. Informal education unfolds in various settings and situations throughout one's life, often spontaneously, such as children learning their first language from their parents or individuals mastering cooking skills by preparing a dish together. Some theorists differentiate between the three types based on the learning environment: formal education occurs within schools, non-formal education takes place in settings not regularly frequented, such as museums, and informal education unfolds in the context of everyday routines. Additionally, there are disparities in the source of motivation. Formal education tends to be propelled by extrinsic motivation, driven by external rewards. Conversely, in non-formal and informal education, intrinsic motivation, stemming from the enjoyment of the learning process, typically prevails. While the differentiation among the three types is generally clear, certain forms of education may not neatly fit into a single category. In primitive cultures, education predominantly occurred informally, with little distinction between educational activities and other daily endeavors. Instead, the entire environment served as a classroom, and adults commonly assumed the role of educators. However, informal education often proves insufficient for imparting large quantities of knowledge. To address this limitation, formal educational settings and trained instructors are typically necessary. This necessity contributed to the increasing significance of formal education throughout history. Over time, formal education led to a shift towards more abstract learning experiences and topics, distancing itself from daily life. There was a greater emphasis on understanding general principles and concepts rather than simply observing and imitating specific behaviors.",0,Wikipedia,Education,https://en.wikipedia.org/wiki/Education,,Education,wikipedia_api
human_wiki_0209,"Levels Types of education are often categorized into different levels or stages. One influential framework is the International Standard Classification of Education, maintained by the United Nations Educational, Scientific and Cultural Organization (UNESCO). This classification encompasses both formal and non-formal education and distinguishes levels based on factors such as the student's age, the duration of learning, and the complexity of the content covered. Additional criteria include entry requirements, teacher qualifications, and the intended outcome of successful completion. The levels are grouped into early childhood education (level 0), primary education (level 1), secondary education (levels 2–3), post-secondary non-tertiary education (level 4), and tertiary education (levels 5–8). Early childhood education, also referred to as preschool education or nursery education, encompasses the period from birth until the commencement of primary school. It is designed to facilitate holistic child development, addressing physical, mental, and social aspects. Early childhood education is pivotal in fostering socialization and personality development, while also imparting fundamental skills in communication, learning, and problem-solving. Its overarching goal is to prepare children for the transition to primary education. While preschool education is typically optional, in certain countries such as Brazil, it is mandatory starting from the age of four.",0,Wikipedia,Education,https://en.wikipedia.org/wiki/Education,,Education,wikipedia_api
human_wiki_0210,"Ethics is the philosophical study of moral phenomena. Also called moral philosophy, it investigates normative questions about what people ought to do or which behavior is morally right. Its main branches include normative ethics, applied ethics, and metaethics. Normative ethics aims to find general principles that govern how people should act. Applied ethics examines concrete ethical problems in real-life situations, such as abortion, treatment of animals, and business practices. Metaethics explores the underlying assumptions and concepts of ethics. It asks whether there are objective moral facts, how moral knowledge is possible, and how moral judgments motivate people. Influential normative theories are consequentialism, deontology, and virtue ethics. According to consequentialists, an act is right if it leads to the best consequences. Deontologists focus on acts themselves, saying that they must adhere to duties, like telling the truth and keeping promises. Virtue ethics sees the manifestation of virtues, like courage and compassion, as the fundamental principle of morality. Ethics is closely connected to value theory, which studies the nature and types of value, like the contrast between intrinsic and instrumental value. Moral psychology is a related empirical field and investigates psychological processes involved in morality, such as reasoning and the formation of character. Descriptive ethics describes the dominant moral codes and beliefs in different societies and considers their historical dimension. The history of ethics started in the ancient period with the development of ethical principles and theories in ancient Egypt, India, China, and Greece. This period saw the emergence of ethical teachings associated with Hinduism, Buddhism, Confucianism, Daoism, and contributions of philosophers like Socrates and Aristotle. During the medieval period, ethical thought was strongly influenced by religious teachings. In the modern period, this focus shifted to a more secular approach concerned with moral experience, reasons for acting, and the consequences of actions. An influential development in the 20th century was the emergence of metaethics.",0,Wikipedia,Ethics,https://en.wikipedia.org/wiki/Ethics,,Ethics,wikipedia_api
human_wiki_0211,"Definition Ethics, also called moral philosophy, is the study of moral phenomena. It is one of the main branches of philosophy and investigates the nature of morality and the principles that govern the moral evaluation of conduct, character traits, and institutions. It examines what obligations people have, what behavior is right and wrong, and how to lead a good life. Some of its key questions are ""How should one live?"" and ""What gives meaning to life?"". In contemporary philosophy, ethics is usually divided into normative ethics, applied ethics, and metaethics. Morality is about what people ought to do rather than what they actually do, what they want to do, or what social conventions require. As a rational and systematic field of inquiry, ethics studies practical reasons why people should act one way rather than another. Most ethical theories seek universal principles that express a general standpoint of what is objectively right and wrong. In a slightly different sense, the term ethics can also refer to individual ethical theories in the form of a rational system of moral principles, such as Aristotelian ethics, and to a moral code that certain societies, social groups, or professions follow, as in Protestant work ethic and medical ethics. The English word ethics has its roots in the Ancient Greek word êthos (ἦθος), meaning 'character' and 'personal disposition'. This word gave rise to the Ancient Greek word ēthikós (ἠθικός), which was translated into Latin as ethica and entered the English language in the 15th century through the Old French term éthique. The term morality originates in the Latin word moralis, meaning 'manners' and 'character'. It was introduced into the English language during the Middle English period through the Old French term moralité. The terms ethics and morality are usually used interchangeably but some philosophers distinguish between the two. According to one view, morality focuses on what moral obligations people have while ethics is broader and includes ideas about what is good and how to lead a meaningful life. Another difference is that codes of conduct in specific areas, such as business and environment, are usually termed ethics rather than morality, as in business ethics and environmental ethics.",0,Wikipedia,Ethics,https://en.wikipedia.org/wiki/Ethics,,Ethics,wikipedia_api
human_wiki_0212,"Normative ethics Normative ethics is the philosophical study of ethical conduct and investigates the fundamental principles of morality. It aims to discover and justify general answers to questions like ""How should one live?"" and ""How should people act?"", usually in the form of universal or domain-independent principles that determine whether an act is right or wrong. For example, given the particular impression that it is wrong to set a child on fire for fun, normative ethics aims to find more general principles that explain why this is the case, like the principle that one should not cause extreme suffering to the innocent, which may itself be explained in terms of a more general principle. Many theories of normative ethics also aim to guide behavior by helping people make moral decisions. Theories in normative ethics state how people should act or what kind of behavior is correct. They do not aim to describe how people normally act, what moral beliefs ordinary people have, how these beliefs change over time, or what ethical codes are upheld in certain social groups. These topics belong to descriptive ethics and are studied in fields like anthropology, sociology, and history rather than normative ethics. Some systems of normative ethics arrive at a single principle covering all possible cases. Others encompass a small set of basic rules that address all or at least the most important moral considerations. One difficulty for systems with several basic principles is that these principles may conflict with each other in some cases and lead to ethical dilemmas. Distinct theories in normative ethics suggest different principles as the foundation of morality. The three most influential schools of thought are consequentialism, deontology, and virtue ethics. These schools are usually presented as exclusive alternatives, but depending on how they are defined, they can overlap and do not necessarily exclude one another. In some cases, they differ in which acts they see as right or wrong. In other cases, they recommend the same course of action but provide different justifications for why it is right.",0,Wikipedia,Ethics,https://en.wikipedia.org/wiki/Ethics,,Ethics,wikipedia_api
human_wiki_0213,"Consequentialism Consequentialism, also called teleological ethics, says that morality depends on consequences. According to the most common view, an act is right if it brings the best future. This means that there is no alternative course of action that has better consequences. A key aspect of consequentialist theories is that they provide a characterization of what is good and then define what is right in terms of what is good. For example, classical utilitarianism says that pleasure is good and that the action leading to the most overall pleasure is right. Consequentialism has been discussed indirectly since the formulation of classical utilitarianism in the late 18th century. A more explicit analysis of this view happened in the 20th century, when the term was coined by G. E. M. Anscombe. Consequentialists usually understand the consequences of an action in a very wide sense that includes the totality of its effects. This is based on the idea that actions make a difference in the world by bringing about a causal chain of events that would not have existed otherwise. A core intuition behind consequentialism is that the future should be shaped to achieve the best possible outcome. The act itself is usually not seen as part of the consequences. This means that if an act has intrinsic value or disvalue, it is not included as a factor. Some consequentialists see this as a flaw, saying that all value-relevant factors need to be considered. They try to avoid this complication by including the act itself as part of the consequences. A related approach is to characterize consequentialism not in terms of consequences but in terms of outcome, with the outcome being defined as the act together with its consequences. Most forms of consequentialism are agent-neutral. This means that the value of consequences is assessed from a neutral perspective, that is, acts should have consequences that are good in general and not just good for the agent. It is controversial whether agent-relative moral theories, like ethical egoism, should be considered as types of consequentialism.",0,Wikipedia,Ethics,https://en.wikipedia.org/wiki/Ethics,,Ethics,wikipedia_api
human_wiki_0214,"Types There are many different types of consequentialism. They differ based on what type of entity they evaluate, what consequences they take into consideration, and how they determine the value of consequences. Most theories assess the moral value of acts. However, consequentialism can also be used to evaluate motives, character traits, rules, and policies. Many types assess the value of consequences based on whether they promote happiness or suffering. But there are also alternative evaluative principles, such as desire satisfaction, autonomy, freedom, knowledge, friendship, beauty, and self-perfection. Some forms of consequentialism hold that there is only a single source of value. The most prominent among them is classical utilitarianism, which states that the moral value of acts only depends on the pleasure and suffering they cause. An alternative approach says that there are many different sources of value, which all contribute to one overall value. Before the 20th century, consequentialists were only concerned with the total of value or the aggregate good. In the 20th century, alternative views were developed that additionally consider the distribution of value. One of them states that an equal distribution of goods is better than an unequal distribution even if the aggregate good is the same. There are disagreements about which consequences should be assessed. An important distinction is between act consequentialism and rule consequentialism. According to act consequentialism, the consequences of an act determine its moral value. This means that there is a direct relation between the consequences of an act and its moral value. Rule consequentialism, by contrast, holds that an act is right if it follows a certain set of rules. Rule consequentialism determines the best rules by considering their outcomes at a community level. People should follow the rules that lead to the best consequences when everyone in the community follows them. This implies that the relation between an act and its consequences is indirect. For example, if telling the truth is one of the best rules, then according to rule consequentialism, a person should tell the truth even in specific cases where lying would lead to better consequences. Another disagreement is between actual and expected consequentialism. According to the traditional view, only the actual consequences of an act affect its moral value. One difficulty of this view is that many consequences cannot be known in advance. This means that in some cases, even well-planned and intentioned acts are morally wrong if they inadvertently lead to negative outcomes. An alternative perspective states that what matters are not the actual consequences but the expected consequences. This view takes into account that when deciding what to do, people have to rely on their limited knowledge of the total consequences of their actions. According to this view, a course of action has positive moral value despite leading to an overall negative outcome if it had the highest expected value, for example, because the negative outcome could not be anticipated or was unlikely. A further difference is between maximizing and satisficing consequentialism. According to maximizing consequentialism, only the best possible act is morally permitted. This means that acts with positive consequences are wrong if there are alternatives with even better consequences. One criticism of maximizing consequentialism is that it demands too much by requiring that people do significantly more than they are socially expected to. For example, if the best action for someone with a good salary would be to donate 70% of their income to charity, it would be morally wrong for them to only donate 65%. Satisficing consequentialism, by contrast, only requires that an act is ""good enough"" even if it is not the best possible alternative. According to this view, it is possible to do more than one is morally required to do. Mohism in ancient Chinese philosophy is one of the earliest forms of consequentialism. It arose in the 5th century BCE and argued that political action should promote justice as a means to increase the welfare of the people.",0,Wikipedia,Ethics,https://en.wikipedia.org/wiki/Ethics,,Ethics,wikipedia_api
human_wiki_0215,"The International Space Station (ISS) is a large space station that was assembled and is maintained in low Earth orbit by a collaboration of five space agencies and their contractors: NASA (United States), Roscosmos (Russia), ESA (Europe), JAXA (Japan), and CSA (Canada). As the largest space station ever constructed, it primarily serves as a platform for conducting scientific experiments in microgravity and studying the space environment. The station is divided into two main sections: the Russian Orbital Segment (ROS), developed by Roscosmos, and the US Orbital Segment (USOS), built by NASA, ESA, JAXA, and CSA. A striking feature of the ISS is the Integrated Truss Structure, which connect the station's vast system of solar panels and radiators to its pressurized modules. These modules support diverse functions, including scientific research, crew habitation, storage, spacecraft control, and airlock operations. The ISS has eight docking and berthing ports for visiting spacecraft. The station orbits the Earth at an average altitude of 400 kilometres (250 miles) and circles the Earth in roughly 93 minutes, completing 15.5 orbits per day. The ISS programme combines two previously planned crewed Earth-orbiting stations: the United States' Space Station Freedom and the Soviet Union's Mir-2. The first ISS module was launched in 1998, with major components delivered by Proton and Soyuz rockets and the Space Shuttle. Long-term occupancy began on 2 November 2000, with the arrival of the Expedition 1 crew. Since then, the ISS has remained continuously inhabited for 25 years and 30 days, the longest continuous human presence in space. As of August 2025, 290 individuals from 26 countries had visited the station. Future plans for the ISS include the addition of at least one module, Axiom Space's Payload Power Thermal Module. The station is expected to remain operational until the end of 2030, after which it will be de-orbited using the US Deorbit Vehicle.",0,Wikipedia,International Space Station,https://en.wikipedia.org/wiki/International_Space_Station,,International_Space_Station,wikipedia_api
human_wiki_0216,"Conception Purpose The ISS was originally intended to be a laboratory, observatory, and factory while providing transportation, maintenance, and a low Earth orbit staging base for possible future missions to the Moon, Mars, and asteroids. However, not all of the uses envisioned in the initial memorandum of understanding between NASA and Roscosmos have been realised. In the 2010 United States National Space Policy, the ISS was given additional roles of serving commercial, diplomatic, and educational purposes.",0,Wikipedia,International Space Station,https://en.wikipedia.org/wiki/International_Space_Station,,International_Space_Station,wikipedia_api
human_wiki_0217,"Scientific research The ISS provides a platform to conduct scientific research, with power, data, cooling, and crew available to support experiments. Small uncrewed spacecraft can also provide platforms for experiments, especially those involving zero gravity and exposure to space, but space stations offer a long-term environment where studies can be performed potentially for decades, combined with ready access by human researchers. The ISS simplifies individual experiments by allowing groups of experiments to share the same launches and crew time. Research is conducted in a wide variety of fields, including astrobiology, astronomy, physical sciences, materials science, space weather, meteorology, and human research including space medicine and the life sciences. Scientists on Earth have timely access to the data and can suggest experimental modifications to the crew. If follow-on experiments are necessary, the routinely scheduled launches of resupply craft allows new hardware to be launched with relative ease. Crews fly expeditions of several months' duration, providing approximately 160 man-hours per week of labour with a crew of six. However, a considerable amount of crew time is taken up by station maintenance. Perhaps the most notable ISS experiment is the Alpha Magnetic Spectrometer (AMS), which is intended to detect dark matter and answer other fundamental questions about our universe. According to NASA, the AMS is as important as the Hubble Space Telescope. Currently docked on station, it could not have been easily accommodated on a free flying satellite platform because of its power and bandwidth needs. On 3 April 2013, scientists reported that hints of dark matter may have been detected by the AMS. According to the scientists, ""The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays"". The space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind, in addition to cosmic rays), high vacuum, extreme temperatures, and microgravity. Some simple forms of life called extremophiles, as well as small invertebrates called tardigrades can survive in this environment in an extremely dry state through desiccation. Medical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy, bone loss, and fluid shift. These data will be used to determine whether high duration human spaceflight and space colonisation are feasible. In 2006, data on bone loss and muscular atrophy suggested that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars. Medical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult. In August 2020, scientists reported that bacteria from Earth, particularly Deinococcus radiodurans bacteria, which is highly resistant to environmental hazards, were found to survive for three years in outer space, based on studies conducted on the International Space Station. These findings supported the notion of panspermia, the hypothesis that life exists throughout the Universe, distributed in various ways, including space dust, meteoroids, asteroids, comets, planetoids or contaminated spacecraft. Remote sensing of the Earth, astronomy, and deep space research on the ISS have significantly increased during the 2010s after the completion of the US Orbital Segment in 2011. Throughout the more than 20 years of the ISS program, researchers aboard the ISS and on the ground have examined aerosols, ozone, lightning, and oxides in Earth's atmosphere, as well as the Sun, cosmic rays, cosmic dust, antimatter, and dark matter in the universe. Examples of Earth-viewing remote sensing experiments that have flown on the ISS are the Orbiting Carbon Observatory 3, ISS-RapidScat, ECOSTRESS, the Global Ecosystem Dynamics Investigation, and the Cloud Aerosol Transport System. ISS-based astronomy telescopes and experiments include SOLAR, the Neutron Star Interior Composition Explorer, the Calorimetric Electron Telescope, the Monitor of All-sky X-ray Image (MAXI), and the Alpha Magnetic Spectrometer.",0,Wikipedia,International Space Station,https://en.wikipedia.org/wiki/International_Space_Station,,International_Space_Station,wikipedia_api
human_wiki_0218,"Freefall Researchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of the data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues and the unusual protein crystals that can be formed in space. Investigating the physics of fluids in microgravity will provide better models of the behaviour of fluids. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. Examining reactions that are slowed by low gravity and low temperatures will improve our understanding of superconductivity. The study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on Earth. Other areas of interest include the effect of low gravity on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve knowledge about energy production and lead to economic and environmental benefits.",0,Wikipedia,International Space Station,https://en.wikipedia.org/wiki/International_Space_Station,,International_Space_Station,wikipedia_api
human_wiki_0219,"Exploration The ISS provides a location in the relative safety of low Earth orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance, and repair and replacement activities on-orbit. This will help develop essential skills in operating spacecraft farther from Earth, reduce mission risks, and advance the capabilities of interplanetary spacecraft. Referring to the MARS-500 experiment, a crew isolation experiment conducted on Earth, ESA states, ""Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations"". Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a ""shorter version"" of MARS-500 may be carried out on the ISS. In 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, ""When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars."" A crewed mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other four partners that China, India, and South Korea be invited to join the ISS partnership. NASA chief Charles Bolden stated in February 2011, ""Any mission to Mars is likely to be a global effort."" Currently, US federal legislation prevents NASA co-operation with China on space projects without approval by the FBI and Congress.",0,Wikipedia,International Space Station,https://en.wikipedia.org/wiki/International_Space_Station,,International_Space_Station,wikipedia_api
human_wiki_0220,"Psychology is the scientific study of behavior and mind. Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psychologists aim to understand the behavior of individuals and groups. A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors. As part of an interdisciplinary field, psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind. Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation. While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society. Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings. Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media.",0,Wikipedia,Psychology,https://en.wikipedia.org/wiki/Psychology,,Psychology,wikipedia_api
human_wiki_0221,"Etymology and definitions The word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word psychology derives from -λογία -logia, which means ""study"" or ""research"". The word psychology was first used in the Renaissance. In its Latin form psychiologia, it was first employed by the Croatian humanist and Latinist Marko Marulić in his book Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the decade 1510–1520 The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to ""Anatomy, which treats the Body, and Psychology, which treats of the Soul."" Ψ (psi), the first letter of the Greek word psyche from which the term psychology is derived, is commonly associated with the field of psychology. In 1890, William James defined psychology as ""the science of mental life, both of its phenomena and their conditions."" This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by John B. Watson, who in 1913 asserted the methodological behaviorist view of psychology as a purely objective experimental branch of natural science, the theoretical goal of which ""is the prediction and control of behavior."" Since James defined ""psychology"", the term more strongly implicates scientific experimentation. Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with psychology professionals' understanding.",0,Wikipedia,Psychology,https://en.wikipedia.org/wiki/Psychology,,Psychology,wikipedia_api
human_wiki_0222,"History The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise), addressed the workings of the mind. As early as the 4th century BCE, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes. In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BC Aristotle suggested that it was the heart. In China, the foundations of psychological thought emerged from the philosophical works of ancient thinkers like Laozi and Confucius, as well as the teachings of Buddhism. This body of knowledge drew insights from introspection, observation, and techniques for focused thinking and behavior. It viewed the universe as comprising physical and mental realms, along with the interplay between the two. Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function. Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India. Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant advanced the idea of anthropology as a discipline, with psychology an important subdivision. Kant, however, explicitly rejected the idea of an experimental psychology, writing that ""the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object."" In 1783, Ferdinand Ueberwasser (1752–1812) designated himself Professor of Empirical Psychology and Logic and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars. At the end of the Napoleonic era, Prussian authorities discontinued the Old University of Münster. Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation. In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's crowded ""lunatic"" asylums.",0,Wikipedia,Psychology,https://en.wikipedia.org/wiki/Psychology,,Psychology,wikipedia_api
human_wiki_0223,"Beginning of experimental psychology Philosopher John Stuart Mill believed that the human mind was open to scientific investigation, even if the science is in some ways inexact. Mill proposed a ""mental chemistry"" in which elementary thoughts could combine into ideas of greater complexity. Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s. He articulated the principle that human perception of a stimulus varies logarithmically according to its intensity. The principle became known as the Weber–Fechner law. Fechner's 1860 Elements of Psychophysics challenged Kant's negative view with regard to conducting quantitative research on the mind. Fechner's achievement was to show that ""mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods."" In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, where he established the psychological laboratory that brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of materials. Paul Flechsig and Emil Kraepelin soon created another influential laboratory at Leipzig, a psychology-related lab, that focused more on experimental psychiatry. James McKeen Cattell, a professor of psychology at the University of Pennsylvania and Columbia University and the co-founder of Psychological Review, was the first professor of psychology in the United States. The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was a 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting. In the early 20th century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology of Fritz Perls. The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintain that whole of experience is important, ""and is something else than the sum of its parts, because summing is a meaningless procedure, whereas the whole-part relationship is meaningful."" Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally influential. The lab was located at Johns Hopkins University. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo. Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta. Wundt's students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests of mental ability. Cattell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Witmer focused on the mental testing of children; Scott, on employee selection. Another student of Wundt, the Englishman Edward Titchener, created the psychology program at Cornell University and advanced ""structuralist"" psychology. The idea behind structuralism was to analyze and classify different aspects of the mind, primarily through the method of introspection. William James, John Dewey, and Harvey Carr advanced the idea of functionalism, an expansive approach to psychology that underlined the Darwinian idea of a behavior's usefulness to the individual. In 1890, James wrote an influential book, The Principles of Psychology, which expanded on the structuralism. He memorably described ""stream of consciousness."" James's ideas interested many American students in the emerging discipline. Dewey integrated psychology with societal concerns, most notably by promoting progressive education, inculcating moral values in children, and assimilating immigrants. A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. In Russia, too, researchers placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, ""Who Is to Develop Psychology and How?"" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic view of human behavior. The Russian-Soviet physiologist Ivan Pavlov discovered in dogs a learning process that was later termed ""classical conditioning"" and applied the process to human beings.",0,Wikipedia,Psychology,https://en.wikipedia.org/wiki/Psychology,,Psychology,wikipedia_api
human_wiki_0224,"Consolidation and funding One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted from 1885 to 1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the 400 attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held at different locations in Europe and with wide international participation. The Sixth Congress, held in Geneva in 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA. Tokyo Imperial University led the way in bringing new psychology to the East. New ideas about psychology diffused from Japan into China. American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests (""Army Alpha"" and ""Army Beta"") to almost 1.8 million soldiers. Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research. Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing. Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S. Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes. In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England. During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology by way of the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941. He observed that ""the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government."" Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test to assess the ability of millions of soldiers. The Army also engaged in large-scale psychological research of troop morale and mental health. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency (CIA) to fund research on psychological warfare. In 1965, public controversy called attention to the Army's Project Camelot, the ""Manhattan Project"" of social science, an effort which enlisted psychologists and anthropologists to analyze the plans and policies of foreign countries for strategic purposes. In Germany after World War I, psychology held institutional power through the military, which was subsequently expanded along with the rest of the military during Nazi Germany. Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler, founders of psychoanalysis who were also Jewish. The Göring Institute was well-financed throughout the war with a mandate to create a ""New German Psychotherapy."" This psychotherapy aimed to align suitable Germans with the overall goals of the Reich. As described by one physician, ""Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft."" Psychologists were to provide Seelenführung [lit., soul guidance], the leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process. After the war, new institutions were created although some psychologists, because of their Nazi affiliation, were discredited. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche. With funding from the Rockefeller Foundation, Mitscherlich established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students. After the Russian Revolution, the Bolsheviks promoted psychology as a way to engineer the ""New Man"" of socialism. Consequently, university psychology departments trained large numbers of students in psychology. At the completion of training, positions were made available for those students at schools, workplaces, cultural institutions, and in the military. The Russian state emphasized pedology and the study of child development. Lev Vygotsky became prominent in the field of child development. The Bolsheviks also promoted free love and embraced the doctrine of psychoanalysis as an antidote to sexual repression. Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet Union. Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society. Following World War II, Jewish psychologists past and present, including Lev Vygotsky, A.R. Luria, and Aron Zalkind, were denounced; Ivan Pavlov (posthumously) and Stalin himself were celebrated as heroes of Soviet psychology. Soviet academics experienced a degree of liberalization during the Khrushchev Thaw. The topics of cybernetics, linguistics, and genetics became acceptable again. The new field of engineering psychology emerged. The field involved the study of the mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior. Twentieth-century Chinese psychology originally modeled itself on U.S. psychology, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning. Chinese psychologists were drawn to the idea that education would enable modernization. John Dewey, who lectured to Chinese audiences between 1919 and 1921, had a significant influence on psychology in China. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism. After the Chinese Communist Party gained control of the country, the Stalinist Soviet Union became the major influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved means of behavior change. Chinese psychologists elaborated on Lenin's model of a ""reflective"" consciousness, envisioning an ""active consciousness"" (pinyin: tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of ""recognition"" (pinyin: jen-shih) which referred to the interface between individual perceptions and the socially accepted worldview; failure to correspond with party doctrine was ""incorrect recognition."" Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951, the academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Because most leading psychologists were educated in the United States, the first concern of the academy was the re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for the purpose of a nationally cohesive education remained a central goal of the discipline.",0,Wikipedia,Psychology,https://en.wikipedia.org/wiki/Psychology,,Psychology,wikipedia_api
human_wiki_0225,"Diabetes mellitus, commonly known as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough of the hormone insulin or the cells of the body becoming unresponsive to insulin's effects. Classic symptoms include the three Ps: polydipsia (excessive thirst), polyuria (excessive urination), polyphagia (excessive hunger), weight loss, and blurred vision. If left untreated, the disease can lead to various health complications, including disorders of the cardiovascular system, eye, kidney, and nerves. Diabetes accounts for approximately 4.2 million deaths every year, with an estimated 1.5 million caused by either untreated or poorly treated diabetes. The major types of diabetes are type 1 and type 2. The most common treatment for type 1 is insulin replacement therapy (insulin injections), while anti-diabetic medications (such as metformin and semaglutide or tirzepatide) and lifestyle modifications can be used to manage type 2. Gestational diabetes, a form that sometimes arises during pregnancy, normally resolves shortly after delivery. Type 1 diabetes is an autoimmune condition where the body's immune system attacks the beta cells (β-cell) in the pancreas, preventing the production of insulin. This condition is typically present from birth or develops early in life. Type 2 diabetes occurs when the body becomes resistant to insulin, meaning the cells do not respond effectively to it, and thus, glucose remains in the bloodstream instead of being absorbed by the cells. Additionally, diabetes can also result from other specific causes, such as genetic conditions (monogenic diabetes syndromes like neonatal diabetes and maturity-onset diabetes of the young), diseases affecting the pancreas (such as pancreatitis), or the use of certain medications and chemicals (such as glucocorticoids, other specific drugs and after organ transplantation). The number of people diagnosed as living with diabetes has increased sharply in recent decades, from 200 million in 1990 to 830 million by 2022. It affects one in seven of the adult population, with type 2 diabetes accounting for more than 95% of cases. These numbers have already risen beyond earlier projections of 783 million adults by 2045. The prevalence of the disease continues to increase, most dramatically in low- and middle-income nations. Rates are similar in women and men, with diabetes being the seventh leading cause of death globally. The global expenditure on diabetes-related healthcare is an estimated US$760 billion a year.",0,Wikipedia,Diabetes,https://en.wikipedia.org/wiki/Diabetes,,Diabetes,wikipedia_api
human_wiki_0226,"Signs and symptoms Common symptoms of diabetes include increased thirst, frequent urination, extreme hunger, and unintended weight loss. Several other non-specific signs and symptoms may also occur, including fatigue, blurred vision, sweet smelling urine/semen and genital itchiness due to Candida infection. About half of affected individuals may also be asymptomatic. Type 1 presents abruptly following a pre-clinical phase, while type 2 has a more insidious onset; patients may remain asymptomatic for many years. Diabetic ketoacidosis is a medical emergency that occurs most commonly in type 1, but may also occur in type 2 if it has been longstanding or if the individual has significant β-cell dysfunction. Excessive production of ketone bodies leads to signs and symptoms including nausea, vomiting, abdominal pain, the smell of acetone in the breath, deep breathing known as Kussmaul breathing, and in severe cases decreased level of consciousness. Hyperosmolar hyperglycemic state is another emergency characterized by dehydration secondary to severe hyperglycemia, with resultant hypernatremia leading to an altered mental state and possibly coma. Hypoglycemia is a recognized complication of insulin treatment used in diabetes. An acute presentation can include mild symptoms such as sweating, trembling, and palpitations, to more serious effects including impaired cognition, confusion, seizures, coma, and rarely death. Recurrent hypoglycemic episodes may lower the glycemic threshold at which symptoms occur, meaning mild symptoms may not appear before cognitive deterioration begins to occur.",0,Wikipedia,Diabetes,https://en.wikipedia.org/wiki/Diabetes,,Diabetes,wikipedia_api
human_wiki_0227,"Long-term complications The major long-term complications of diabetes relate to damage to blood vessels at both macrovascular and microvascular levels. Diabetes doubles the risk of cardiovascular disease, and about 75% of deaths in people with diabetes are due to coronary artery disease. Other macrovascular morbidities include stroke and peripheral artery disease. Microvascular disease affects the eyes, kidneys, and nerves. Damage to the retina, known as diabetic retinopathy, is the most common cause of blindness in people of working age. The eyes can also be affected in other ways, including development of cataract and glaucoma. It is recommended that people with diabetes visit an optometrist or ophthalmologist once a year. Diabetic nephropathy is a major cause of chronic kidney disease, accounting for over 50% of patients on dialysis in the United States. Diabetic neuropathy, damage to nerves, manifests in various ways, including sensory loss, neuropathic pain, and autonomic dysfunction (such as postural hypotension, diarrhea, and erectile dysfunction). Loss of pain sensation predisposes to trauma that can lead to diabetic foot problems (such as ulceration), the most common cause of non-traumatic lower-limb amputation. Hearing loss is another long-term complication associated with diabetes. Based on extensive data and numerous cases of gallstone disease, it appears that a causal link might exist between type 2 diabetes and gallstones. People with diabetes are at a higher risk of developing gallstones compared to those without diabetes. There is a link between cognitive deficit and diabetes; studies have shown that diabetic individuals are at a greater risk of cognitive decline, and have a greater rate of decline compared to those without the disease. Diabetes increases the risk of dementia, and the earlier that one is diagnosed with diabetes, the higher the risk becomes. The condition also predisposes to falls in the elderly, especially those treated with insulin.",0,Wikipedia,Diabetes,https://en.wikipedia.org/wiki/Diabetes,,Diabetes,wikipedia_api
human_wiki_0228,"Type 1 diabetes Type 2 diabetes Hybrid forms of diabetes(including slowly evolving, immune-mediated diabetes of adults and ketosis-prone type 2 diabetes) Hyperglycemia first detected during pregnancy Other specific types Unclassified diabetes Diabetes is a more variable disease than once thought, and individuals may have a combination of forms.",0,Wikipedia,Diabetes,https://en.wikipedia.org/wiki/Diabetes,,Diabetes,wikipedia_api
human_wiki_0229,"Type 1 Type 1 accounts for 5 to 10% of diabetes cases and is the most common type of diabetes diagnosed in patients under 20 years; however, the older term ""juvenile-onset diabetes"" is no longer used as onset in adulthood is possible. The disease is characterized by loss of the insulin-producing beta cells of the pancreatic islets, leading to severe insulin deficiency, and can be further classified as immune-mediated or idiopathic (without known cause). The majority of cases are immune-mediated, in which a T cell-mediated autoimmune attack causes loss of beta cells and thus insulin deficiency. Patients often have irregular and unpredictable blood sugar levels due to very low insulin and an impaired counter-response to hypoglycemia.  Type 1 diabetes is partly inherited, with multiple genes, including certain HLA genotypes, known to influence the risk of diabetes. In genetically susceptible people, the onset of diabetes can be triggered by one or more environmental factors, such as a viral infection or diet. Several viruses have been implicated, but to date there is no stringent evidence to support this hypothesis in humans. The genes that are responsible for diabetes are still being researched, but scientists have narrowed them down by investigating the gene mutations related to the capability of the body’s β-cells to produce insulin. Genes related to environmental responses (metabolism, pregnancy symptoms, autoimmune disorder development, etc.) also contribute to a person’s amount of genetic risk for diabetes. The occurrence of diabetes in monozygotic and dizygotic twins has been tested, and these rates can give insight into the genetic component of diabetes. In type 1 diabetes, the chance of monozygotic twins both developing the disease was greater than the risk for dizygotic twins. However, the rate of any siblings contracting the disease was much greater with type 2 diabetes. This indicates that there must be a large environmental factor involved in type 2, and some genetic factor with type 1. It must be noted, also, that type 1 diabetes has only about a 50% concordance rate (the percentage of two identical twins both having the condition). So, it is not fully genetic, but the results from the twin studies point to some inherited risk. Type 1 diabetes can occur at any age, and a significant proportion is diagnosed during adulthood. Latent autoimmune diabetes of adults (LADA) is the diagnostic term applied when type 1 diabetes develops in adults; it has a slower onset than the same condition in children. Given this difference, some use the unofficial term ""type 1.5 diabetes"" for this condition. Adults with LADA are frequently initially misdiagnosed as having type 2 diabetes, based on age rather than a cause. LADA leaves adults with higher levels of insulin production than type 1 diabetes, but not enough insulin production for healthy blood sugar levels.",0,Wikipedia,Diabetes,https://en.wikipedia.org/wiki/Diabetes,,Diabetes,wikipedia_api
human_wiki_0230,"Engineering is the practice of using natural science, mathematics, and the engineering design process to solve problems within technology, increase efficiency and productivity, and improve systems. The traditional disciplines of engineering are civil, mechanical, electrical, and chemical. The academic discipline of engineering encompasses a broad range of more specialized subfields, and each can have a more specific emphasis for applications of mathematics and science. In turn, modern engineering practice spans multiple fields of engineering, which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems. For related terms, see glossary of engineering. As a human endeavor, engineering has existed since ancient times, starting with the six classic simple machines. Examples of large-scale engineering projects from antiquity include impressive structures like the pyramids, elegant temples such as the Parthenon, and water conveyances like hulled watercraft, canals, and the Roman aqueduct. Early machines were powered by humans and animals, then later by wind. Machines of war were invented for siegecraft. In Europe, the scientific and industrial revolutions advanced engineering into a scientific profession and resulted in continuing technological improvements. The steam engine provided much greater power than animals, leading to mechanical propulsion for ships and railways. Further scientific advances resulted in the application of engineering to electrical, chemical, and aerospace requirements, plus the use of new materials for greater efficiencies. The word engineering is derived from the Latin ingenium. Engineers typically follow a code of ethics that favors honesty and integrity, while being dedicated to public safety and welfare. Engineering tasks involve finding optimal solutions based on constraints, with testing and simulations being used prior to production. When a deployed product fails, forensic engineering is used to determine what went wrong in order to find a fix. Much of this product lifecycle management is now assisted with computer software, from design to testing and manufacturing. At larger scales, this process normally funded by a company, multiple investors, or the government, so a knowledge of economics and business practices is needed.",0,Wikipedia,Engineering,https://en.wikipedia.org/wiki/Engineering,,Engineering,wikipedia_api
human_wiki_0231,"The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.",0,Wikipedia,Engineering,https://en.wikipedia.org/wiki/Engineering,,Engineering,wikipedia_api
human_wiki_0232,"History Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc. The term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to ""a constructor of military engines"". In this context, now obsolete, an ""engine"" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers. The word ""engine"" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning ""innate quality, especially mental power, hence a clever invention."" Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.",0,Wikipedia,Engineering,https://en.wikipedia.org/wiki/Engineering,,Engineering,wikipedia_api
human_wiki_0233,"Ancient era The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World. The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times. The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC. The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c. 3000 BC, and then in ancient Egyptian technology c. 2000 BC. The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC, and ancient Egypt during the Twelfth Dynasty (1991–1802 BC). The screw, the last of the simple machines to be invented, first appeared in Mesopotamia during the Neo-Assyrian period (911–609) BC. The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza. The earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC. The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC. Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy. Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation. Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC. Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush. Wooden plank-built seafaring ships were being engineered and built during the bronze age, as evidenced by the Uluburun shipwreck, dated from around 1300 BCE. Ancient Greece developed machines in both civilian and military domains, as evidenced by the writings of Philo of Byzantium and others. The Antikythera mechanism, an early known mechanical analog computer, and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering. Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC, the trireme, the ballista and the catapult, the trebuchet by Chinese circa 6th-5th century BCE.",0,Wikipedia,Engineering,https://en.wikipedia.org/wiki/Engineering,,Engineering,wikipedia_api
human_wiki_0234,"Middle Ages The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD. The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt. The cotton gin was invented in India by the 6th century AD, and the spinning wheel was invented in the Islamic world by the early 11th century, both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century. The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century. In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.",0,Wikipedia,Engineering,https://en.wikipedia.org/wiki/Engineering,,Engineering,wikipedia_api
human_wiki_0235,"Environment and natural resources Nature conservation, the protection and management of the environment and natural resources Wetland conservation, protecting and preserving areas where water exists at or near the Earth's surface, such as swamps, marshes and bogs. Conservation biology, the science of protection and management of biodiversity Conservation movement, political, environmental, or social movement that seeks to protect natural resources, including biodiversity and habitat Conservation organization, an organization dedicated to protection and management of the environment or natural resources Wildlife conservation, the practice of protecting wild species and their habitats in order to prevent species from going extinct Conservation (magazine), published by the Society for Conservation Biology from 2000 to 2014 Conservation Biology (journal), scientific journal of the Society for Conservation Biology Water conservation, the aim to sustainably manage the natural resource of fresh water, protect the hydrosphere, and meet current and future human demand.",0,Wikipedia,Conservation,https://en.wikipedia.org/wiki/Conservation,,Conservation,wikipedia_api
human_wiki_0236,"Physical laws Conservation law, principle that a particular measurable property of an isolated physical system does not change as the system evolves over time Conservation of energy, principle that the total energy of an isolated system remains constant over time Conservation of mass, principle that the mass of any closed system must remain constant over time Conservation of linear momentum, principle that the total momentum of a closed system is constant Conservation of angular momentum, principle that total angular momentum of a system is constant Charge conservation, principle that the total electric charge in an isolated system never changes",0,Wikipedia,Conservation,https://en.wikipedia.org/wiki/Conservation,,Conservation,wikipedia_api
human_wiki_0237,"Land designated for conservation Conservation area (United Kingdom), an area considered worthy of preservation because of its architectural or historic interest Conservation designation, the status of an area of land in terms of conservation or protection Conservation district, government entities that help manage and protect land and water resources in U.S. states and insular areas Conservation easement, a power of an organization to constrain the exercise of rights otherwise held by a landowner to achieve certain conservation purposes Conservation community, a community committed to saving large parcels of land from ecological degradation",0,Wikipedia,Conservation,https://en.wikipedia.org/wiki/Conservation,,Conservation,wikipedia_api
human_wiki_0238,"Other uses Conservation (psychology), learning development of logical thinking, according to Jean Piaget Conservation and restoration of cultural property, the conservation or restoration of cultural heritage Conservation and restoration of immovable cultural property Conservation science (cultural property), the interdisciplinary study of conservation of cultural works",0,Wikipedia,Conservation,https://en.wikipedia.org/wiki/Conservation,,Conservation,wikipedia_api
human_wiki_0239,"See also All pages with titles beginning with Conservation  All pages with titles containing Conservation Conservation ministry (disambiguation) Conservation science (disambiguation) Conservatism (disambiguation) Conserve (disambiguation) Conserved quantity, in mathematics, a function of dependent variables that remains constant Conserved sequence, similar or identical sequences of nucleic acids, proteins, protein structures, or polymeric carbohydrates Conversation (disambiguation) Preservation (disambiguation) Sustainable forest management Wildlife management, management to conserve wild species and their habitats",0,Wikipedia,Conservation,https://en.wikipedia.org/wiki/Conservation,,Conservation,wikipedia_api
human_wiki_0240,"In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and ""training"" them to process data. The adjective ""deep"" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.",0,Wikipedia,Deep learning,https://en.wikipedia.org/wiki/Deep_learning,,Deep_learning,wikipedia_api
human_wiki_0241,"Overview Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction. The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.",0,Wikipedia,Deep learning,https://en.wikipedia.org/wiki/Deep_learning,,Deep_learning,wikipedia_api
human_wiki_0242,"Interpretations Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit. The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.",0,Wikipedia,Deep learning,https://en.wikipedia.org/wiki/Deep_learning,,Deep_learning,wikipedia_api
human_wiki_0243,"History Before 1980 There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on ""Intelligent Machinery""  that was not published in his lifetime, containing ""ideas related to artificial evolution and learning RNNs"". Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons ""with adaptive preterminal networks"" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) ""functionally equivalent to a variation of"" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or ""gates"". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.",0,Wikipedia,Deep learning,https://en.wikipedia.org/wiki/Deep_learning,,Deep_learning,wikipedia_api
human_wiki_0244,"1980s-2000s The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.  In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This ""neural history compressor"" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. The ""P"" in ChatGPT refers to such pre-training. Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem.  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a ""forget gate"", introduced in 1999, which became the standard RNN architecture. In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014, this principle was used in generative adversarial networks (GANs). During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics. Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.",0,Wikipedia,Deep learning,https://en.wikipedia.org/wiki/Deep_learning,,Deep_learning,wikipedia_api
human_wiki_0245,"The brain is an organ that serves as the center of the nervous system in all vertebrate and most invertebrate animals. It consists of nervous tissue and is typically located in the head (cephalization), usually near organs for special senses such as vision, hearing, and olfaction. Being the most specialized organ, it is responsible for receiving information from the sensory nervous system, processing that information (thought, cognition, and intelligence) and the coordination of motor control (muscle activity and endocrine system). While invertebrate brains arise from paired segmental ganglia (each of which is only responsible for the respective body segment) of the ventral nerve cord, vertebrate brains develop axially from the midline dorsal nerve cord as a vesicular enlargement at the rostral end of the neural tube, with centralized control over all body segments. All vertebrate brains can be embryonically divided into three parts: the forebrain (prosencephalon, subdivided into telencephalon and diencephalon), midbrain (mesencephalon) and hindbrain (rhombencephalon, subdivided into metencephalon and myelencephalon). The spinal cord, which directly interacts with somatic functions below the head, can be considered a caudal extension of the myelencephalon enclosed inside the vertebral column. Together, the brain and spinal cord constitute the central nervous system in all vertebrates. In humans, the cerebral cortex contains approximately 14–16 billion neurons, and the estimated number of neurons in the cerebellum is 55–70 billion. Each neuron is connected by synapses to several thousand other neurons, typically communicating with one another via cytoplasmic processes known as dendrites and axons. Axons are usually myelinated and carry trains of rapid micro-electric signal pulses called action potentials to target specific recipient cells in other areas of the brain or distant parts of the body. The prefrontal cortex, which controls executive functions, is particularly well developed in humans. Physiologically, brains exert centralized control over a body's other organs. They act on the rest of the body both by generating patterns of muscle activity and by driving the secretion of chemicals called hormones. This centralized control allows rapid and coordinated responses to changes in the environment. Some basic types of responsiveness such as reflexes can be mediated by the spinal cord or peripheral ganglia, but sophisticated purposeful control of behavior based on complex sensory input requires the information-integrating capabilities of a centralized brain. The operations of individual brain cells are now understood in considerable detail but the way they cooperate in ensembles of millions is yet to be solved. Recent models in modern neuroscience treat the brain as a biological computer, very different in mechanism from a digital computer, but similar in the sense that it acquires information from the surrounding world, stores it, and processes it in a variety of ways. This article compares the properties of brains across the entire range of animal species, with the greatest attention to vertebrates. It deals with the human brain insofar as it shares the properties of other brains. The ways in which the human brain differs from other brains are covered in the human brain article. Several topics that might be covered here are instead covered there because much more can be said about them in a human context. The most important that are covered in the human brain article are brain disease and the effects of brain damage.",0,Wikipedia,Brain,https://en.wikipedia.org/wiki/Brain,,Brain,wikipedia_api
human_wiki_0246,"Structure The shape and size of the brain varies greatly between species, and identifying common features is often difficult. Nevertheless, there are a number of principles of brain architecture that apply across a wide range of species. Some aspects of brain structure are common to almost the entire range of animal species; others distinguish ""advanced"" brains from more primitive ones, or distinguish vertebrates from invertebrates. The simplest way to gain information about brain anatomy is by visual inspection, but many more sophisticated techniques have been developed. Brain tissue in its natural state is too soft to work with, but it can be hardened by immersion in alcohol or other fixatives, and then sliced apart for examination of the interior. Visually, the interior of the brain consists of areas of so-called grey matter, with a dark color, separated by areas of white matter, with a lighter color. Further information can be gained by staining slices of brain tissue with a variety of chemicals that bring out areas where specific types of molecules are present in high concentrations. It is also possible to examine the microstructure of brain tissue using a microscope, and to trace the pattern of connections from one brain area to another.",0,Wikipedia,Brain,https://en.wikipedia.org/wiki/Brain,,Brain,wikipedia_api
human_wiki_0247,"Cellular structure The brains of all species are composed primarily of two broad classes of brain cells: neurons and glial cells. Glial cells (also known as glia or neuroglia) come in several types, and perform a number of critical functions, including structural support, metabolic support, insulation, and guidance of development. Neurons, however, are usually considered the most important cells in the brain. In humans, the cerebral cortex contains approximately 14–16 billion neurons, and the estimated number of neurons in the cerebellum is 55–70 billion. Each neuron is connected by synapses to several thousand other neurons. The property that makes neurons unique is their ability to send signals to specific target cells, sometimes over long distances. They send these signals by means of an axon, which is a thin protoplasmic fiber that extends from the cell body and projects, usually with numerous branches, to other areas, sometimes nearby, sometimes in distant parts of the brain or body. The length of an axon can be extraordinary: for example, if a pyramidal cell (an excitatory neuron) of the cerebral cortex were magnified so that its cell body became the size of a human body, its axon, equally magnified, would become a cable a few centimeters in diameter, extending more than a kilometer. These axons transmit signals in the form of electrochemical pulses called action potentials, which last less than a thousandth of a second and travel along the axon at speeds of 1–100 meters per second. Some neurons emit action potentials constantly, at rates of 10–100 per second, usually in irregular patterns; other neurons are quiet most of the time, but occasionally emit a burst of action potentials. Axons transmit signals to other neurons by means of specialized junctions called synapses. A single axon may make as many as several thousand synaptic connections with other cells. When an action potential, traveling along an axon, arrives at a synapse, it causes a chemical called a neurotransmitter to be released. The neurotransmitter binds to receptor molecules in the membrane of the target cell. Synapses are the key functional elements of the brain. The essential function of the brain is cell-to-cell communication, and synapses are the points at which communication occurs. The human brain has been estimated to contain approximately 100 trillion synapses; even the brain of a fruit fly contains several million. The functions of these synapses are very diverse: some are excitatory (exciting the target cell); others are inhibitory; others work by activating second messenger systems that change the internal chemistry of their target cells in complex ways. A large number of synapses are dynamically modifiable; that is, they are capable of changing strength in a way that is controlled by the patterns of signals that pass through them. It is widely believed that activity-dependent modification of synapses is the brain's primary mechanism for learning and memory. Most of the space in the brain is taken up by axons, which are often bundled together in what are called nerve fiber tracts. A myelinated axon is wrapped in a fatty insulating sheath of myelin, which serves to greatly increase the speed of signal propagation. (There are also unmyelinated axons). Myelin is white, making parts of the brain filled exclusively with nerve fibers appear as light-colored white matter, in contrast to the darker-colored grey matter that marks areas with high densities of neuron cell bodies.",0,Wikipedia,Brain,https://en.wikipedia.org/wiki/Brain,,Brain,wikipedia_api
human_wiki_0248,"Evolution Generic bilaterian nervous system Except for a few primitive organisms such as sponges (which have no nervous system) and cnidarians (which have a diffuse nervous system consisting of a nerve net), all living multicellular animals are bilaterians, meaning animals with a bilaterally symmetric body plan (that is, left and right sides that are approximate mirror images of each other). All bilaterians are thought to have descended from a common ancestor that appeared late in the Cryogenian period, 700–650 million years ago, and it has been hypothesized that this common ancestor had the shape of a simple tubeworm with a segmented body. At a schematic level, that basic worm-shape continues to be reflected in the body and nervous system architecture of all modern bilaterians, including vertebrates. The fundamental bilateral body form is a tube with a hollow gut cavity running from the mouth to the anus, and a nerve cord with an enlargement (a ganglion) for each body segment, with an especially large ganglion at the front, called the brain. The brain is small and simple in some species, such as nematode worms; in other species, such as vertebrates, it is a large and very complex organ. Some types of worms, such as leeches, also have an enlarged ganglion at the back end of the nerve cord, known as a ""tail brain"". There are a few types of existing bilaterians that lack a recognizable brain, including echinoderms and tunicates. It has not been definitively established whether the existence of these brainless species indicates that the earliest bilaterians lacked a brain, or whether their ancestors evolved in a way that led to the disappearance of a previously existing brain structure.",0,Wikipedia,Brain,https://en.wikipedia.org/wiki/Brain,,Brain,wikipedia_api
human_wiki_0249,"Invertebrates This category includes tardigrades, arthropods, molluscs, and numerous types of worms. The diversity of invertebrate body plans is matched by an equal diversity in brain structures. Two groups of invertebrates have notably complex brains: arthropods (insects, crustaceans, arachnids, and others), and cephalopods (octopuses, squids, and similar molluscs). The brains of arthropods and cephalopods arise from twin parallel nerve cords that extend through the body of the animal. Arthropods have a central brain, the supraesophageal ganglion, with three divisions and large optical lobes behind each eye for visual processing. Cephalopods such as the octopus and squid have the largest brains of any invertebrates. There are several invertebrate species whose brains have been studied intensively because they have properties that make them convenient for experimental work:",0,Wikipedia,Brain,https://en.wikipedia.org/wiki/Brain,,Brain,wikipedia_api
human_wiki_0250,"Chemistry is the scientific study of the properties and behavior of matter. It is a physical science within the natural sciences that studies the chemical elements that make up matter and compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during reactions with other substances. Chemistry also addresses the nature of chemical bonds in chemical compounds. In the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the Moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics). Chemistry has existed under various names since ancient times. It has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry.",0,Wikipedia,Chemistry,https://en.wikipedia.org/wiki/Chemistry,,Chemistry,wikipedia_api
human_wiki_0251,"Etymology The word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry. The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'.",0,Wikipedia,Chemistry,https://en.wikipedia.org/wiki/Chemistry,,Chemistry,wikipedia_api
human_wiki_0252,"Modern principles The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory. The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.",0,Wikipedia,Chemistry,https://en.wikipedia.org/wiki/Chemistry,,Chemistry,wikipedia_api
human_wiki_0253,"A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws. Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:",0,Wikipedia,Chemistry,https://en.wikipedia.org/wiki/Chemistry,,Chemistry,wikipedia_api
human_wiki_0254,"Matter In chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well – not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.",0,Wikipedia,Chemistry,https://en.wikipedia.org/wiki/Chemistry,,Chemistry,wikipedia_api
human_wiki_0255,"Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients (divided into macro- and micro-) which can be metabolized to create energy and chemical structures; too much or too little of an essential nutrient can cause malnutrition. Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition. The type of organism determines what nutrients it needs and how it obtains them. Organisms obtain nutrients by consuming organic matter, consuming inorganic matter, absorbing light, or some combination of these. Some can produce nutrients internally by consuming basic elements, while others must consume other organisms to obtain pre-existing nutrients. All forms of life require carbon, energy, and water as well as various other molecules. Animals require complex nutrients such as carbohydrates, lipids, and proteins, obtaining them by consuming other organisms. Humans have developed agriculture and cooking to replace foraging and advance human nutrition. Plants acquire nutrients through the soil and the atmosphere. Fungi absorb nutrients around them by breaking them down and absorbing them through the mycelium.",0,Wikipedia,Nutrition,https://en.wikipedia.org/wiki/Nutrition,,Nutrition,wikipedia_api
human_wiki_0256,"History Scientific analysis of food and nutrients began during the chemical revolution in the late 18th century. Chemists in the 18th and 19th centuries experimented with different elements and food sources to develop theories of nutrition. Modern nutrition science began in the 1910s as individual micronutrients began to be identified. The first vitamin to be chemically identified was thiamine in 1926, and vitamin C was identified as a protection against scurvy in 1932. The role of vitamins in nutrition was studied in the following decades. The first recommended dietary allowances for humans were developed to address fears of disease caused by food deficiencies during the Great Depression and the Second World War. Due to its importance in human health, the study of nutrition has heavily emphasized human nutrition and agriculture, while ecology is a secondary concern.",0,Wikipedia,Nutrition,https://en.wikipedia.org/wiki/Nutrition,,Nutrition,wikipedia_api
human_wiki_0257,"Nutrients Nutrients are substances that provide energy and physical components to the organism, allowing it to survive, grow, and reproduce. Nutrients can be basic elements or complex macromolecules. Approximately 30 elements are found in organic matter, with nitrogen, carbon, and phosphorus being the most important. Macronutrients are the primary substances required by an organism, and micronutrients are substances required by an organism in trace amounts. Organic micronutrients are classified as vitamins, and inorganic micronutrients are classified as minerals. Over-nutrition of macronutrients is a major cause of obesity and increases the risk of developing various non-communicable diseases (NCDs), including type 2 diabetes, stroke, hypertension, coronary heart disease, osteoporosis, and some forms of cancer. Nutrients can also be classified as essential or nonessential, with essential meaning the body cannot synthesize the nutrient on its own. Nutrients are absorbed by the cells and used in metabolic biochemical reactions. These include fueling reactions that create precursor metabolites and energy, biosynthetic reactions that convert precursor metabolites into building block molecules, polymerizations that combine these molecules into macromolecule polymers, and assembly reactions that use these polymers to construct cellular structures.",0,Wikipedia,Nutrition,https://en.wikipedia.org/wiki/Nutrition,,Nutrition,wikipedia_api
human_wiki_0258,"Nutritional groups Organisms can be classified by how they obtain carbon and energy. Heterotrophs are organisms that obtain nutrients by consuming the carbon of other organisms, while autotrophs are organisms that produce their own nutrients from the carbon of inorganic substances like carbon dioxide. Mixotrophs are organisms that can be heterotrophs and autotrophs, including some plankton and carnivorous plants. Phototrophs obtain energy from light, while chemotrophs obtain energy by consuming chemical energy from matter. Organotrophs consume other organisms to obtain electrons, while lithotrophs obtain electrons from inorganic substances, such as water, hydrogen sulfide, dihydrogen, iron(II), sulfur, or ammonium. Prototrophs can create essential nutrients from other compounds, while auxotrophs must consume preexisting nutrients.",0,Wikipedia,Nutrition,https://en.wikipedia.org/wiki/Nutrition,,Nutrition,wikipedia_api
human_wiki_0259,"Diet In nutrition, the diet of an organism is the sum of the foods it eats. A healthy diet improves the physical and mental health of an organism. This requires ingestion and absorption of vitamins, minerals, essential amino acids from protein and essential fatty acids from fat-containing food. Carbohydrates, protein and fat play major roles in ensuring the quality of life, health and longevity of the organism. Some cultures and religions have restrictions on what is acceptable for their diet.",0,Wikipedia,Nutrition,https://en.wikipedia.org/wiki/Nutrition,,Nutrition,wikipedia_api
human_wiki_0260,"Geology is a branch of natural science concerned with the Earth and other astronomical bodies, the rocks of which they are composed, and the processes by which they change over time. The name comes from Ancient Greek  γῆ (gê) 'earth' and  λoγία (-logía) 'study of, discourse'. Modern geology significantly overlaps all other Earth sciences, including hydrology. It is integrated with Earth system science and planetary science. Geology describes the structure of the Earth on and beneath its surface and the processes that have shaped that structure. Geologists study the mineralogical composition of rocks in order to get insight into their history of formation. Geology determines the relative ages of rocks found at a given location; geochemistry (a branch of geology) determines their absolute ages. By combining various petrological, crystallographic, and paleontological tools, geologists are able to chronicle the geological history of the Earth as a whole. One aspect is to demonstrate the age of the Earth. Geology provides evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates. Geologists broadly study the properties and processes of Earth and other terrestrial planets. Geologists use a wide variety of methods to understand the Earth's structure and evolution, including fieldwork, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding natural hazards, remediating environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.",0,Wikipedia,Geology,https://en.wikipedia.org/wiki/Geology,,Geology,wikipedia_api
human_wiki_0261,"Minerals Minerals are naturally occurring elements and compounds with a definite homogeneous chemical composition and an ordered atomic arrangement. Amorphous substances that resemble a mineral are sometimes referred to as mineraloids, although there are exceptions such as georgeite and autunite. Some amorphous substances formed by geological processes are considered minerals if the original substance was a mineral before metamictisation. Each mineral has distinct physical properties, and there are many tests to determine each of them. Minerals are often identified through these tests. The specimens can be tested for:",0,Wikipedia,Geology,https://en.wikipedia.org/wiki/Geology,,Geology,wikipedia_api
human_wiki_0262,"Color: Minerals are grouped by their color. Mostly diagnostic but impurities can change a mineral's color. Streak: Performed by scratching the sample on a porcelain plate. The color of the streak can help identify the mineral. Hardness: The resistance of a mineral to scratching or indentation. Breakage pattern: A mineral can either show fracture or cleavage, the former being breakage of uneven surfaces, and the latter a breakage along closely spaced parallel planes. Luster: Quality of light reflected from the surface of a mineral. Examples are metallic, pearly, waxy, dull. Specific gravity: the weight of a specific volume of a mineral. Effervescence: Involves dripping hydrochloric acid on the mineral to test for fizzing. Magnetism: Involves using a magnet to test for magnetism. Taste: Minerals can have a distinctive taste such as halite (which tastes like table salt).",0,Wikipedia,Geology,https://en.wikipedia.org/wiki/Geology,,Geology,wikipedia_api
human_wiki_0263,"Rock A rock is any naturally occurring solid mass or aggregate of minerals or mineraloids. Most research in geology is associated with the study of rocks, as they provide the primary record of the majority of the geological history of the Earth. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle illustrates the relationships among them (see diagram). When a rock solidifies or crystallizes from melt (magma or lava), it is an igneous rock. The active flow of molten rock is closely studied in volcanology, and igneous petrology aims to determine the history of igneous rocks from their original molten source to their final crystallization. Rocks can be weathered and eroded, then redeposited and lithified into a sedimentary rock. Sedimentary rocks are mainly divided into four categories: sandstone, shale, carbonate, and evaporite. This group of classifications focuses partly on the size of sedimentary particles (sandstone and shale), and partly on mineralogy and formation processes (carbonation and evaporation). Igneous and sedimentary rocks can then be turned into metamorphic rocks by heat and pressure that change its mineral content, resulting in a characteristic fabric. All three types may melt again, and when this happens, new magma is formed, from which an igneous rock may once again solidify. Organic matter, such as coal, bitumen, oil, and natural gas, is linked mainly to organic-rich sedimentary rocks. To study all three types of rock, geologists evaluate the minerals of which they are composed and their other physical properties, such as texture and fabric.",0,Wikipedia,Geology,https://en.wikipedia.org/wiki/Geology,,Geology,wikipedia_api
human_wiki_0264,"Unlithified material Geologists study unlithified materials (referred to as superficial deposits) that lie above the bedrock. This study is often known as Quaternary geology, after the Quaternary period of geologic history, which is the most recent period of geologic time.",0,Wikipedia,Geology,https://en.wikipedia.org/wiki/Geology,,Geology,wikipedia_api
human_wiki_0265,"Neuroscience is the scientific study of the nervous system (the brain, spinal cord, and peripheral nervous system), its functions, and its disorders. It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia, and neural circuits. The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the ""epic challenge"" of the biological sciences. The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales. The techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor, and cognitive tasks in the brain.",0,Wikipedia,Neuroscience,https://en.wikipedia.org/wiki/Neuroscience,,Neuroscience,wikipedia_api
human_wiki_0266,"History The earliest study of the nervous system dates to ancient Egypt. Trepanation, the surgical practice of either drilling or scraping a hole into the skull for the purpose of curing head injuries or mental disorders, or relieving cranial pressure, was first recorded during the Neolithic period. Manuscripts dating to 1700 BC indicate that the Egyptians had some knowledge about symptoms of brain damage. Early views on the function of the brain regarded it to be a ""cranial stuffing"" of sorts. In Egypt, from the late Middle Kingdom onwards, the brain was regularly removed in preparation for mummification. It was believed at the time that the heart was the seat of intelligence. According to Herodotus, the first step of mummification was to ""take a crooked piece of iron, and with it draw out the brain through the nostrils, thus getting rid of a portion, while the skull is cleared of the rest by rinsing with drugs."" The view that the heart was the source of consciousness was not challenged until the time of the Greek physician Hippocrates. He believed that the brain was not only involved with sensation—since most specialized organs (e.g., eyes, ears, tongue) are located in the head near the brain—but was also the seat of intelligence. Plato also speculated that the brain was the seat of the rational part of the soul. Aristotle, however, believed the heart was the center of intelligence and that the brain regulated the amount of heat from the heart. This view was generally accepted until the Roman physician Galen, a follower of Hippocrates and physician to Roman gladiators, observed that his patients lost their mental faculties when they had sustained damage to their brains. Abulcasis, Averroes, Avicenna, Avenzoar, and Maimonides, active in the Medieval Muslim world, described a number of medical problems related to the brain. In Renaissance Europe, Vesalius (1514–1564), René Descartes (1596–1650), Thomas Willis (1621–1675) and Jan Swammerdam (1637–1680) also made several contributions to neuroscience.",0,Wikipedia,Neuroscience,https://en.wikipedia.org/wiki/Neuroscience,,Neuroscience,wikipedia_api
human_wiki_0267,"Luigi Galvani's pioneering work in the late 1700s set the stage for studying the electrical excitability of muscles and neurons. In 1843 Emil du Bois-Reymond demonstrated the electrical nature of the nerve signal, whose speed Hermann von Helmholtz proceeded to measure, and in 1875 Richard Caton found electrical phenomena in the cerebral hemispheres of rabbits and monkeys. Adolf Beck published in 1890 similar observations of spontaneous electrical activity of the brain of rabbits and dogs. Studies of the brain became more sophisticated after the invention of the microscope and the development of a staining procedure by Camillo Golgi during the late 1890s. The procedure used a silver chromate salt to reveal the intricate structures of individual neurons. His technique was used by Santiago Ramón y Cajal and led to the formation of the neuron doctrine, the hypothesis that the functional unit of the brain is the neuron. Golgi and Ramón y Cajal shared the Nobel Prize in Physiology or Medicine in 1906 for their extensive observations, descriptions, and categorizations of neurons throughout the brain. In parallel with this research, in 1815 Jean Pierre Flourens induced localized lesions of the brain in living animals to observe their effects on motricity, sensibility and behavior. Work with brain-damaged patients by Marc Dax in 1836 and Paul Broca in 1865 suggested that certain regions of the brain were responsible for certain functions. At the time, these findings were seen as a confirmation of Franz Joseph Gall's theory that language was localized and that certain psychological functions were localized in specific areas of the cerebral cortex. The localization of function hypothesis was supported by observations of epileptic patients conducted by John Hughlings Jackson, who correctly inferred the organization of the motor cortex by watching the progression of seizures through the body. Carl Wernicke further developed the theory of the specialization of specific brain structures in language comprehension and production. In 1894, neurologist and psychiatrist Edward Flatau published a human brain atlas “Atlas of the Human Brain and the Course of the Nerve-Fibres” which consisted of long-exposure photographs of fresh brain sections. In 1897, Charles Scott Sherrington introduced the name ""synapse"" for the connection between neurons.",0,Wikipedia,Neuroscience,https://en.wikipedia.org/wiki/Neuroscience,,Neuroscience,wikipedia_api
human_wiki_0268,"In 1909, German anatomist Korbinian Brodmann published his original research on brain mapping, defining 52 distinct regions of the cerebral cortex, known as Brodmann areas. Modern research through neuroimaging techniques, still uses the Brodmann cerebral cytoarchitectonic map (referring to the study of cell structure) anatomical definitions from this era in continuing to show that distinct areas of the cortex are activated in the execution of specific tasks. During the 20th century, neuroscience began to be recognized as a distinct academic discipline in its own right, rather than as studies of the nervous system within other disciplines. Eric Kandel and collaborators have cited David Rioch, Francis O. Schmitt, and Stephen Kuffler as having played critical roles in establishing the field. Rioch originated the integration of basic anatomical and physiological research with clinical psychiatry at the Walter Reed Army Institute of Research, starting in the 1950s. During the same period, Schmitt established a neuroscience research program within the Biology Department at the Massachusetts Institute of Technology, bringing together biology, chemistry, physics, and mathematics. The first freestanding neuroscience department (then called Psychobiology) was founded in 1964 at the University of California, Irvine by James L. McGaugh. This was followed by the Department of Neurobiology at Harvard Medical School, which was founded in 1966 by Stephen Kuffler.",0,Wikipedia,Neuroscience,https://en.wikipedia.org/wiki/Neuroscience,,Neuroscience,wikipedia_api
human_wiki_0269,"In the process of treating epilepsy, Wilder Penfield produced maps of the location of various functions (motor, sensory, memory, vision) in the brain. He summarized his findings in a 1950 book called The Cerebral Cortex of Man. Wilder Penfield and his co-investigators Edwin Boldrey and Theodore Rasmussen are considered to be the originators of the cortical homunculus. The understanding of neurons and of nervous system function became increasingly precise and molecular during the 20th century. For example, in 1952, Alan Lloyd Hodgkin and Andrew Huxley presented a mathematical model for the transmission of electrical signals in neurons of the giant axon of a squid, which they called ""action potentials"", and how they are initiated and propagated, known as the Hodgkin–Huxley model. In 1961–1962, Richard FitzHugh and J. Nagumo simplified Hodgkin–Huxley, in what is called the FitzHugh–Nagumo model. In 1962, Bernard Katz modeled neurotransmission across the space between neurons known as synapses. Beginning in 1966, Eric Kandel and collaborators examined biochemical changes in neurons associated with learning and memory storage in Aplysia. In 1981 Catherine Morris and Harold Lecar combined these models in the Morris–Lecar model. Such increasingly quantitative work gave rise to numerous biological neuron models and models of neural computation. As a result of the increasing interest about the nervous system, several prominent neuroscience organizations have been formed to provide a forum to all neuroscientists during the 20th century. For example, the International Brain Research Organization was founded in 1961, the International Society for Neurochemistry in 1963, the European Brain and Behaviour Society in 1968, and the Society for Neuroscience in 1969. Recently, the application of neuroscience research results has also given rise to applied disciplines as neuroeconomics, neuroeducation, neuroethics, and neurolaw. Over time, brain research has gone through philosophical, experimental, and theoretical phases, with work on neural implants and brain simulation predicted to be important in the future.",0,Wikipedia,Neuroscience,https://en.wikipedia.org/wiki/Neuroscience,,Neuroscience,wikipedia_api
human_wiki_0270,"Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.  Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession. Data science is ""a concept to unify statistics, data analysis, informatics, and their related methods"" to ""understand and analyze actual phenomena"" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge. Data science is often described as a multidisciplinary field because it draws on techniques from diverse areas, such as computer science, statistics, information science, and other subject-specific disciplines. Some researchers say that the combination of the different fields is similar to how information science was decades ago (Mayernik, 2023). These similarities help us understand how data science became its own field of study.  A data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.",0,Wikipedia,Data science,https://en.wikipedia.org/wiki/Data_science,,Data_science,wikipedia_api
human_wiki_0271,"Foundations Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science. Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.",0,Wikipedia,Data science,https://en.wikipedia.org/wiki/Data_science,,Data_science,wikipedia_api
human_wiki_0272,"Etymology Early usage In 1962, John Tukey described a field he called ""data analysis"", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term ""data science"" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing. The term ""data science"" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In his 1974 book Concise Survey of Computer Methods, Peter Naur proposed using the term ‘data science’ rather than ‘computer science’ to reflect the growing emphasis on data-driven methods In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.",0,Wikipedia,Data science,https://en.wikipedia.org/wiki/Data_science,,Data_science,wikipedia_api
human_wiki_0273,"Modern usage In 2012, technologists Thomas H. Davenport and DJ Patil declared ""Data Scientist: The Sexiest Job of the 21st Century"", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that ""the job is more in demand than ever with employers"". The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science. Over the last few years, many colleges have begun to create more structured undergraduate programs in data science. According to a report by the National Academies, strong programs typically include training in statistics, computing, ethics, and communication, as well as hands-on work in a specific field (National Academies of Sciences, Engineering, and Medicine, 2018). As schools try to prepare students for jobs that use data, these practices become more common.  The professional title of ""data scientist"" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report ""Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century"", it referred broadly to any key role in managing a digital data collection.",0,Wikipedia,Data science,https://en.wikipedia.org/wiki/Data_science,,Data_science,wikipedia_api
human_wiki_0274,"Data science and data analysis In data science, data analysis is the process of inspecting, cleaning, transforming, and modelling data to discover useful information, draw conclusions, and support decision-making. It includes exploratory data analysis (EDA), which uses graphics and descriptive statistics to explore patterns and generate hypotheses, and confirmatory data analysis, which applies statistical inference to test hypotheses and quantify uncertainty. Typical activities comprise:",0,Wikipedia,Data science,https://en.wikipedia.org/wiki/Data_science,,Data_science,wikipedia_api
human_wiki_0275,"The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication networks. The IoT encompasses electronics, communication, and computer science engineering. ""Internet of things"" has been considered a misnomer because devices do not need to be connected to the public Internet; they only need to be connected to a network and be individually addressable. The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, and control systems independently and collectively enable the Internet of Things. While in the consumer market, IoT technology is most synonymous with ""smart home"" products—including devices and appliances like thermostats and smart speakers—the technology's largest applications are in the business and industrial sectors. Commercial asset tracking and fleet management represent the largest single application of IoT, accounting for 22% of the total market, driven by the need to monitor mobile assets like vehicles and shipping containers. Other major applications include industrial monitoring, smart metering in utilities, and connected healthcare. However, several concerns exist regarding the risks associated with the growth and diffusion of IoT technologies and products, particularly in the areas of privacy and security. Consequently, several industries, technology companies, and governments (or their branches, ministries, bureaus, departments, etc.) of many countries have taken multiple steps and implemented a variety of precautionary measures to address these concerns adequately and minimize safety risks, including the development and implementation of international and local standards, guidelines, and regulatory frameworks. Due to their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.",0,Wikipedia,Internet of things,https://en.wikipedia.org/wiki/Internet_of_things,,Internet_of_things,wikipedia_api
human_wiki_0276,"Background Around 1972, for its remote site use, the Stanford Artificial Intelligence Laboratory developed a computer-controlled vending machine, adapted from a machine rented from Canteen Vending, which sold for cash or, through a computer terminal (Teletype Model 33 KSR), on credit. Amongst its products were beer, yogurt, and milk. It was named Prancing Pony, after the name of the room, which was named after an inn in J. R. R. Tolkien's epic fantasy novel The Lord of the Rings. A successor version still operates in the Computer Science Department at Stanford, with updated hardware and software.",0,Wikipedia,Internet of things,https://en.wikipedia.org/wiki/Internet_of_things,,Internet_of_things,wikipedia_api
human_wiki_0277,"History In 1982, an early concept of a network connected smart device was constructed as an Internet interface for sensors installed in the Carnegie Mellon University Computer Science Department's departmental Coca-Cola vending machine, supplied by graduate student volunteers, provided a temperature model and an inventory status, inspired by the computer controlled vending machine in the Prancing Pony room at Stanford Artificial Intelligence Laboratory. While it was initially accessible only on the CMU campus, it gained prominence as the first ARPANET-connected appliance.  Mark Weiser's 1991 paper on ubiquitous computing, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom, produced the contemporary vision of the IoT. In 1994, Reza Raji described the concept in IEEE Spectrum as ""[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories."" Between 1993 and 1997, several companies proposed solutions, such as Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as part of his ""Six Webs"" framework, which was presented at the World Economic Forum in Davos in 1999. The concept of the ""Internet of things"" and the term itself first appeared in a speech by Peter T. Lewis to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C., published in September 1985. According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes, and technology with connectable devices and sensors to enable remote monitoring, status, manipulation, and evaluation of trends of such devices."" The term ""Internet of things"" was coined independently by Kevin Ashton of Procter & Gamble, later of Massachusetts Institute of Technology's Auto-ID Center, in 1999, despite preferring the phrase ""Internet for things."" At that point, he considered radio-frequency identification (RFID) an essential component of the Internet of things, as it would effectively enable computers to manage all individual things. The primary defining characteristic of the Internet of things has been considered its ability to embed short-range mobile transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius ""Pete"" Peterson, CEO of NetSilicon, predicted that ""The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations."" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as ""simply the point in time when more 'things or objects' were connected to the Internet than people"", Cisco Systems estimated that the IoT was ""born"" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010.",0,Wikipedia,Internet of things,https://en.wikipedia.org/wiki/Internet_of_things,,Internet_of_things,wikipedia_api
human_wiki_0278,"Consumers A growing portion of IoT devices is created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities.",0,Wikipedia,Internet of things,https://en.wikipedia.org/wiki/Internet_of_things,,Internet_of_things,wikipedia_api
human_wiki_0279,"Home automation IoT devices are part of the broader concept of home automation, which generally includes lighting, heating and air conditioning, media and security systems, and camera systems. Moreover, long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage. A smart home, also known as an automated home, could be based on a platform or hubs that control smart devices and appliances. For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch. This could be a dedicated app or iOS native applications such as Siri. This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge. There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products. These include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub. In addition to the commercial systems, there are many non-proprietary, open source ecosystems, including Home Assistant, OpenHAB, and Domoticz.",0,Wikipedia,Internet of things,https://en.wikipedia.org/wiki/Internet_of_things,,Internet_of_things,wikipedia_api
human_wiki_0280,"Statistics (from German: Statistik, orig. ""description of a state, a country"") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments. When census data (comprising every member of the target population) cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena. A standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a ""false positive"") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a ""false negative""). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.",0,Wikipedia,Statistics,https://en.wikipedia.org/wiki/Statistics,,Statistics,wikipedia_api
human_wiki_0281,"Introduction ""Statistics is both the science of uncertainty and the technology of extracting information from data."" - featured in the International Encyclopedia of Statistical Science.Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population. Statistics is regarded as a body of science or a branch of mathematics. It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics were once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty. Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification. Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification. The word statistics ultimately comes from the Latin word Status, meaning ""situation"" or ""condition"" in society, which in late Latin adopted the meaning ""state"". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works. In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.",0,Wikipedia,Statistics,https://en.wikipedia.org/wiki/Statistics,,Statistics,wikipedia_api
human_wiki_0282,"Statistical data Data collection Sampling When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population. Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.",0,Wikipedia,Statistics,https://en.wikipedia.org/wiki/Statistics,,Statistics,wikipedia_api
human_wiki_0283,"Experimental and observational studies A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies—for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.",0,Wikipedia,Statistics,https://en.wikipedia.org/wiki/Statistics,,Statistics,wikipedia_api
human_wiki_0284,"Planning the research, including finding the number of replicates of the study, using the following information:  preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects. Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data. Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol. Further examining the data set in secondary analyses, to suggest new hypotheses for future study. Documenting and presenting the results of the study. Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.",0,Wikipedia,Statistics,https://en.wikipedia.org/wiki/Statistics,,Statistics,wikipedia_api
human_wiki_0285,"The Internet (or internet) is the global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices. It is a network of networks that comprises private, public, academic, business, and government networks of local to global scope, linked by electronic, wireless, and optical networking technologies. The Internet carries a vast range of information services and resources, such as the interlinked hypertext documents and applications of the World Wide Web (WWW), electronic mail, internet telephony, streaming media and file sharing. Most traditional communication media, including telephone, radio, television, paper mail, newspapers, and print publishing, have been transformed by the Internet, giving rise to new media such as email, online music, digital newspapers, news aggregators, and audio and video streaming websites. The Internet has enabled and accelerated new forms of personal interaction through instant messaging, Internet forums, and social networking services. Online shopping has also grown to occupy a significant market across industries, enabling firms to extend brick and mortar presences to serve larger markets. Business-to-business and financial services on the Internet affect supply chains across entire industries.  The Internet has no single centralized governance in either technological implementation or policies for access and usage. Each constituent network sets its own policies. The overarching definitions of the two principal name spaces on the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the non-profit Internet Engineering Task Force (IETF).",0,Wikipedia,Internet,https://en.wikipedia.org/wiki/Internet,,Internet,wikipedia_api
human_wiki_0286,"Terminology The word internetted was used as early as 1849, meaning interconnected or interwoven. The word Internet was used in 1945 by the United States War Department in a radio operator's manual, and in 1974 as the shorthand form of Internetwork. Today, the term Internet most commonly refers to the global system of interconnected computer networks, though it may also refer to any group of smaller networks. The word Internet may be capitalized as a proper noun, although this is becoming less common. This reflects the tendency in English to capitalize new terms and move them to lowercase as they become familiar. The word is sometimes still capitalized to distinguish the global internet from smaller networks, though many publications, including the AP Stylebook since 2016, recommend the lowercase form in every case. In 2016, the Oxford English Dictionary found that, based on a study of around 2.5 billion printed and online sources, ""Internet"" was capitalized in 54% of cases. The terms Internet and World Wide Web are often used interchangeably; it is common to speak of ""going on the Internet"" when using a web browser to view web pages. However, the World Wide Web, or the Web, is only one of a large number of Internet services. It is the global collection of web pages, documents and other web resources linked by hyperlinks and URLs.",0,Wikipedia,Internet,https://en.wikipedia.org/wiki/Internet,,Internet,wikipedia_api
human_wiki_0287,"History 1960s The origins of the Internet date back to research that enabled the time-sharing of computer resources, the development of packet switching, and the design of computer networks for data communication.  In the 1960s, computer scientists began developing systems for time-sharing of computer resources. J. C. R. Licklider proposed the idea of a universal network while working at Bolt Beranek & Newman and, later, leading the Information Processing Techniques Office at the Advanced Research Projects Agency (ARPA) of the United States Department of Defense. Research into packet switching, one of the fundamental Internet technologies, started in the work of Paul Baran at RAND in the early 1960s and, independently, Donald Davies at the United Kingdom's National Physical Laboratory in 1965.  After the Symposium on Operating Systems Principles in 1967, packet switching from the proposed NPL network and routing concepts proposed by Baran were incorporated into the design of the ARPANET, an experimental resource sharing network proposed by ARPA.",0,Wikipedia,Internet,https://en.wikipedia.org/wiki/Internet,,Internet,wikipedia_api
human_wiki_0288,"1970s The set of communication protocols to enable internetworking on the Internet arose from research and development commissioned in the 1970s by the Defense Advanced Research Projects Agency (DARPA) of the United States Department of Defense in collaboration with universities and researchers across the United States and in the United Kingdom and France.  ARPANET development began with two network nodes which were interconnected between the University of California, Los Angeles and the Stanford Research Institute on 29 October 1969. The third site was at the University of California, Santa Barbara, followed by the University of Utah. By the end of 1971, 15 sites were connected to the young ARPANET. Thereafter, the ARPANET gradually developed into a decentralized communications network, connecting remote centers and military bases in the United States. Other user networks and research networks, such as the Merit Network and CYCLADES, were developed in the late 1960s and early 1970s. Early international collaborations for the ARPANET were rare. Connections were made in 1973 to Norway (NORSAR and NDRE) and to Peter Kirstein's research group at University College London, which provided a gateway to British academic networks, the first internetwork for resource sharing.  ARPA projects, the International Network Working Group and commercial initiatives led to the development of various protocols and standards by which multiple separate networks could become a single network, or a network of networks. In 1974, Vint Cerf at Stanford University and Bob Kahn at DARPA published a proposal for ""A Protocol for Packet Network Intercommunication"". Cerf and his students used the term internet as a shorthand for internetwork in RFC 675. The Internet Experiment Notes and later RFCs repeated this use. The work of Louis Pouzin and others had important influences on the resulting TCP/IP design. National PTTs and commercial providers developed the X.25 standard and deployed it on public data networks.",0,Wikipedia,Internet,https://en.wikipedia.org/wiki/Internet,,Internet,wikipedia_api
human_wiki_0289,"1980s The ARPANET initially served as a backbone for the interconnection of regional academic and military networks in the United States to enable resource sharing. Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET).  In 1982, the Internet Protocol Suite (TCP/IP) was standardized, which facilitated worldwide proliferation of interconnected networks. TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNet) provided access to supercomputer sites in the United States for researchers, first at speeds of 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s.  The NSFNet expanded into academic and research organizations in Europe, Australia, New Zealand and Japan in 1988–89. Although other network protocols such as UUCP and PTT public data networks had global reach well before this time, this marked the beginning of the Internet as an intercontinental network. Commercial Internet service providers emerged in 1989 in the United States and Australia. The ARPANET was decommissioned in 1990.",0,Wikipedia,Internet,https://en.wikipedia.org/wiki/Internet,,Internet,wikipedia_api
human_wiki_0290,"The Big Bang is a physical theory that describes how the universe expanded from an initial state of high density and temperature. Various cosmological models based on the Big Bang concept explain a broad range of phenomena,  including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure. The uniformity of the universe, known as the horizon and flatness problems, is explained through cosmic inflation: a phase of accelerated expansion during the earliest stages. Detailed measurements of the expansion rate of the universe place the initial singularity at an estimated 13.787±0.02 billion years ago, which is considered the age of the universe. A wide range of empirical evidence strongly favors the Big Bang event, which is now widely accepted. Extrapolating this cosmic expansion backward in time using the known laws of physics, the models describe an extraordinarily hot and dense primordial universe. Physics lacks a widely accepted theory that can model the earliest conditions of the Big Bang. As the universe expanded, it cooled sufficiently to allow the formation of subatomic particles, and later atoms. These primordial elements—mostly hydrogen, with some helium and lithium—then coalesced under the force of gravity aided by dark matter, forming early stars and galaxies.  Measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to a concept called dark energy. The concept of an expanding universe was introduced by the physicist Alexander Friedmann in 1922 with the mathematical derivation of the Friedmann equations. The earliest empirical observation of an expanding universe is known as Hubble's law, published in work by physicist Edwin Hubble in 1929, which discerned that galaxies are moving away from Earth at a rate that accelerates proportionally with distance. Independent of Friedmann's work, and independent of Hubble's observations, in 1931 physicist Georges Lemaître proposed that the universe emerged from a ""primeval atom"", introducing the modern notion of the Big Bang. In 1964, the CMB was discovered. Over the next few years measurements showed this radiation to be uniform over directions in the sky and the shape of the energy versus intensity curve, both consistent with the Big Bang models of high temperatures and densities in the distant past.  By the late 1960s most cosmologists were convinced that competing steady-state model of cosmic evolution was incorrect. There remain aspects of the observed universe that are not yet adequately explained by the Big Bang models. These include the unequal abundances of matter and antimatter known as baryon asymmetry, the detailed nature of dark matter surrounding galaxies, and the origin of dark energy.",0,Wikipedia,Big Bang,https://en.wikipedia.org/wiki/Big_Bang,,Big_Bang,wikipedia_api
human_wiki_0291,"Features of the models Assumptions Big Bang cosmology models depend on three major assumptions: the universality of physical laws, the cosmological principle, and that the matter content can be modeled as a perfect fluid.  The universality of physical laws is one of the underlying principles of the theory of relativity. The cosmological principle states that on large scales the universe is homogeneous and isotropic—appearing the same in all directions regardless of location. A perfect fluid has no viscosity; the pressure of a perfect fluid is proportional to its density. These ideas were initially taken as postulates, but later efforts were made to test each of them. For example, the first assumption has been tested by observations showing that the largest possible deviation of the fine-structure constant over much of the age of the universe is of order 10−5. The key physical law behind these models, general relativity has passed stringent tests on the scale of the Solar System and binary stars. The cosmological principle has been confirmed to a level of 10−5 via observations of the temperature of the CMB. At the scale of the CMB horizon, the universe has been measured to be homogeneous with an upper bound on the order of 10% inhomogeneity, as of 1995.",0,Wikipedia,Big Bang,https://en.wikipedia.org/wiki/Big_Bang,,Big_Bang,wikipedia_api
human_wiki_0292,"Expansion prediction The cosmological principle dramatically simplifies the equations of general relativity, giving the Friedmann–Lemaître–Robertson–Walker metric to describe the geometry of the universe and, with the assumption of a perfect fluid, the Friedmann equations giving the time dependence of that geometry. The only parameter at this level of description is the mass-energy density: the geometry of the universe and its expansion is a direct consequence of its density. All of the major features of Big Bang cosmology are related to these results.",0,Wikipedia,Big Bang,https://en.wikipedia.org/wiki/Big_Bang,,Big_Bang,wikipedia_api
human_wiki_0293,"Mass-energy density In Big Bang cosmology, the mass–energy density controls the shape and evolution of the universe. By combining astronomical observations with known laws of thermodynamics and particle physics, cosmologists have worked out the components of the density over the lifespan of the universe. In the current universe, luminous matter, the stars, planets, and so on makes up less than 5% of the density. Dark matter accounts for 27%  and dark energy the remaining 68%.",0,Wikipedia,Big Bang,https://en.wikipedia.org/wiki/Big_Bang,,Big_Bang,wikipedia_api
human_wiki_0294,"Horizons An important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not yet had time to reach earth. This places a limit or a past horizon on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never ""catch up"" to very distant objects. This defines a future horizon, which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the Friedmann–Lemaître–Robertson–Walker (FLRW) metric that describes the expansion of the universe. Our understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.",0,Wikipedia,Big Bang,https://en.wikipedia.org/wiki/Big_Bang,,Big_Bang,wikipedia_api
human_wiki_0295,"Public health is ""the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals"". Analyzing the determinants of health of a population and the threats it faces is the basis for public health. The public can be as small as a handful of people or as large as a village or an entire city; in the case of a pandemic it may encompass several continents. The concept of health takes into account physical, psychological, and social well-being, among other factors. Public health is an interdisciplinary field. For example, epidemiology, biostatistics, social sciences and management of health services are all relevant. Other important sub-fields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, health politics, occupational safety, disability, oral health, gender issues in health, and sexual and reproductive health. Public health, together with primary care, secondary care, and tertiary care, is part of a country's overall healthcare system. Public health is implemented through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promotion of hand-washing and breastfeeding, delivery of vaccinations, promoting ventilation and improved air quality both indoors and outdoors, suicide prevention, smoking cessation, obesity education, increasing healthcare accessibility and distribution of condoms to control the spread of sexually transmitted diseases. There is a significant disparity in access to health care and public health initiatives between developed countries and developing countries, as well as within developing countries. In developing countries, public health infrastructures are still forming. There may not be enough trained healthcare workers, monetary resources, or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention. A major public health concern in developing countries is poor maternal and child health, exacerbated by malnutrition and poverty and limited implementation of comprehensive public health policies. Developed nations are at greater risk of certain public health crises, including childhood obesity, although overweight populations in low- and middle-income countries are catching up. From the beginnings of human civilization, communities promoted health and fought disease at the population level. In complex, pre-industrialized societies, interventions designed to reduce health risks could be the initiative of different stakeholders, such as army generals, the clergy or rulers. Great Britain became a leader in the development of public health initiatives, beginning in the 19th century, due to the fact that it was the first modern urban nation worldwide. The public health initiatives that began to emerge initially focused on sanitation (for example, the Liverpool and London sewerage systems), control of infectious diseases (including vaccination and quarantine) and an evolving infrastructure of various sciences, e.g. statistics, microbiology, epidemiology, sciences of engineering.",0,Wikipedia,Public health,https://en.wikipedia.org/wiki/Public_health,,Public_health,wikipedia_api
human_wiki_0296,"Definition Public health has been defined as ""the science and art of preventing disease"", prolonging life and improving quality of life through organized efforts and informed choices of society, organizations (public and private), communities and individuals.  The public can be as small as a handful of people or as large as a village or an entire city. The concept of health takes into account physical, psychological, and social well-being. As such, according to the World Health Organization, ""health is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity"".",0,Wikipedia,Public health,https://en.wikipedia.org/wiki/Public_health,,Public_health,wikipedia_api
human_wiki_0297,"Related terms Public health is related to global health which is the health of populations in the worldwide context. It has been defined as ""the area of study, research and practice that places a priority on improving health and achieving equity in ""Health for all"" people worldwide"". International health is a field of health care, usually with a public health emphasis, dealing with health across regional or national boundaries. Public health is not the same as public healthcare (publicly funded health care). The term preventive medicine is related to public health. The American Board of Preventive Medicine separates three categories of preventive medicine: aerospace health, occupational health, and public health and general preventative medicine. Jung, Boris and Lushniak argue that preventive medicine should be considered the medical specialty for public health but note that the American College of Preventive Medicine and American Board of Preventive Medicine do not prominently use the term ""public health"". Preventive medicine specialists are trained as clinicians and address complex health needs of a population such as by assessing the need for disease prevention programs, using the best methods to implement them, and assessing their effectiveness. Since the 1990s many scholars in public health have been using the term population health. There are no medical specialties directly related to population health. Valles argues that consideration of health equity is a fundamental part of population health. Scholars such as Coggon and Pielke express concerns about bringing general issues of wealth distribution into population health. Pielke worries about ""stealth issue advocacy"" in population health. Jung, Boris and Lushniak consider population health to be a concept that is the goal of an activity called public health practiced through the specialty preventive medicine. Lifestyle medicine uses individual lifestyle modification to prevent or revert disease and can be considered a component of preventive medicine and public health. It is implemented as part of primary care rather than a specialty in its own right. Valles argues that the term social medicine has a narrower and more biomedical focus than the term population health.",0,Wikipedia,Public health,https://en.wikipedia.org/wiki/Public_health,,Public_health,wikipedia_api
human_wiki_0298,"Purpose The purpose of a public health intervention is to prevent and mitigate diseases, injuries, and other health conditions. The overall goal is to improve the health of individuals and populations, and to increase life expectancy.",0,Wikipedia,Public health,https://en.wikipedia.org/wiki/Public_health,,Public_health,wikipedia_api
human_wiki_0299,"Components Public health is a complex term, composed of many elements and different practices. It is a multi-faceted, interdisciplinary field. For example, epidemiology, biostatistics, social sciences and management of health services are all relevant. Other important sub-fields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, health politics, occupational safety, disability, gender issues in health, and sexual and reproductive health. Modern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, physician assistants, public health nurses, midwives, medical microbiologists, pharmacists, economists, sociologists, geneticists, data managers, environmental health officers (public health inspectors), bioethicists, gender experts, sexual and reproductive health specialists, physicians, and veterinarians. The elements and priorities of public health have evolved over time, and are continuing to evolve. Common public health initiatives include promotion of hand-washing and breastfeeding, delivery of vaccinations, suicide prevention, smoking cessation, obesity education, increasing healthcare accessibility and distribution of condoms to control the spread of sexually transmitted diseases.",0,Wikipedia,Public health,https://en.wikipedia.org/wiki/Public_health,,Public_health,wikipedia_api
human_wiki_0300,"Agriculture is the practice of cultivating the soil, planting, raising, and harvesting both food and non-food crops, as well as livestock production. Broader definitions also include forestry and aquaculture. Agriculture was a key factor in the rise of sedentary human civilization, whereby farming of domesticated plants and animals created food surpluses that enabled people to live in the cities. While humans started gathering grains at least 105,000 years ago, nascent farmers only began planting them around 11,500 years ago. Sheep, goats, pigs, and cattle were domesticated around 10,000 years ago. Plants were independently cultivated in at least 11 regions of the world. In the 20th century, industrial agriculture based on large-scale monocultures came to dominate agricultural output. As of 2021, small farms, of which the vast majority are one hectare (about 2.5 acres) or smaller, produce about one-third of the world's food. Moreover, five of every six farms in the world consist of fewer than 2 hectares (4.9 acres) and take up only around 12% of all agricultural land. In terms of total land use, large farms are dominant. While only 1% of all farms globally are greater than 50 hectares (120 acres), they encompass more than 70% of the world's farmland. Further, nearly 40% of all global agricultural land is found on farms larger than 1,000 hectares (2,500 acres).   Farms and farming greatly influence rural economics and greatly shape rural society, affecting both the direct agricultural workforce and broader businesses that support the farms and farming populations. The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber and timber). Food classes include cereals (grains), vegetables, fruits, cooking oils, meat, milk, eggs, and fungi. Global agricultural production amounts to approximately 11 billion tonnes of food, 32 million tonnes of natural fibers and 4 billion m3 of wood. However, around 14% of the world's food is lost from production before reaching the retail level. Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased crop yields, but also contributed to ecological and environmental damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage. Environmental issues include contributions to climate change, depletion of aquifers, deforestation, antibiotic resistance, and other agricultural pollution. Agriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation, and climate change, all of which can cause decreases in crop yield. Genetically modified organisms are widely used, although some countries ban them.",0,Wikipedia,Agriculture,https://en.wikipedia.org/wiki/Agriculture,,Agriculture,wikipedia_api
human_wiki_0301,"Etymology and scope The word agriculture is a late Middle English adaptation of Latin agricultūra, from ager 'field' and cultūra 'cultivation' or 'growing'. While agriculture usually refers to human activities, certain species of ant, termite and beetle have been cultivating crops for up to 60 million years. Agriculture is defined with varying scopes, in its broadest sense using natural resources to ""produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services"". Thus defined, it includes arable farming, horticulture, animal husbandry and forestry, but horticulture and forestry are in practice often excluded. It may also be broadly decomposed into plant agriculture, which concerns the cultivation of useful plants, and animal agriculture, the production of agricultural animals.",0,Wikipedia,Agriculture,https://en.wikipedia.org/wiki/Agriculture,,Agriculture,wikipedia_api
human_wiki_0302,"History Origins The development of agriculture enabled the human population to grow many times larger than could be sustained by hunting and gathering. Agriculture began independently in different parts of the globe, and included a diverse range of taxa, in at least 11 separate centers of origin. Wild grains were collected and eaten from at least 105,000 years ago. In the Paleolithic Levant, 23,000 years ago, cereals cultivation of emmer, barley, and oats has been observed near the sea of Galilee. Rice was domesticated in China between 11,500 and 6,200 BC with the earliest known cultivation from 5,700 BC, followed by mung, soy and azuki beans. Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago. Pig production emerged in Eurasia, including Europe, East Asia and Southwest Asia, where wild boar were first domesticated about 10,500 years ago. In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs. Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago. Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago. Cotton was domesticated in Peru by 5,600 years ago, and was independently domesticated in Eurasia. In Mesoamerica, wild teosinte was bred into maize (corn) from 10,000 to 6,000 years ago. The horse was domesticated in the Eurasian Steppes around 3500 BC. Scholars have offered multiple hypotheses to explain the historical origins of agriculture. Studies of the transition from hunter-gatherer to agricultural societies indicate an initial period of intensification and increasing sedentism; examples are the Natufian culture in the Levant, and the Early Chinese Neolithic in China. Then, wild stands that had previously been harvested started to be planted, and gradually came to be domesticated.",0,Wikipedia,Agriculture,https://en.wikipedia.org/wiki/Agriculture,,Agriculture,wikipedia_api
human_wiki_0303,"Civilizations In Eurasia, the Sumerians started to live in villages from about 8,000 BC, relying on the Tigris and Euphrates rivers and a canal system for irrigation. Ploughs appear in pictographs around 3,000 BC; seed-ploughs around 2,300 BC. Farmers grew wheat, barley, vegetables such as lentils and onions, and fruits including dates, grapes, and figs. Ancient Egyptian agriculture relied on the Nile River and its seasonal flooding. Farming started in the predynastic period at the end of the Paleolithic, after 10,000 BC. Staple food crops were grains such as wheat and barley, alongside industrial crops such as flax and papyrus. In India, wheat, barley and jujube were domesticated by 9,000 BC, soon followed by sheep and goats. Cattle, sheep and goats were domesticated in Mehrgarh culture by 8,000–6,000 BC. Cotton was cultivated by the 5th–4th millennium BC. Archeological evidence indicates an animal-drawn plough from 2,500 BC in the Indus Valley civilization.",0,Wikipedia,Agriculture,https://en.wikipedia.org/wiki/Agriculture,,Agriculture,wikipedia_api
human_wiki_0304,"In China, from the 5th century BC, there was a nationwide granary system and widespread silk farming. Water-powered grain mills were in use by the 1st century BC, followed by irrigation. By the late 2nd century, heavy ploughs had been developed with iron ploughshares and mouldboards. These spread westwards across Eurasia. Asian rice was domesticated 8,200–13,500 years ago – depending on the molecular clock estimate that is used– on the Pearl River in southern China with a single genetic origin from the wild rice Oryza rufipogon. In Greece and Rome, the major cereals were wheat, emmer, and barley, alongside vegetables including peas, beans, and olives. Sheep and goats were kept mainly for dairy products. In the Americas, crops domesticated in Mesoamerica (apart from teosinte) include squash, beans, and cacao. Cocoa was domesticated by the Mayo Chinchipe of the upper Amazon around 3,000 BC. The turkey was probably domesticated in Mexico or the American Southwest. The Aztecs developed irrigation systems, formed terraced hillsides, fertilized their soil, and developed chinampas or artificial islands. The Mayas used extensive canal and raised field systems to farm swampland from 400 BC. In South America agriculture may have begun about 9000 BC with the domestication of squash (Cucurbita) and other plants. Coca was domesticated in the Andes, as were the peanut, tomato, tobacco, and pineapple. Cotton was domesticated in Peru by 3,600 BC. Animals including llamas, alpacas, and guinea pigs were domesticated there. In North America, the indigenous people of the East domesticated crops such as sunflower, tobacco, squash and Chenopodium. Wild foods including wild rice and maple sugar were harvested. The domesticated strawberry is a hybrid of a Chilean and a North American species, developed by breeding in Europe and North America. The indigenous people of the Southwest and the Pacific Northwest practiced forest gardening and fire-stick farming. The natives controlled fire on a regional scale to create a low-intensity fire ecology that sustained a low-density agriculture in loose rotation; a sort of ""wild"" permaculture. A system of companion planting called the Three Sisters was developed in North America. The three crops were winter squash, maize, and climbing beans. Indigenous Australians, long supposed to have been nomadic hunter-gatherers, practiced systematic burning, possibly to enhance natural productivity in fire-stick farming. Scholars have pointed out that hunter-gatherers need a productive environment to support gathering without cultivation. Because the forests of New Guinea have few food plants, early humans may have used ""selective burning"" to increase the productivity of the wild karuka fruit trees to support the hunter-gatherer way of life. The Gunditjmara and other groups developed eel farming and fish trapping systems from some 5,000 years ago. There is evidence of 'intensification' across the whole continent over that period. In two regions of Australia, the central west coast and eastern central, early farmers cultivated yams, native millet, and bush onions, possibly in permanent settlements.",0,Wikipedia,Agriculture,https://en.wikipedia.org/wiki/Agriculture,,Agriculture,wikipedia_api
human_wiki_0305,"History is the systematic study of the past, focusing primarily on the human past. As an academic discipline, it analyses and interprets evidence to construct narratives about what happened and explain why it happened. Some theorists categorize history as a social science, while others see it as part of the humanities or consider it a hybrid discipline. Similar debates surround the purpose of history—for example, whether its main aim is theoretical, to uncover the truth, or practical, to learn lessons from the past. In a more general sense, the term history refers not to an academic field but to the past itself, times in the past, or to individual texts about the past. Historical research relies on primary and secondary sources to reconstruct past events and validate interpretations. Source criticism is used to evaluate these sources, assessing their authenticity, content, and reliability. Historians strive to integrate the perspectives of several sources to develop a coherent narrative. Different schools of thought, such as positivism, the Annales school, Marxism, and postmodernism, have distinct methodological approaches. History is a broad discipline encompassing many branches. Some focus on specific time periods, such as ancient history, while others concentrate on particular geographic regions, such as the history of Africa. Thematic categorizations include political history, military history, social history, and economic history. Branches associated with specific research methods and sources include quantitative history, comparative history, and oral history. History emerged as a field of inquiry in antiquity to replace myth-infused narratives, with influential early traditions originating in Greece, China, and later in the Islamic world. Historical writing evolved throughout the ages and became increasingly professional, particularly during the 19th century, when a rigorous methodology and various academic institutions were established. History is related to many fields, including historiography, philosophy, education, and politics.",0,Wikipedia,History,https://en.wikipedia.org/wiki/History,,History,wikipedia_api
human_wiki_0306,"Definition As an academic discipline, history is the study of the past with the main focus on the human past. It conceptualizes and describes what happened by collecting and analysing evidence to construct narratives. These narratives cover not only how events developed over time but also why they happened and in which contexts, providing an explanation of relevant background conditions and causal mechanisms. History further examines the meaning of historical events and the underlying human motives driving them. In a slightly different sense, history refers to the past events themselves. Under this interpretation, history is what happened rather than the academic field studying what happened. When used as a countable noun, a history is a representation of the past in the form of a history text. History texts are cultural products involving active interpretation and reconstruction. The narratives presented in them can change as historians discover new evidence or reinterpret already-known sources. The past itself, by contrast, is static and unchangeable. Some historians focus on the interpretative and explanatory aspects to distinguish histories from chronicles, arguing that chronicles only catalogue events in chronological order, whereas histories aim at a comprehensive understanding of their causes, contexts, and consequences. History has been primarily concerned with written documents. It focused on recorded history since the invention of writing, leaving prehistory to other fields, such as archaeology. Its scope broadened in the 20th century as historians became interested in the human past before the invention of writing. Historians debate whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather objective evidence, and present arguments based on this evidence. At the same time, history aligns closely with the humanities because of its reliance on subjective aspects associated with interpretation, storytelling, human experience, and cultural heritage. Some historians strongly support one or the other classification while others characterize history as a hybrid discipline that does not belong to one category at the exclusion of the other. History contrasts with pseudohistory, a label used to describe practices that deviate from historiographical standards by relying on disputed historical evidence, selectively ignoring genuine evidence, or using other means to distort the historical record. Often motivated by specific ideological agendas, pseudohistorical practices mimic historical methodology to promote biased, misleading narratives that lack rigorous analysis and scholarly consensus.",0,Wikipedia,History,https://en.wikipedia.org/wiki/History,,History,wikipedia_api
human_wiki_0307,"Purpose Various suggestions about the purpose or value of history have been made. Some historians propose that its primary function is the pure discovery of truth about the past. This view emphasizes that the disinterested pursuit of truth is an end in itself, while external purposes, associated with ideology or politics, threaten to undermine the accuracy of historical research by distorting the past. In this role, history also challenges traditional myths lacking factual support. A different perspective suggests that the main value of history lies in the lessons it teaches for the present. This view is based on the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes. A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across different contexts—similar to what one can learn by visiting foreign countries. History can also foster social cohesion by providing people with a collective identity through a shared past, helping to preserve and cultivate cultural heritage and values across generations. For some scholars, including Whig historians and the Marxist scholar E. H. Carr, history is a key to understanding the present and, in Carr's case, shaping the future. History has sometimes been used for political or ideological purposes, for instance, to justify the status quo by emphasising the respectability of certain traditions or to promote change by highlighting past injustices.  In extreme forms, evidence is intentionally ignored or misinterpreted to construct misleading narratives, which can result in pseudohistory or historical denialism. Influential examples are Holocaust denial, Armenian genocide denial, Nanjing Massacre denial, and Holodomor denial.",0,Wikipedia,History,https://en.wikipedia.org/wiki/History,,History,wikipedia_api
human_wiki_0308,"Etymology The word history comes from the Ancient Greek term ἵστωρ (histōr), meaning 'learned, wise man'. It gave rise to the Ancient Greek word ἱστορία (historiā), which had a wide meaning associated with inquiry in general and giving testimony. The term was later adopted into Classical Latin as historia. In Hellenistic and Roman times, the meaning of the term shifted, placing more emphasis on narrative aspects and the art of presentation rather than focusing on investigation and testimony. The word entered Middle English in the 14th century via the Old French term histoire. At this time, it meant 'story, tale', encompassing both factual and fictional narratives. In the 15th century, its meaning shifted to cover the branch of knowledge studying the past in addition to narratives about the past. In the 18th and 19th centuries, the word history became more closely associated with factual accounts and evidence-based inquiry, coinciding with the professionalization of historical inquiry, a meaning still dominant in contemporary usage. The dual meaning, referring to both mere stories and factual accounts of the past, is present in the terms for history in many other European languages. They include the French histoire, the Italian storia, and the German Geschichte.",0,Wikipedia,History,https://en.wikipedia.org/wiki/History,,History,wikipedia_api
human_wiki_0309,"Methods The historical method is a set of techniques historians use to research and interpret the past, covering the processes of collecting, evaluating, and synthesizing evidence. It seeks to ensure scholarly rigour, accuracy, and reliability in how historical evidence is chosen, analysed, and interpreted. Historical research often starts with a research question to define the scope of the inquiry. Some research questions focus on a simple description of what happened. Others aim to explain why a particular event occurred, refute an existing theory, or confirm a new hypothesis.",0,Wikipedia,History,https://en.wikipedia.org/wiki/History,,History,wikipedia_api
human_wiki_0310,"Physics is the scientific study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. It is one of the most fundamental scientific disciplines. A scientist who specializes in the field of physics is called a physicist. Physics is one of the oldest academic disciplines. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century, these natural sciences branched into separate research endeavors. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy. Advances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of technologies that have transformed modern society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.",0,Wikipedia,Physics,https://en.wikipedia.org/wiki/Physics,,Physics,wikipedia_api
human_wiki_0311,"History The word physics comes from the Latin physica ('study of nature'), which itself is a borrowing of the Greek φυσική (phusikḗ 'natural science'), a term derived from φύσις (phúsis 'origin, nature, property').",0,Wikipedia,Physics,https://en.wikipedia.org/wiki/Physics,,Physics,wikipedia_api
human_wiki_0312,"Ancient astronomy Astronomy is one of the oldest natural sciences. Early civilizations dating before 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which could not explain the positions of the planets. According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the Northern Hemisphere.",0,Wikipedia,Physics,https://en.wikipedia.org/wiki/Physics,,Physics,wikipedia_api
human_wiki_0313,"Natural philosophy Natural philosophy has its origins in Greece during the Archaic period (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus.",0,Wikipedia,Physics,https://en.wikipedia.org/wiki/Physics,,Physics,wikipedia_api
human_wiki_0314,"Aristotle and Hellenistic physics During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy developed along many lines of inquiry. Aristotle (Greek: Ἀριστοτέλης, Aristotélēs) (384–322 BCE), a student of Plato, wrote on many subjects, including a substantial treatise on ""Physics"" – in the 4th century BC.  Aristotelian physics was influential for about two millennia. His approach mixed some limited observation with logical deductive arguments, but did not rely on experimental verification of deduced statements.  Aristotle's foundational work in Physics, though very imperfect, formed a framework against which later thinkers further developed the field. His approach is entirely superseded today. He explained ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that each of the four classical elements (air, fire, water, earth) had its own natural place.  Because of their differing densities, each element will revert to its own specific place in the atmosphere.  So, because of their weights, fire would be at the top, air underneath fire, then water, then lastly earth. He also stated that when a small amount of one element enters the natural place of another, the less abundant element will automatically go towards its own natural place.  For example, if there is a fire on the ground, the flames go up into the air in an attempt to go back into its natural place where it belongs.  His laws of motion included: that heavier objects will fall faster, the speed being proportional to the weight and the speed of the object that is falling depends inversely on the density object it is falling through (e.g. density of air). He also stated that, when it comes to violent motion (motion of an object when a force is applied to it by a second object) that the speed that object moves, will only be as fast or strong as the measure of force applied to it.   The problem of motion and its causes was studied carefully, leading to the philosophical notion of a ""prime mover"" as the ultimate source of all motion in the world (Book 8 of his treatise Physics).",0,Wikipedia,Physics,https://en.wikipedia.org/wiki/Physics,,Physics,wikipedia_api
human_wiki_0315,"Medicine is the science and practice of caring for patients, managing the diagnosis, prognosis, prevention, treatment, palliation of their injury or disease, and promoting their health. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others. Medicine has been practiced since prehistoric times, and for most of this time it was an art (an area of creativity and skill), frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). For example, while stitching technique for sutures is an art learned through practice, knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science. Prescientific forms of medicine, now known as traditional medicine or folk medicine, remain commonly used in the absence of scientific medicine and are thus called alternative medicine. Alternative treatments outside of scientific medicine with ethical, safety and efficacy concerns are termed quackery.",0,Wikipedia,Medicine,https://en.wikipedia.org/wiki/Medicine,,Medicine,wikipedia_api
human_wiki_0316,"Etymology Medicine (UK:  , US:  ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease. The word ""medicine"" is derived from Latin medicus, meaning ""a physician"". The word ""physic"" itself, from which ""physician"" derives, was the old word for what is now called a medicine, and also the field of medicine.",0,Wikipedia,Medicine,https://en.wikipedia.org/wiki/Medicine,,Medicine,wikipedia_api
human_wiki_0317,"Clinical practice Medical availability and clinical practice vary across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed and widespread in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine which has limited evidence and efficacy and no required formal training for practitioners. In the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm. In modern clinical practice, physicians and mid-level practitioners such as physician assistants personally assess patients to diagnose, prognose, treat, and prevent disease using clinical judgment. An initial medical encounter with a patient typically begins with a review of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g., stethoscope, tongue depressor) are typically used. After examining for signs and interviewing for symptoms, the doctor may order medical tests (e.g., blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust within the context of the doctor-patient relationship. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-up encounters may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks, depending on the complexity of the issue. The components of the medical interview and encounter are:",0,Wikipedia,Medicine,https://en.wikipedia.org/wiki/Medicine,,Medicine,wikipedia_api
human_wiki_0318,"Chief complaint (CC): the reason for the current medical visit. These are the symptoms. They are in the patient's own words and are recorded along with the duration of each one. Also called chief concern or presenting complaint. Current activity: occupation, hobbies, what the patient actually does. Family history (FH): listing of diseases in the family that may impact the patient. A family tree is sometimes used. History of present illness (HPI): the chronological order of events of symptoms and further clarification of each symptom. Distinguishable from history of previous illness, often called past medical history (PMH). Medical history comprises HPI and PMH. Medications (Rx): what drugs the patient takes including prescribed, over-the-counter, and home remedies, as well as alternative and herbal medicines or remedies. Allergies are also recorded. Past medical history (PMH/PMHx): concurrent medical problems, past hospitalizations and operations, injuries, past infectious diseases or vaccinations, history of known allergies. Review of systems (ROS) or systems inquiry: a set of additional questions to ask, which may be missed on HPI: a general enquiry (have you noticed any weight loss, change in sleep quality, fevers, lumps and bumps? etc.), followed by questions on the body's main organ systems (heart, lungs, digestive tract, urinary tract, etc.). Social history (SH): birthplace, residences, marital history, social and economic status, habits (including diet, medications, tobacco, alcohol). The physical examination is the examination of the patient for medical signs of disease that are objective and observable, in contrast to symptoms that are volunteered by the patient and are not necessarily objectively observable. The healthcare provider uses sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order, although auscultation occurs prior to percussion and palpation for abdominal assessments. The clinical examination involves the study of:",0,Wikipedia,Medicine,https://en.wikipedia.org/wiki/Medicine,,Medicine,wikipedia_api
human_wiki_0319,"Abdomen and rectum Cardiovascular (heart and blood vessels) General appearance of the patient and specific indicators of disease (nutritional status, presence of jaundice, pallor or clubbing) Genitalia (and pregnancy if the patient is or could be pregnant) Head, eye, ear, nose, and throat (HEENT) Musculoskeletal (including spine and extremities) Neurological (consciousness, awareness, brain, vision, cranial nerves, spinal cord and peripheral nerves) Psychiatric (orientation, mental state, mood, evidence of abnormal perception or thought). Respiratory (large airways and lungs) Skin Vital signs including height, weight, body temperature, blood pressure, pulse, respiration rate, and hemoglobin oxygen saturation It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above. The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. A follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of ""utilization review"", such as prior authorization of tests, may place barriers on accessing expensive services. The medical decision-making (MDM) process includes the analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem. On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, lab or imaging results, or specialist consultations.",0,Wikipedia,Medicine,https://en.wikipedia.org/wiki/Medicine,,Medicine,wikipedia_api
human_wiki_0320,"Genetics is the study of genes, genetic variation, and heredity in organisms. It is an important branch in biology because heredity is vital to organisms' evolution. Gregor Mendel, a Moravian Augustinian friar working in the 19th century in Brno, was the first to study genetics scientifically. Mendel studied ""trait inheritance"", patterns in the way traits are handed down from parents to offspring over time. He observed that organisms (pea plants) inherit traits by way of discrete ""units of inheritance"". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene. Trait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded to study the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including molecular genetics, epigenetics, population genetics, and paleogenetics. Organisms studied within the broad field span the domains of life (archaea, bacteria, and eukarya). Genetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a living cell or organism may increase or decrease gene transcription. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate (lacking sufficient waterfall or rain). While the average height the two corn stalks could grow to is genetically determined, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.",0,Wikipedia,Genetics,https://en.wikipedia.org/wiki/Genetics,,Genetics,wikipedia_api
human_wiki_0321,"History The observation that living things inherit traits from their parents has been used since prehistoric times to improve crop plants and animals through selective breeding. The modern science of genetics, seeking to understand this process, began with the work of the Augustinian friar Gregor Mendel in the mid-19th century.",0,Wikipedia,Genetics,https://en.wikipedia.org/wiki/Genetics,,Genetics,wikipedia_api
human_wiki_0322,"Prior to Mendel, Imre Festetics, a Hungarian noble, who lived in Kőszeg before Mendel, was the first who used the word ""genetic"" in hereditarian context, and is considered the first geneticist. He described several rules of biological inheritance in his work The genetic laws of nature (Die genetischen Gesetze der Natur, 1819). His second law is the same as that which Mendel published. In his third law, he developed the basic principles of mutation (he can be considered a forerunner of Hugo de Vries). Festetics argued that changes observed in the generation of farm animals, plants, and humans are the result of scientific laws. Festetics empirically deduced that organisms inherit their characteristics, not acquire them. He recognized recessive traits and inherent variation by postulating that traits of past generations could reappear later, and organisms could produce progeny with different attributes. These observations represent an important prelude to Mendel's theory of particulate inheritance insofar as it features a transition of heredity from its status as myth to that of a scientific discipline, by providing a fundamental theoretical basis for genetics in the twentieth century.",0,Wikipedia,Genetics,https://en.wikipedia.org/wiki/Genetics,,Genetics,wikipedia_api
human_wiki_0323,"Other theories of inheritance preceded Mendel's work. A popular theory during the 19th century, and implied by Charles Darwin's 1859 On the Origin of Species, was blending inheritance: the idea that individuals inherit a smooth blend of traits from their parents. Mendel's work provided examples where traits were definitely not blended after hybridization, showing that traits are produced by combinations of distinct genes rather than a continuous blend. Blending of traits in the progeny is now explained by the action of multiple genes with quantitative effects. Another theory that had some support at that time was the inheritance of acquired characteristics: the belief that individuals inherit traits strengthened by their parents. This theory (commonly associated with Jean-Baptiste Lamarck) is now known to be wrong—the experiences of individuals do not affect the genes they pass to their children. Other theories included Darwin's pangenesis (which had both acquired and inherited aspects) and Francis Galton's reformulation of pangenesis as both particulate and inherited.",0,Wikipedia,Genetics,https://en.wikipedia.org/wiki/Genetics,,Genetics,wikipedia_api
human_wiki_0324,"Mendelian genetics Modern genetics started with Mendel's studies of the nature of inheritance in plants. In his paper ""Versuche über Pflanzenhybriden"" (""Experiments on Plant Hybridization""), presented in 1865 to the Naturforschender Verein (Society for Research in Nature) in Brno, Mendel traced the inheritance patterns of certain traits in pea plants and described them mathematically. Although this pattern of inheritance could only be observed for a few traits, Mendel's work suggested that heredity was particulate, not acquired, and that the inheritance patterns of many traits could be explained through simple rules and ratios. The importance of Mendel's work did not gain wide understanding until 1900, after his death, when Hugo de Vries and other scientists rediscovered his research. William Bateson, a proponent of Mendel's work, coined the word genetics in 1905. The adjective genetic, derived from the Greek word genesis—γένεσις, ""origin"", predates the noun and was first used in a biological sense in 1860. Bateson both acted as a mentor and was aided significantly by the work of other scientists from Newnham College at Cambridge, specifically the work of Becky Saunders, Nora Darwin Barlow, and Muriel Wheldale Onslow. Bateson popularized the usage of the word genetics to describe the study of inheritance in his inaugural address to the Third International Conference on Plant Hybridization in London in 1906. After the rediscovery of Mendel's work, scientists tried to determine which molecules in the cell were responsible for inheritance. In 1900, Nettie Stevens began studying the mealworm. Over the next 11 years, she discovered that females only had the X chromosome and males had both X and Y chromosomes. She was able to conclude that sex is a chromosomal factor and is determined by the male. In 1911, Thomas Hunt Morgan argued that genes are on chromosomes, based on observations of a sex-linked white eye mutation in fruit flies. In 1913, his student Alfred Sturtevant used the phenomenon of genetic linkage to show that genes are arranged linearly on the chromosome.",0,Wikipedia,Genetics,https://en.wikipedia.org/wiki/Genetics,,Genetics,wikipedia_api
human_wiki_0325,"Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body. These contrast with benign tumors, which do not spread. Possible signs and symptoms of cancer include a lump, abnormal bleeding, prolonged cough, unexplained weight loss, and a change in bowel movements. While these symptoms may indicate cancer, they can also have other causes. Over 100 types of cancers affect humans. About 33% of deaths from cancer are caused by tobacco and alcohol consumption, obesity, lack of fruit and vegetables in diet and lack of exercise. Other factors include certain infections, exposure to ionizing radiation, and environmental pollutants. Infection with specific viruses, bacteria, and parasites causes approximately 16–18% of cancers worldwide. These infectious agents include Helicobacter pylori, hepatitis B, hepatitis C, HPV, Epstein–Barr virus, Human T-lymphotropic virus 1, Kaposi's sarcoma-associated herpesvirus and Merkel cell polyomavirus. Human immunodeficiency virus (HIV) does not directly cause cancer, but it causes immune deficiency that can increase the risk of cancer from other infections, sometimes up to several thousandfold (in the case of Kaposi's sarcoma). Importantly, vaccination against the hepatitis B virus and the human papillomavirus have been shown to nearly eliminate the risk of cancers caused by these viruses in persons successfully vaccinated prior to infection. These environmental factors act, at least partly, by changing the genes of a cell. Typically, many genetic changes are required before cancer develops. Approximately 5–10% of cancers are due to inherited genetic defects. Cancer can be detected by certain signs and symptoms or screening tests. It is then typically further investigated by medical imaging and confirmed by biopsy. The risk of developing certain cancers can be reduced by not smoking, maintaining a healthy weight, limiting alcohol intake, eating plenty of vegetables, fruits, and whole grains, vaccination against certain infectious diseases, limiting consumption of processed meat and red meat, and limiting exposure to direct sunlight. Early detection through screening is useful for cervical and colorectal cancer. The benefits of screening for breast cancer are controversial. Cancer is often treated with some combination of radiation therapy, surgery, chemotherapy and targeted therapy. More personalized therapies that harness a patient's immune system are emerging in the field of cancer immunotherapy. Palliative care is a medical specialty that delivers advanced pain and symptom management, which may be particularly important in those with advanced disease.. The chance of survival depends on the type of cancer and extent of disease at the start of treatment. In children under 15 at diagnosis, the five-year survival rate in the developed world is on average 80%. For cancer in the United States, the average five-year survival rate is 66% for all ages. In 2015, about 90.5 million people worldwide had cancer. In 2019, annual cancer cases grew by 23.6 million people, and there were 10 million deaths worldwide, representing over the previous decade increases of 26% and 21%, respectively. The most common types of cancer in males are lung cancer, prostate cancer, colorectal cancer, and stomach cancer. In females, the most common types are breast cancer, colorectal cancer, lung cancer, and cervical cancer. If skin cancer other than melanoma were included in total new cancer cases each year, it would account for around 40% of cases. In children, acute lymphoblastic leukemia and brain tumors are most common, except in Africa, where non-Hodgkin lymphoma occurs more often. In 2012, about 165,000 children under 15 years of age were diagnosed with cancer. The risk of cancer increases significantly with age, and many cancers occur more commonly in developed countries. Rates are increasing as more people live to an old age and as lifestyle changes occur in the developing world. The global total economic costs of cancer were estimated at US$1.16 trillion (equivalent to $1.67 trillion in 2024) per year as of 2010.",0,Wikipedia,Cancer,https://en.wikipedia.org/wiki/Cancer,,Cancer,wikipedia_api
human_wiki_0326,"Etymology and definitions The word comes from the ancient Greek καρκίνος, meaning 'crab' and 'tumor'. Greek physicians Hippocrates and Galen, among others, noted the similarity of crabs to some tumors with swollen veins. The word was introduced in English in the modern medical sense around 1600. Cancers comprise a large family of diseases that involve abnormal cell growth with the potential to invade or spread to other parts of the body. They form a subset of neoplasms. A neoplasm or tumor is a group of cells that have undergone unregulated growth and will often form a mass or lump, but may be distributed diffusely. All tumor cells show the six hallmarks of cancer. These characteristics are required to produce a malignant tumor. They include:",0,Wikipedia,Cancer,https://en.wikipedia.org/wiki/Cancer,,Cancer,wikipedia_api
human_wiki_0327,Cell growth and division absent the proper signals Continuous growth and division even given contrary signals Avoidance of programmed cell death Limitless number of cell divisions Promoting blood vessel construction Invasion of tissue and formation of metastases The progression from normal cells to cells that can form a detectable mass to cancer involves multiple steps known as malignant progression.,0,Wikipedia,Cancer,https://en.wikipedia.org/wiki/Cancer,,Cancer,wikipedia_api
human_wiki_0328,"Signs and symptoms When cancer begins, it produces no symptoms. Signs and symptoms appear as the mass grows or ulcerates. The findings that result depend on cancer's type and location. Few symptoms are specific. Many frequently occur in individuals who have other conditions. Cancer can be difficult to diagnose and can be considered a ""great imitator"". People may become anxious or depressed post-diagnosis. The risk of suicide in people with cancer is approximately double.",0,Wikipedia,Cancer,https://en.wikipedia.org/wiki/Cancer,,Cancer,wikipedia_api
human_wiki_0329,"Local symptoms Local symptoms may occur due to the mass of the tumor or its ulceration. For example, mass effects from lung cancer can block the bronchus resulting in cough or pneumonia; esophageal cancer can cause narrowing of the esophagus, making it difficult or painful to swallow; and colorectal cancer may lead to narrowing or blockages in the bowel, affecting bowel habits. Masses in breasts or testicles may produce observable lumps. Ulceration can cause bleeding that can lead to symptoms such as coughing up blood (lung cancer), anemia or rectal bleeding (colon cancer), blood in the urine (bladder cancer), or abnormal vaginal bleeding (endometrial or cervical cancer). Although localized pain may occur in advanced cancer, the initial tumor is usually painless. Some cancers can cause a buildup of fluid within the chest or abdomen.",0,Wikipedia,Cancer,https://en.wikipedia.org/wiki/Cancer,,Cancer,wikipedia_api
human_wiki_0330,"Space exploration is the physical investigation of outer space by uncrewed robotic space probes and through human spaceflight. While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries. The early era of space exploration was driven by a ""Space Race"" in which the Soviet Union and the United States vied to demonstrate their technological superiority. Landmarks of this era include the launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969. The Soviet space program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Alexei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971.  In the 1970s, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation, the foremost example being the International Space Station (ISS), built between 1998 and 2011. The 2000s brought advancements in the national space-exploration programs of China, the European Union, Japan, and India. The 2010s saw the rise of the private space industry in earnest with the development of private launch vehicles, space capsules, and satellite manufacturing. In the 2020s, the two primary global programs gaining traction are Moon-focused: the Chinese-led International Lunar Research Station and the U.S.-led Artemis Program, with its plan to build the Lunar Gateway and the Artemis Base Camp, each with a set of international partners.",0,Wikipedia,Space exploration,https://en.wikipedia.org/wiki/Space_exploration,,Space_exploration,wikipedia_api
human_wiki_0331,"History of exploration First telescopes The first telescope is said to have been invented in 1608 in the Netherlands by an eyeglass maker named Hans Lippershey, but their first recorded use in astronomy was by Galileo Galilei in 1609. In 1668 Isaac Newton built his own reflecting telescope, the first fully functional telescope of this kind, and a landmark for future developments due to its superior features over the previous Galilean telescope. A string of discoveries in the Solar System (and beyond) followed, then and in the next centuries: the mountains of the Moon, the phases of Venus, the main satellites of Jupiter and Saturn, the rings of Saturn, many comets, the asteroids, the new planets Uranus and Neptune, and many more satellites. The Orbiting Astronomical Observatory 2 was the first space telescope launched 1968, but the launch of the Hubble Space Telescope in 1990 set a milestone. As of 1 December 2022, there were 5,284 confirmed exoplanets discovered. The Milky Way is estimated to contain 100–400 billion stars and more than 100 billion planets. There are at least 2 trillion galaxies in the observable universe. HD1 is the most distant known object from Earth, reported as 33.4 billion light-years away.",0,Wikipedia,Space exploration,https://en.wikipedia.org/wiki/Space_exploration,,Space_exploration,wikipedia_api
human_wiki_0332,"First outer space flights MW 18014 was a German V-2 rocket test launch that took place on 20 June 1944, at the Peenemünde Army Research Center in Peenemünde. It was the first human-made object to reach outer space, attaining an apogee of 176 kilometers, which is well above the Kármán line. It was a vertical test launch. Although the rocket reached space, it did not reach orbital velocity, and therefore returned to Earth in an impact, becoming the first sub-orbital spaceflight. In 1949, the Bumper-WAC reached an altitude of 393 kilometres (244 mi), becoming the first human-made object to enter space, according to NASA.",0,Wikipedia,Space exploration,https://en.wikipedia.org/wiki/Space_exploration,,Space_exploration,wikipedia_api
human_wiki_0333,"First object in orbit The first successful orbital launch was of the Soviet uncrewed Sputnik 1 (""Satellite 1"") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted ""beeps"" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data were encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.",0,Wikipedia,Space exploration,https://en.wikipedia.org/wiki/Space_exploration,,Space_exploration,wikipedia_api
human_wiki_0334,"First human outer space flight The first successful human spaceflight was Vostok 1 (""East 1""), carrying the 27-year-old Russian cosmonaut, Yuri Gagarin, on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.",0,Wikipedia,Space exploration,https://en.wikipedia.org/wiki/Space_exploration,,Space_exploration,wikipedia_api
human_wiki_0335,"Social media are new media technologies that facilitate the creation, sharing and aggregation of content (such as ideas, interests, and other forms of expression) amongst virtual communities and networks. Common features include:",0,Wikipedia,Social media,https://en.wikipedia.org/wiki/Social_media,,Social_media,wikipedia_api
human_wiki_0336,"Online platforms  enable users to create and share content and participate in social networking. User-generated content—such as text posts or comments, digital photos or videos, and data generated through online interactions. Service-specific profiles that are designed and maintained by the social media organization. Social media helps the development of online social networks by connecting a user's profile with those of other individuals or groups. The term social in regard to media suggests platforms enable communal activity. Social media helps people connect and build networks. Users access social media through web-based or mobile applications. These interactive platforms allow individuals, communities, businesses, and organizations to share, co-create, discuss, participate in, and modify user-generated or self-curated content. Social media is used to share memories, form friendships, and learn. They may be used to promote people, companies, products, and ideas. Social media can be used to consume, publish, or share news. Social media platforms can be categorized based on their primary function.",0,Wikipedia,Social media,https://en.wikipedia.org/wiki/Social_media,,Social_media,wikipedia_api
human_wiki_0337,"Social networking sites like Facebook and LinkedIn focus on building personal and professional connections. Microblogging platforms, such as Twitter (now X), Threads and Mastodon, emphasize short-form content and rapid information sharing. Media sharing networks, including Instagram, TikTok, YouTube, and Snapchat, allow users to share images, videos, and live streams. Discussion and community forums like Reddit, Quora, and Discord facilitate conversations, Q&A, and niche community engagement. Live streaming platforms, such as Twitch, Facebook Live, and YouTube Live, enable real-time audience interaction. Decentralized social media platforms like Mastodon and Bluesky aim to provide social networking without corporate control, offering users more autonomy over their data and interactions. Popular social media platforms with over 100 million registered users include Twitter, Facebook, WeChat, ShareChat, Instagram, Pinterest, QZone, Weibo, VK, Tumblr, Baidu Tieba, Threads and LinkedIn. Depending on interpretation, other popular platforms that are sometimes referred to as social media services include YouTube, Letterboxd, QQ, Quora, Telegram, WhatsApp, Signal, LINE, Snapchat, Viber, Reddit, Discord, and TikTok. Wikis are examples of collaborative content creation. Social media outlets differ from old media (e.g. newspapers, TV, and radio broadcasting) in many ways, including quality, reach, frequency, usability, relevancy, and permanence. Social media outlets operate in a dialogic transmission system (many sources to many receivers) while traditional media operate under a monologic transmission model (one source to many receivers). For instance, a newspaper is delivered to many subscribers, and a radio station broadcasts the same programs to a city. Social media has been criticized for a range of negative impacts on children and teenagers, including exposure to inappropriate content, exploitation by adults, sleep problems, attention problems, feelings of exclusion, and various mental health maladies. Social media has also received criticism as worsening political polarization and undermining democracy, exacerbated by platform capture by vested interests, with journalist Maria Ressa deeming it ""toxic sludge"" for increasing distrust among members of society. Major news outlets often have strong controls in place to avoid and fix false claims, but social media's unique qualities bring viral content with little to no oversight. ""Algorithms that track user engagement to prioritize what is shown tend to favor content that spurs negative emotions like anger and outrage. Overall, most online misinformation originates from a small minority of ""superspreaders,"" but social media amplifies their reach and influence.""",0,Wikipedia,Social media,https://en.wikipedia.org/wiki/Social_media,,Social_media,wikipedia_api
human_wiki_0338,"History Early computing The PLATO system was launched in 1960 at the University of Illinois and subsequently commercially marketed by Control Data Corporation. It offered early forms of social media features with innovations such as Notes, PLATO's message-forum application; TERM-talk, its instant-messaging feature; Talkomatic, perhaps the first online chat room; News Report, a crowdsourced online newspaper, and blog and Access Lists, enabling the owner of a note file or other application to limit access to a certain set of users, for example, only friends, classmates, or co-workers. ARPANET, which came online in 1969, had by the late 1970s enabled exchange of non-government/business ideas and communication, as evidenced by the network etiquette (or ""netiquette"") described in a 1982 handbook on computing at MIT's Artificial Intelligence Laboratory. ARPANET evolved into the Internet in the 1990s. Usenet, conceived by Tom Truscott and Jim Ellis in 1979 at the University of North Carolina at Chapel Hill and Duke University, was the first open social media app, established in 1980.",0,Wikipedia,Social media,https://en.wikipedia.org/wiki/Social_media,,Social_media,wikipedia_api
human_wiki_0339,"A precursor of the electronic bulletin board system (BBS), known as Community Memory, appeared by 1973. Mainstream BBSs arrived with the Computer Bulletin Board System in Chicago, which launched on February 16, 1978. Before long, most major US cities had more than one BBS, running on TRS-80, Apple II, Atari 8-bit computers, IBM PC, Commodore 64, Sinclair, and others. CompuServe, Prodigy, and AOL were three of the largest BBS companies and were the first to migrate to the Internet in the 1990s. Between the mid-1980s and the mid-1990s, BBSes numbered in the tens of thousands in North America alone. Message forums were the signature BBS phenomenon throughout the 1980s and early 1990s. In 1991, Tim Berners-Lee integrated HTML hypertext software with the Internet, creating the World Wide Web. This breakthrough led to an explosion of blogs, list servers, and email services. Message forums migrated to the web, and evolved into Internet forums, supported by cheaper access as well as the ability to handle far more people simultaneously. These early text-based systems expanded to include images and video in the 21st century, aided by digital cameras and camera phones.",0,Wikipedia,Social media,https://en.wikipedia.org/wiki/Social_media,,Social_media,wikipedia_api
human_wiki_0340,"A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes. Blockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. A blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail. Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model ""snake oil""; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.",0,Wikipedia,Blockchain,https://en.wikipedia.org/wiki/Blockchain,,Blockchain,wikipedia_api
human_wiki_0341,"History Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation ""Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups"". Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block. Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995. The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network. In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020. The words block and chain were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, blockchain, by 2016. According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters' phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce. In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ""planning or [looking at] active experimentation with blockchain"". For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.",0,Wikipedia,Blockchain,https://en.wikipedia.org/wiki/Blockchain,,Blockchain,wikipedia_api
human_wiki_0342,"Structure and design A blockchain is a decentralized, distributed, and often public, digital ledger consisting of records called blocks that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double-spending. A blockchain has been described as a value-exchange protocol. Logically, a blockchain can be seen as consisting of several layers:",0,Wikipedia,Blockchain,https://en.wikipedia.org/wiki/Blockchain,,Blockchain,wikipedia_api
human_wiki_0343,"infrastructure (hardware) networking (node discovery, information propagation and verification) consensus (proof of work, proof of stake) data (blocks, transactions) application (smart contracts/decentralized applications, if applicable)",0,Wikipedia,Blockchain,https://en.wikipedia.org/wiki/Blockchain,,Blockchain,wikipedia_api
human_wiki_0344,"Blocks Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree. Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the initial block, which is known as the genesis block (Block 0). To assure the integrity of a block and the data contained in it, the block is usually digitally signed. Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially as more blocks are built on top of it, eventually becoming very low. For example, bitcoin uses a proof-of-work system, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.",0,Wikipedia,Blockchain,https://en.wikipedia.org/wiki/Blockchain,,Blockchain,wikipedia_api
human_wiki_0345,"The immune system is a network of biological systems that protects an organism from diseases. It detects and responds to a wide variety of pathogens, such as viruses, bacteria, and parasites, as well as cancer cells and objects, such as wood splinters—distinguishing them from the organism's own healthy tissue. Many species have two major subsystems of the immune system. The innate immune system provides a preconfigured response to broad groups of situations and stimuli. The adaptive immune system provides a tailored response to each stimulus by learning to recognize molecules it has previously encountered. Both use molecules and cells to perform their functions. Nearly all organisms have some kind of immune system. Bacteria have a rudimentary immune system in the form of enzymes that protect against viral infections. Other basic immune mechanisms evolved in ancient plants and animals and remain in their modern descendants. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms, including the ability to adapt to recognize pathogens more efficiently. Adaptive (or acquired) immunity creates an immunological memory leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination. Dysfunction of the immune system can cause autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. Autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.",0,Wikipedia,Immune system,https://en.wikipedia.org/wiki/Immune_system,,Immune_system,wikipedia_api
human_wiki_0346,"Layered defense The immune system protects its host from infection with layered defenses of increasing specificity. Physical barriers prevent pathogens such as bacteria and viruses from entering the organism. If a pathogen breaches these barriers, the innate immune system provides an immediate, but non-specific response. Innate immune systems are found in all animals. If pathogens successfully evade the innate response, vertebrates possess a second layer of protection, the adaptive immune system, which is activated by the innate response. Here, the immune system adapts its response during an infection to improve its recognition of the pathogen. This improved response is then retained after the pathogen has been eliminated, in the form of an immunological memory, and allows the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered.",0,Wikipedia,Immune system,https://en.wikipedia.org/wiki/Immune_system,,Immune_system,wikipedia_api
human_wiki_0347,"Both innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non-self molecules. In immunology, self molecules are components of an organism's body that can be distinguished from foreign substances by the immune system. Conversely, non-self molecules are those recognized as foreign molecules. One class of non-self molecules are called antigens (originally named for being antibody generators) and are defined as substances that bind to specific immune receptors and elicit an immune response.",0,Wikipedia,Immune system,https://en.wikipedia.org/wiki/Immune_system,,Immune_system,wikipedia_api
human_wiki_0348,"Surface barriers Several barriers protect organisms from infection, including mechanical, chemical, and biological barriers. The waxy cuticle of most leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin are examples of mechanical barriers that are the first line of defense against infection. Organisms cannot be completely sealed from their environments, so systems act to protect body openings such as the lungs, intestines, and the genitourinary tract. In the lungs, coughing and sneezing mechanically eject pathogens and other irritants from the respiratory tract. The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms. Chemical barriers also protect against infection. The skin and respiratory tract secrete antimicrobial peptides such as the β-defensins. Enzymes such as lysozyme and phospholipase A2 in saliva, tears, and breast milk are also antibacterials. Vaginal secretions serve as a chemical barrier following menarche, when they become slightly acidic, while semen contains defensins and zinc to kill pathogens. In the stomach, gastric acid serves as a chemical defense against ingested pathogens. Within the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, changing the conditions in their environment, such as pH or available iron. As a result, the probability that pathogens will reach sufficient numbers to cause illness is reduced.",0,Wikipedia,Immune system,https://en.wikipedia.org/wiki/Immune_system,,Immune_system,wikipedia_api
human_wiki_0349,"Innate immune system Microorganisms or toxins that successfully enter an organism encounter the cells and mechanisms of the innate immune system. The innate response is usually triggered when microbes are identified by pattern recognition receptors, which recognize components that are conserved among broad groups of microorganisms, or when damaged, injured or stressed cells send out alarm signals, many of which are recognized by the same receptors as those that recognize pathogens. Innate immune defenses are non-specific, meaning these systems respond to pathogens in a generic way. This system does not confer long-lasting immunity against a pathogen. The innate immune system is the dominant system of host defense in most organisms, and the only one in plants.",0,Wikipedia,Immune system,https://en.wikipedia.org/wiki/Immune_system,,Immune_system,wikipedia_api
human_wiki_0350,"Wind power is the use of wind energy to generate useful work. Historically, wind power was used by sails, windmills and windpumps, but today it is mostly used to generate electricity. This article deals only with wind power for electricity generation. Today, wind power is generated almost completely using wind turbines, generally grouped into wind farms and connected to the electrical grid. In 2024, wind supplied about 2,500 TWh of electricity, which was over 8% of world electricity. With about 100 GW added during 2021, mostly in China and the United States, global installed wind power capacity exceeded 800 GW. 30 countries generated more than a tenth of their electricity from wind power in 2024 and wind generation has nearly tripled since 2015. To help meet the Paris Agreement goals to limit climate change, analysts say it should expand much faster – by over 1% of electricity generation per year. Wind power is a sustainable, renewable energy source, and has a much smaller impact on the environment than burning fossil fuels. Wind power is variable, so it needs energy storage or other dispatchable generation energy sources to attain a reliable supply of electricity. Land-based (onshore) wind farms have a greater visual impact on the landscape than most other power stations per energy produced. Wind farms sited offshore have less visual impact and have higher capacity factors, although they are generally more expensive. Offshore wind power currently has a share of about 10% of new installations. Wind power is one of the lowest-cost electricity sources per unit of energy produced.  In many locations, new onshore wind farms are cheaper than new coal or gas plants. Regions in the higher northern and southern latitudes have the highest potential for wind power. In most regions, wind power generation is higher in nighttime, and in winter when solar power output is low. So combinations of wind and solar power are suitable in many countries.",0,Wikipedia,Wind power,https://en.wikipedia.org/wiki/Wind_power,,Wind_power,wikipedia_api
human_wiki_0351,"Wind energy resources Wind is air movement in the Earth's atmosphere. In a unit of time, say 1 second, the volume of air that had passed an area                         A                 {\displaystyle A}     is                         A         v                 {\displaystyle Av}    . If the air density is                         ρ                 {\displaystyle \rho }    , the flow rate of this volume of air is                                                                M                                Δ                 t                                                         =         ρ         A         v                 {\displaystyle {\tfrac {M}{\Delta t}}=\rho Av}    , and the power transfer, or energy transfer per second is                         P         =                                                1               2                                                                                 M                                Δ                 t                                                                    v                        2                             =                                                1               2                                          ρ         A                    v                        3                                     {\displaystyle P={\tfrac {1}{2}}{\tfrac {M}{\Delta t}}v^{2}={\tfrac {1}{2}}\rho Av^{3}}    . Wind power is thus proportional to the third power of the wind speed; the available power increases eightfold when the wind speed doubles. Change of wind speed by a factor of 2.1544 increases the wind power by one order of magnitude (multiply by 10). The global wind kinetic energy averaged approximately 1.50 MJ/m2 over the period from 1979 to 2010, 1.31 MJ/m2 in the Northern Hemisphere with 1.70 MJ/m2 in the Southern Hemisphere. The atmosphere acts as a thermal engine, absorbing heat at higher temperatures, releasing heat at lower temperatures. The process is responsible for the production of wind kinetic energy at a rate of 2.46 W/m2 thus sustaining the circulation of the atmosphere against friction. Through wind resource assessment, it is possible to estimate wind power potential globally, by country or region, or for a specific site. The Global Wind Atlas provided by the Technical University of Denmark in partnership with the World Bank provides a global assessment of wind power potential. Unlike 'static' wind resource atlases which average estimates of wind speed and power density across multiple years, tools such as Renewables.ninja provide time-varying simulations of wind speed and power output from different wind turbine models at an hourly resolution. More detailed, site-specific assessments of wind resource potential can be obtained from specialist commercial providers, and many of the larger wind developers have in-house modeling capabilities. The total amount of economically extractable power available from the wind is considerably more than present human power use from all sources. The strength of wind varies, and an average value for a given location does not alone indicate the amount of energy a wind turbine could produce there. To assess prospective wind power sites, a probability distribution function is often fit to the observed wind speed data. Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution can be used as a less accurate, but simpler model.",0,Wikipedia,Wind power,https://en.wikipedia.org/wiki/Wind_power,,Wind_power,wikipedia_api
human_wiki_0352,"Wind farms A wind farm is a group of wind turbines in the same location. A large wind farm may consist of several hundred individual wind turbines distributed over an extended area. The land between the turbines may be used for agricultural or other purposes. A wind farm may also be located offshore. Almost all large wind turbines have the same design — a horizontal axis wind turbine having an upwind rotor with 3 blades, attached to a nacelle on top of a tall tubular tower. In a wind farm, individual turbines are interconnected with a medium voltage (often 34.5 kV) power collection system and communications network. In general, a distance of 7D (7 times the rotor diameter of the wind turbine) is set between each turbine in a fully developed wind farm. At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system.",0,Wikipedia,Wind power,https://en.wikipedia.org/wiki/Wind_power,,Wind_power,wikipedia_api
human_wiki_0353,"Generator characteristics and stability Most modern turbines use variable speed generators combined with either a partial or full-scale power converter between the turbine generator and the collector system, which generally have more desirable properties for grid interconnection and have low voltage ride through-capabilities. Modern turbines use either doubly fed electric machines with partial-scale converters or squirrel-cage induction generators or synchronous generators (both permanently and electrically excited) with full-scale converters. Black start is possible and is being further developed for places (such as Iowa) which generate most of their electricity from wind. Transmission system operators will supply a wind farm developer with a grid code to specify the requirements for interconnection to the transmission grid. This will include the power factor, the constancy of frequency, and the dynamic behaviour of the wind farm turbines during a system fault.",0,Wikipedia,Wind power,https://en.wikipedia.org/wiki/Wind_power,,Wind_power,wikipedia_api
human_wiki_0354,"Offshore wind power Offshore wind power is wind farms in large bodies of water, usually the sea. These installations can use the more frequent and powerful winds that are available in these locations and have less visual impact on the landscape than land-based projects. However, the construction and maintenance costs are considerably higher. As of November 2021, the Hornsea Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 1,218 MW.",0,Wikipedia,Wind power,https://en.wikipedia.org/wiki/Wind_power,,Wind_power,wikipedia_api
human_wiki_0355,"The global COVID-19 pandemic (also known as the coronavirus pandemic), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), began with an outbreak in Wuhan, China, in December 2019. Soon afterward, it spread to other parts of Asia and then worldwide in early 2020. The World Health Organization (WHO) declared the outbreak a public health emergency of international concern (PHEIC) on 30 January 2020, and assessed it as having become a pandemic on 11 March. The WHO declared the public health emergency caused by COVID-19 had ended in May 2023. COVID-19 symptoms range from asymptomatic to deadly, but most commonly include fever, sore throat, nocturnal cough, and fatigue. Transmission of the virus is often through airborne particles. Mutations have produced many strains (variants) with varying degrees of infectivity and virulence. COVID-19 vaccines were developed rapidly and deployed to the general public beginning in December 2020, made available through government and international programmes such as COVAX, aiming to provide vaccine equity. Treatments include novel antiviral drugs and symptom control. Common mitigation measures during the public health emergency included travel restrictions, lockdowns, business restrictions and closures, workplace hazard controls, mask mandates, quarantines, testing systems, and contact tracing of the infected. The pandemic caused severe social and economic disruption around the world, including the largest global recession since the Great Depression. Widespread supply shortages, including food shortages, were caused by supply chain disruptions and panic buying. Reduced human activity led to an unprecedented temporary decrease in pollution. Educational institutions and public areas were partially or fully closed in many jurisdictions, and many events were cancelled or postponed during 2020 and 2021. Telework became much more common for white-collar workers as the pandemic evolved. Misinformation circulated through social media and occasionally through mass media, and political tensions intensified. The pandemic raised issues of racial and geographic discrimination, health equity, and the balance between public health imperatives and individual rights. The disease has continued to circulate since 2023. As of 2024, experts were uncertain as to whether it still qualified as a pandemic. Different definitions of pandemics lead to different determinations of when they end. As of 28 November 2025, COVID-19 has caused 7,102,493 confirmed deaths, and 18.2 to 33.5 million estimated deaths. The pandemic ranks as the fifth-deadliest pandemic or epidemic in history.",0,Wikipedia,COVID-19 pandemic,https://en.wikipedia.org/wiki/COVID-19_pandemic,,COVID-19_pandemic,wikipedia_api
human_wiki_0356,"Terminology Pandemic In epidemiology, a pandemic is defined as ""an epidemic occurring over a very wide area, crossing international boundaries, and usually affecting a large number of people"". During the COVID-19 pandemic, as with other pandemics, the meaning of this term has been challenged. The end of a pandemic or other epidemic only rarely involves the total disappearance of a disease, and historically, much less attention has been given to defining the ends of epidemics than their beginnings. The ends of particular epidemics have been defined in a variety of ways, differing according to academic field, and differently based on location and social group. An epidemic's end can be considered a social phenomenon, not just a biological one. Time reported in March 2024 that expert opinions differ on whether or not COVID-19 is currently considered endemic or pandemic, and that the WHO continued to call the disease a pandemic on its website.",0,Wikipedia,COVID-19 pandemic,https://en.wikipedia.org/wiki/COVID-19_pandemic,,COVID-19_pandemic,wikipedia_api
human_wiki_0357,"Virus names During the initial outbreak in Wuhan, the virus and disease were commonly referred to as ""coronavirus"", ""Wuhan coronavirus"", ""the coronavirus outbreak"" and the ""Wuhan coronavirus outbreak"", with the disease sometimes called ""Wuhan pneumonia"". In January 2020, the WHO recommended 2019-nCoV and 2019-nCoV acute respiratory disease as interim names for the virus and disease per 2015 international guidelines against using geographical locations (e.g. Wuhan, China), animal species, or groups of people in disease and virus names in part to prevent social stigma. WHO finalised the official names COVID-19 and SARS-CoV-2 on 11 February 2020. WHO Director-General Tedros Ghebreyesus explained: CO for corona, VI for virus, D for disease and 19 for when the outbreak was first identified (31 December 2019). WHO additionally uses ""the COVID-19 virus"" and ""the virus responsible for COVID-19"" in public communications. WHO named variants of concern and variants of interest using Greek letters. The initial practice of naming them according to where the variants were identified (e.g. Delta began as the ""Indian variant"") is no longer common. A more systematic naming scheme reflects the variant's PANGO lineage (e.g., Omicron's lineage is B.1.1.529) and is used for other variants.",0,Wikipedia,COVID-19 pandemic,https://en.wikipedia.org/wiki/COVID-19_pandemic,,COVID-19_pandemic,wikipedia_api
human_wiki_0358,"Epidemiology Background SARS-CoV-2 is a virus closely related to bat coronaviruses, pangolin coronaviruses, and SARS-CoV. The first known outbreak (the 2019–2020 COVID-19 outbreak in mainland China) started in Wuhan, Hubei, China, in December 2019. Many early cases were linked to people who had visited the Huanan Seafood Wholesale Market there, but it is possible that human-to-human transmission began earlier. Molecular clock analysis suggests that the first cases were likely to have been between October and November 2019. The scientific consensus is that the virus is most likely of a zoonotic origin, from bats or another closely related mammal. While other explanations such as speculations that SARS-CoV-2 was accidentally released from a laboratory have been proposed, as of 2021 these were not sufficiently supported by evidence.",0,Wikipedia,COVID-19 pandemic,https://en.wikipedia.org/wiki/COVID-19_pandemic,,COVID-19_pandemic,wikipedia_api
human_wiki_0359,"Cases Official ""case"" counts refer to the number of people who have been tested for COVID-19 and whose test has been confirmed positive according to official protocols whether or not they experienced symptomatic disease. Due to the effect of sampling bias, studies which obtain a more accurate number by extrapolating from a random sample have consistently found that total infections considerably exceed the reported case counts. Many countries, early on, had official policies to not test those with only mild symptoms. The strongest risk factors for severe illness are obesity, complications of diabetes, anxiety disorders, and the total number of conditions. During the start of the COVID-19 pandemic, it was not clear whether young people were less likely to be infected, or less likely to develop symptoms and be tested. A retrospective cohort study in China found that children and adults were just as likely to be infected. Among more thorough studies, preliminary results from 9 April 2020 found that in Gangelt, the centre of a major infection cluster in Germany, 15 per cent of a population sample tested positive for antibodies. Screening for COVID-19 in pregnant women in New York City, and blood donors in the Netherlands, found rates of positive antibody tests that indicated more infections than reported. Seroprevalence-based estimates are conservative as some studies show that persons with mild symptoms do not have detectable antibodies. Initial estimates of the basic reproduction number (R0) for COVID-19 in January 2020 were between 1.4 and 2.5, but a subsequent analysis claimed that it may be about 5.7 (with a 95 per cent confidence interval of 3.8 to 8.9). In December 2021, the number of cases continued to climb due to several factors, including new COVID-19 variants. As of that 28 December, 282,790,822 individuals worldwide had been confirmed as infected. As of 14 April 2022, over 500 million cases were confirmed globally. Most cases are unconfirmed, with the Institute for Health Metrics and Evaluation estimating the true number of cases as of early 2022 to be in the billions.",0,Wikipedia,COVID-19 pandemic,https://en.wikipedia.org/wiki/COVID-19_pandemic,,COVID-19_pandemic,wikipedia_api
human_wiki_0360,"5G is the fifth generation of cellular network technology and the successor to 4G.  First deployed in 2019,  its technical standards are developed by the 3rd Generation Partnership Project (3GPP)  in cooperation with the ITU’s IMT-2020 program. 5G networks divide coverage areas into smaller zones called cells, enabling devices to connect to local base stations via radio. Each station connects to the broader telephone network and the Internet through high-speed optical fiber or wireless backhaul. Compared to 4G, 5G offers significantly faster data transfer speed—up to 10Gbit/s in tests—and lower latency, with response times of just a few milliseconds. These advancements allow networks to support more users and applications such as extended reality, autonomous vehicles, remote surgery trials, and fixed wireless access for home internet access. 5G also connects large numbers of sensors and machines, known as the Internet of Things (IoT), and uses edge computing to process data closer to where it is generated.",0,Wikipedia,5G,https://en.wikipedia.org/wiki/5G,,5G,wikipedia_api
human_wiki_0361,"Building 5G networks requires new infrastructure and access to suitable radio spectrum. Network operators report high costs and continue to improve energy efficiency and security. 5G also enables massive connections for sensor and machines,known as the internet of Things,(loT), leveraged edge computing the process. It differs among countries depending on income, geography, and national policy.   Analysts expect 5G to support telehealth, smart transport, and digital media, while operating alongside 4G networks into the 2030s.",0,Wikipedia,5G,https://en.wikipedia.org/wiki/5G,,5G,wikipedia_api
human_wiki_0362,"History Early research (2008–2012) In 2008, NASA and the Machine-to-Machine Intelligence Corporation (M2Mi) conducted nanosatellite communication studies that influenced early next-generation network concepts.   In 2012, New York University established NYU Wireless, a research center focused on millimeter-wave communication. The same year, the University of Surrey founded the 5G Innovation Centre, funded by £35 million from public and industry partners including Huawei and Samsung. Also in 2012, the European Union launched the Mobile and Wireless Communications Enablers for the Twenty-Twenty Information Society (METIS) project to align emerging network research with international standardization.",0,Wikipedia,5G,https://en.wikipedia.org/wiki/5G,,5G,wikipedia_api
human_wiki_0363,"Standardization and early trials (2013–2018) In 2013, the ITU-R Working Party 5D began studies on IMT-2020, later formalized as the 5G standard.   During the same period, major firms such as Samsung Electronics, NTT Docomo, and Huawei conducted early trials. Samsung tested a prototype achieving more than 1 Gbit/s across 2 km using 8 × 8 MIMO antennas. NTT Docomo received a government award at CEATEC for high-speed network development, while Huawei announced a US$600 million program to advance mobile network technology.",0,Wikipedia,5G,https://en.wikipedia.org/wiki/5G,,5G,wikipedia_api
human_wiki_0364,"Commercial rollout (2019–2021) On April 3, 2019, South Korea launched its national network, the first full commercial deployment. Hours later, Verizon began limited service in select U.S. cities. In June 2019, Globe Telecom introduced the Philippines’ first next-generation network, and in December 2019, AT&T launched a consumer service in the United States that expanded nationwide during 2020.   Commercial 5G deployment expanded rapidly through 2020. Beyond public mobile networks, it was also adopted in private industrial and enterprise systems, including operation in unlicensed spectrum (NR-U) and licensed non-public networks (NPNs).   Private 5G networks became important for Industry 4.0 automation and smart manufacturing. Early rollouts used non-standalone (NSA) mode—with 4G cores—before networks transitioned to standalone (SA) mode with dedicated 5G cores. South Korea’s 2019 rollout used equipment from Samsung, Ericsson, and Nokia; LG U Plus also deployed Huawei hardware.   Samsung supplied most of the roughly 86,000 sites, while SK Telecom, KT Corporation, and LG U Plus concentrated coverage in major cities using the 3.5 GHz band under NSA operation. Reported download speeds averaged 200–400 Mbit/s, and subscriptions grew from about 260,000 to 4.7 million during 2019.   Following these early deployments, T-Mobile US launched the first nationwide standalone network in 2020. Ericsson projected that by the mid-2020s, 5G networks would reach about 65 percent of the global population. Major suppliers of 5G radio and core systems included Altiostar, Cisco Systems, Datang Telecom/Fiberhome, Ericsson, Huawei, Nokia, Qualcomm, Samsung, and ZTE. Huawei was estimated to hold about 70 percent of global 5G base stations by 2023.",0,Wikipedia,5G,https://en.wikipedia.org/wiki/5G,,5G,wikipedia_api
human_wiki_0365,"Biodiversity is the variability of life on Earth. It can be measured on various levels, for example, genetic variability, species diversity, ecosystem diversity and phylogenetic diversity. Diversity is not distributed evenly on Earth—it is greater in the tropics as a result of the warm climate and high primary productivity in the region near the equator. Tropical forest ecosystems cover less than one-fifth of Earth's terrestrial area and contain about 50% of the world's species. There are latitudinal gradients in species diversity for both marine and terrestrial taxa. Since life began on Earth, six  major mass extinctions and several minor events have led to large and sudden drops in biodiversity. The Phanerozoic aeon (the last 540 million years) marked a rapid growth in biodiversity via the Cambrian explosion. In this period, the majority of multicellular phyla first appeared. The next 400 million years included repeated, massive biodiversity losses. Those events have been classified as mass extinction events. In the Carboniferous, rainforest collapse may have led to a great loss of plant and animal life. The Permian–Triassic extinction event, 251 million years ago, was the worst; vertebrate recovery took 30 million years. Human activities have led to an ongoing biodiversity loss and an accompanying loss of genetic diversity. This process is often referred to as Holocene extinction, or the sixth mass extinction. For example, it was estimated in 2007 that up to 30% of all species will be extinct by 2050. Destroying habitats for farming is a key reason why biodiversity is decreasing today. Climate change also plays a role. This can be seen for example in the effects of climate change on biomes. This anthropogenic extinction may have started toward the end of the Pleistocene, as some studies suggest that the megafaunal extinction event that took place around the end of the last ice age partly resulted from overhunting.",0,Wikipedia,Biodiversity,https://en.wikipedia.org/wiki/Biodiversity,,Biodiversity,wikipedia_api
human_wiki_0366,"Definitions Biologists most often define biodiversity as the ""totality of genes, species and ecosystems of a region"". An advantage of this definition is that it presents a unified view of the traditional types of biological variety previously identified:",0,Wikipedia,Biodiversity,https://en.wikipedia.org/wiki/Biodiversity,,Biodiversity,wikipedia_api
human_wiki_0367,"taxonomic diversity (usually measured at the species diversity level) ecological diversity (often viewed from the perspective of ecosystem diversity) morphological diversity (which stems from genetic diversity and molecular diversity) functional diversity (which is a measure of the number of functionally disparate species within a population (e.g. different feeding mechanisms, different motility, predator vs prey, etc.)) Biodiversity is most commonly used to replace the more clearly-defined and long-established terms, species diversity and species richness. However, there is no concrete definition for biodiversity, as its definition continues to be reimagined and redefined. To give a couple of examples, the Food and Agriculture Organization of the United Nations (FAO) defined biodiversity in 2019 as ""the variability that exists among living organisms (both within and between species) and the ecosystems of which they are part."" The World Health Organization updated its website's definition of biodiversity to be the ""variability among living organisms from all sources."" Both these definitions, although broad, give a current understanding of what is meant by the term biodiversity.",0,Wikipedia,Biodiversity,https://en.wikipedia.org/wiki/Biodiversity,,Biodiversity,wikipedia_api
human_wiki_0368,"Number of species According to estimates by Mora et al. (2011), there are approximately 8.7 million terrestrial species and 2.2 million oceanic species. The authors note that these estimates are strongest for eukaryotic organisms and likely represent the lower bound of prokaryotic diversity. Other estimates include:",0,Wikipedia,Biodiversity,https://en.wikipedia.org/wiki/Biodiversity,,Biodiversity,wikipedia_api
human_wiki_0369,"220,000 vascular plants, estimated using the species-area relation method 0.7–1 million marine species 10–30 million insects; (of some 0.9 million we know today) 5–10 million bacteria; 1.5-3 million fungi, estimates based on data from the tropics, long-term non-tropical sites and molecular studies that have revealed cryptic speciation. Some 0.075 million species of fungi had been documented by 2001; 1 million mites The number of microbial species is not reliably known, but the Global Ocean Sampling Expedition dramatically increased the estimates of genetic diversity by identifying an enormous number of new genes from near-surface plankton samples at various marine locations, initially over the 2004–2006 period. The findings may eventually cause a significant change in the way science defines species and other taxonomic categories. Since the rate of extinction has increased, many extant species may become extinct before they are described. Not surprisingly, in the Animalia the most studied groups are birds and mammals, whereas fishes and arthropods are the least studied animal groups.",0,Wikipedia,Biodiversity,https://en.wikipedia.org/wiki/Biodiversity,,Biodiversity,wikipedia_api
human_wiki_0370,"Deoxyribonucleic acid (; DNA) is a polymer composed of two polynucleotide chains that coil around each other to form a double helix. The polymer carries genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids. Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life. The two DNA strands are known as polynucleotides as they are composed of simpler monomeric units called nucleotides. Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds (known as the phosphodiester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, the single-ringed pyrimidines and the double-ringed purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine. Both strands of double-stranded DNA store the same biological information. This information is replicated when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (or bases). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U). Under the genetic code, these RNA strands specify the sequence of amino acids within proteins in a process called translation. Within eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus as nuclear DNA, and some in the mitochondria as mitochondrial DNA or in chloroplasts as chloroplast DNA. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm, in circular chromosomes. Within eukaryotic chromosomes, chromatin proteins, such as histones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.",0,Wikipedia,DNA,https://en.wikipedia.org/wiki/DNA,,DNA,wikipedia_api
human_wiki_0371,"Properties DNA is a long polymer made from repeating units called nucleotides. The structure of DNA is dynamic along its length, being capable of coiling into tight loops and other shapes. In all species it is composed of two helical chains, bound to each other by hydrogen bonds. Both chains are coiled around the same axis, and have the same pitch of 34 ångströms (3.4 nm). The pair of chains have a radius of 10 Å (1.0 nm). According to another study, when measured in a different solution, the DNA chain measured 22–26 Å (2.2–2.6 nm) wide, and one nucleotide unit measured 3.3 Å (0.33 nm) long. The buoyant density of most DNA is 1.7g/cm3. DNA does not usually exist as a single strand, but instead as a pair of strands that are held tightly together. These two long strands coil around each other, in the shape of a double helix. The nucleotide contains both a segment of the backbone of the molecule (which holds the chain together) and a nucleobase (which interacts with the other DNA strand in the helix). A nucleobase linked to a sugar is called a nucleoside, and a base linked to a sugar and to one or more phosphate groups is called a nucleotide. A biopolymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide. The backbone of the DNA strand is made from alternating phosphate and sugar groups. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings. These are known as the 3′-end (three prime end), and 5′-end (five prime end) carbons, the prime symbol being used to distinguish these carbon atoms from those of the base to which the deoxyribose forms a glycosidic bond. Therefore, any DNA strand normally has one end at which there is a phosphate group attached to the 5′ carbon of a ribose (the 5′ phosphoryl) and another end at which there is a free hydroxyl group attached to the 3′ carbon of a ribose (the 3′ hydroxyl). The orientation of the 3′ and 5′ carbons along the sugar-phosphate backbone confers directionality (sometimes called polarity) to each DNA strand. In a nucleic acid double helix, the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are antiparallel. The asymmetric ends of DNA strands are said to have a directionality of five prime end (5′ ), and three prime end (3′), with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the related pentose sugar ribose in RNA.",0,Wikipedia,DNA,https://en.wikipedia.org/wiki/DNA,,DNA,wikipedia_api
human_wiki_0372,"The DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. The four bases found in DNA are adenine (A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar-phosphate to form the complete nucleotide, as shown for adenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine, forming A-T and G-C base pairs.",0,Wikipedia,DNA,https://en.wikipedia.org/wiki/DNA,,DNA,wikipedia_api
human_wiki_0373,"Nucleobase classification The nucleobases are classified into two types: the purines, A and G, which are fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA, many artificial nucleic acid analogues have been created to study the properties of nucleic acids, or for use in biotechnology.",0,Wikipedia,DNA,https://en.wikipedia.org/wiki/DNA,,DNA,wikipedia_api
human_wiki_0374,"Non-canonical bases Modified bases occur in DNA. The first of these recognized was 5-methylcytosine, which was found in the genome of Mycobacterium tuberculosis in 1925. The reason for the presence of these noncanonical bases in bacterial viruses (bacteriophages) is to avoid the restriction enzymes present in bacteria. This enzyme system acts at least in part as a molecular immune system protecting bacteria from infection by viruses. Modifications of the bases cytosine and adenine, the more common and modified DNA bases, play vital roles in the epigenetic control of gene expression in plants and animals. A number of noncanonical bases are known to occur in DNA. Most of these are modifications of the canonical bases plus uracil.",0,Wikipedia,DNA,https://en.wikipedia.org/wiki/DNA,,DNA,wikipedia_api
human_wiki_0375,"Energy (from Ancient Greek  ἐνέργεια (enérgeia) 'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity—the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J). Forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, the internal energy contained within a thermodynamic system, and rest energy associated with an object's rest mass. These are not mutually exclusive. All living organisms constantly take in and release energy. The Earth's climate and ecosystems processes are driven primarily by radiant energy from the Sun.",0,Wikipedia,Energy,https://en.wikipedia.org/wiki/Energy,,Energy,wikipedia_api
human_wiki_0376,"Forms The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object – or the composite motion of the object's components – while potential energy reflects the potential of an object to have motion, generally being based upon the object's position within a field or what is stored within the field itself. While these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.",0,Wikipedia,Energy,https://en.wikipedia.org/wiki/Energy,,Energy,wikipedia_api
human_wiki_0377,"History The word energy derives from the Ancient Greek: ἐνέργεια, romanized: energeia, lit. 'activity, operation', which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure. In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two. Writing in the early 18th century, Émilie du Châtelet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's Principia Mathematica, which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called ""energy"". In 1807, Thomas Young was possibly the first to use the term ""energy"" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described ""kinetic energy"" in 1829 in its modern sense, and in 1853, William Rankine coined the term ""potential energy"". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat. These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, Walther Nernst, and others. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time. Albert Einstein's 1905 theory of special relativity showed that rest mass corresponds to an equivalent amount of rest energy. This means that rest mass can be converted to or from equivalent amounts of (non-material) forms of energy, for example, kinetic energy, potential energy, and electromagnetic radiant energy. When this happens, rest mass is not conserved, unlike the total mass or total energy. All forms of energy contribute to the total mass and total energy. Thus, conservation of energy (total, including material or rest energy) and conservation of mass (total, not just rest) are one (equivalent) law. In the 18th century, these had appeared as two seemingly-distinct laws. The first evidence of quantization in atoms was the observation of spectral lines in light from the sun in the early 1800s by Joseph von Fraunhofer and William Hyde Wollaston. The notion of quantized energy levels was proposed in 1913 by Danish physicist Niels Bohr in the Bohr theory of the atom. The modern quantum mechanical theory giving an explanation of these energy levels in terms of the Schrödinger equation was advanced by Erwin Schrödinger and Werner Heisenberg in 1926. Noether's theorem shows that the symmetry of this equation is equivalent to a conservation of probability. At the quantum level, mass-energy interactions are all subject to this principle. During wave function collapse, the conservation of energy does not hold at the local level, although statistically the principle holds on average for sufficiently large numbers of collapses. Conservation of energy does apply during wave function collapse in H. Everett's many-worlds interpretation of quantum mechanics.",0,Wikipedia,Energy,https://en.wikipedia.org/wiki/Energy,,Energy,wikipedia_api
human_wiki_0378,"Units of measure In dimensional analysis, the base units of energy are given by: Work = Force × Distance = M L2 T−2, with the fundamental dimensions of Mass M, Length L, and time T. In the International System of Units (SI), the unit of energy is the joule. It is a derived unit that is equal to the energy expended, or work done, in applying a force of one newton through a distance of one metre. The SI unit of power, defined as energy per unit of time, is the watt, which is one joule per second. Thus, a kilowatt-hour (kWh), which can be realized as the energy delivered by one kilowatt of power for an hour, is equal to 3.6 million joules. The CGS energy unit is the erg and the imperial and US customary unit is the foot-pound. Other energy units such as the electronvolt, food calorie, thermodynamic kilocalorie and BTU are used in specific areas of science and commerce.",0,Wikipedia,Energy,https://en.wikipedia.org/wiki/Energy,,Energy,wikipedia_api
human_wiki_0379,"Scientific use Classical mechanics In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept. Work, a function of energy, is force times distance.",0,Wikipedia,Energy,https://en.wikipedia.org/wiki/Energy,,Energy,wikipedia_api
human_wiki_0380,"Proteins are large biomolecules and macromolecules that comprise one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, providing structure to cells and organisms, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific 3D structure that determines its activity. A linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20–30 residues, are rarely considered to be proteins and are commonly called peptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; but in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by post-translational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Some proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can work together to achieve a particular function, and they often associate to form stable protein complexes. Once formed, proteins only exist for a certain period and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable. Like other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyse biochemical reactions and are vital to metabolism. Some proteins have structural or mechanical functions, such as actin and myosin in muscle, and the cytoskeleton's scaffolding proteins that maintain cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. In animals, proteins are needed in the diet to provide the essential amino acids that cannot be synthesized. Digestion breaks the proteins down for metabolic use.",0,Wikipedia,Protein,https://en.wikipedia.org/wiki/Protein,,Protein,wikipedia_api
human_wiki_0381,"History and etymology Discovery and early studies Proteins have been studied and recognized since the 1700s by Antoine Fourcroy and others, who often collectively called them ""albumins"", or ""albuminous materials"" (Eiweisskörper, in German). Gluten, for example, was first separated from wheat in published research around 1747, and later determined to exist in many plants. In 1789, Antoine Fourcroy recognized three distinct varieties of animal proteins: albumin, fibrin, and gelatin. Vegetable (plant) proteins studied in the late 1700s and early 1800s included gluten, plant albumin, gliadin, and legumin. Proteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist Jöns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, C400H620N100O120P1S1. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term ""protein"" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word πρώτειος (proteios), meaning ""primary"", ""in the lead"", or ""standing in front"", + -in. Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of 131 Da. Early nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that ""flesh makes flesh"". Around 1862, Karl Heinrich Ritthausen isolated the amino acid glutamic acid. Thomas Burr Osborne compiled a detailed review of the vegetable proteins at the Connecticut Agricultural Experiment Station. Osborne, alongside Lafayette Mendel, established several nutritionally essential amino acids in feeding experiments with laboratory rats. Diets lacking an essential amino acid stunts the rats' growth, consistent with Liebig's law of the minimum. The final essential amino acid to be discovered, threonine, was identified by William Cumming Rose. The difficulty in purifying proteins impeded work by early protein biochemists. Proteins could be obtained in large quantities from blood, egg whites, and keratin, but individual proteins were unavailable.  In the 1950s, the Armour Hot Dog Company purified 1 kg of bovine pancreatic ribonuclease A and made it freely available to scientists.  This gesture helped ribonuclease A become a major target for biochemical study for the following decades.",0,Wikipedia,Protein,https://en.wikipedia.org/wiki/Protein,,Protein,wikipedia_api
human_wiki_0382,"Polypeptides The understanding of proteins as polypeptides, or chains of amino acids, came through the work of Franz Hofmeister and Hermann Emil Fischer in 1902. The central role of proteins as enzymes in living organisms that catalyzed reactions was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein. Linus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstrøm-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions. The first protein to have its amino acid chain sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958. Christian Anfinsen's studies of the oxidative folding process of ribonuclease A, for which he won the nobel prize in 1972, solidified the thermodynamic hypothesis of protein folding, according to which the folded form of a protein represents its free energy minimum.",0,Wikipedia,Protein,https://en.wikipedia.org/wiki/Protein,,Protein,wikipedia_api
human_wiki_0383,"Structure With the development of X-ray crystallography, it became possible to determine protein structures as well as their sequences. The first protein structures to be solved were hemoglobin by Max Perutz and myoglobin by John Kendrew, in 1958. The use of computers and increasing computing power has supported the sequencing of complex proteins. In 1999, Roger Kornberg sequenced the highly complex structure of RNA polymerase using high intensity X-rays from synchrotrons. Since then, cryo-electron microscopy (cryo-EM) of large macromolecular assemblies has been developed. Cryo-EM uses protein samples that are frozen rather than crystals, and beams of electrons rather than X-rays. It causes less damage to the sample, allowing scientists to obtain more information and analyze larger structures. Computational protein structure prediction of small protein structural domains has helped researchers to approach atomic-level resolution of protein structures. As of April 2024, the Protein Data Bank contains 181,018 X-ray, 19,809 EM and 12,697 NMR protein structures.",0,Wikipedia,Protein,https://en.wikipedia.org/wiki/Protein,,Protein,wikipedia_api
human_wiki_0384,"Classification Proteins are primarily classified by sequence and structure, although other classifications are commonly used. Especially for enzymes the EC number system provides a functional classification scheme. Similarly, gene ontology classifies both genes and proteins by their biological and biochemical function, and by their intracellular location. Sequence similarity is used to classify proteins both in terms of evolutionary and functional similarity. This may use either whole proteins or protein domains, especially in multi-domain proteins. Protein domains allow protein classification by a combination of sequence, structure and function, and they can be combined in many ways. In an early study of 170,000 proteins, about two-thirds were assigned at least one domain, with larger proteins containing more domains (e.g. proteins larger than 600 amino acids having an average of more than 5 domains).",0,Wikipedia,Protein,https://en.wikipedia.org/wiki/Protein,,Protein,wikipedia_api
human_wiki_0385,"Educational technology, or e-learning E-learning (theory) List of online educational resources Distance education Online school Online learning in higher education Online tutoring Massive open online courses Online machine learning, in computer science and statistics",0,Wikipedia,Online learning,https://en.wikipedia.org/wiki/Online_learning,,Online_learning,wikipedia_api
human_wiki_0386,"An antibiotic is a type of antimicrobial substance active against bacteria. It is the most important type of antibacterial agent for fighting bacterial infections, and antibiotic medications are widely used in the treatment and prevention of such infections. They may either kill or inhibit the growth of bacteria. A limited number of antibiotics also possess antiprotozoal activity. Antibiotics are not effective against viruses such as the ones which cause the common cold or influenza. Drugs which inhibit growth of viruses are termed antiviral drugs or antivirals. Antibiotics are also not effective against fungi. Drugs which inhibit growth of fungi are called antifungal drugs. Sometimes, the term antibiotic—literally ""opposing life"", from the Greek roots ἀντι anti, ""against"" and βίος bios, ""life""—is broadly used to refer to any substance used against microbes, but in the usual medical usage, antibiotics (such as penicillin) are those produced naturally (by one microorganism fighting another), whereas non-antibiotic antibacterials (such as sulfonamides and antiseptics) are fully synthetic. However, both classes have the same effect of killing or preventing the growth of microorganisms, and both are included in antimicrobial chemotherapy. ""Antibacterials"" include bactericides, bacteriostatics, antibacterial soaps, and chemical disinfectants, whereas antibiotics are an important class of antibacterials used more specifically in medicine and sometimes in livestock feed.",0,Wikipedia,Antibiotic,https://en.wikipedia.org/wiki/Antibiotic,,Antibiotic,wikipedia_api
human_wiki_0387,"Early history The earliest use of antibiotics was found in northern Sudan, where ancient Sudanese societies as early as 350–550 CE were systematically consuming antibiotics as part of their diet. Chemical analyses of Nubian skeletons show consistent, high levels of tetracycline, a powerful antibiotic. Researchers believe they were brewing beverages from grain fermented with Streptomyces, a bacterium that naturally produces tetracycline. This intentional routine use of antibiotics marks a foundational moment in medical history.",0,Wikipedia,Antibiotic,https://en.wikipedia.org/wiki/Antibiotic,,Antibiotic,wikipedia_api
human_wiki_0388,"Given the amount of tetracycline there, they had to know what they were doing. In other ancient civilizations, including Egypt, China, Serbia, Greece, and Rome, later evidence shows topical application of moldy bread to treat infections. The first person to directly document the use of molds to treat infections was John Parkinson (1567–1650). Antibiotics revolutionized medicine in the 20th century. Synthetic antibiotic chemotherapy as a science and the development of antibacterials began in Germany with Paul Ehrlich in the late 1880s. Alexander Fleming (1881–1955) discovered modern-day penicillin in 1928, the widespread use of which proved significantly beneficial during wartime. The first sulfonamide and the first systemically active antibacterial drug, Prontosil, was developed by a research team led by Gerhard Domagk in 1932 or 1933 at the Bayer Laboratories of the IG Farben conglomerate in Germany. However, the effectiveness and easy access to antibiotics have also led to their overuse and some bacteria have evolved resistance to them. Antimicrobial resistance (AMR), a naturally occurring process, is primarily driven by the misuse and overuse of antimicrobials. Yet, at the same time, many people around the world do not have access to essential antimicrobials. The World Health Organization has classified AMR as a widespread ""serious threat [that] is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country"". Each year, nearly 5 million deaths are associated with AMR globally. Global deaths attributable to AMR numbered 1.27 million in 2019.",0,Wikipedia,Antibiotic,https://en.wikipedia.org/wiki/Antibiotic,,Antibiotic,wikipedia_api
human_wiki_0389,"Etymology The term 'antibiosis', meaning ""against life"", was introduced by the French bacteriologist Jean Paul Vuillemin as a descriptive name of the phenomenon exhibited by these early antibacterial drugs. Antibiosis was first described in 1877 in bacteria when Louis Pasteur and Robert Koch observed that an airborne bacillus could inhibit the growth of Bacillus anthracis. These drugs were later renamed antibiotics by Selman Waksman, an American microbiologist, in 1947. The term antibiotic was first used in 1942 by Selman Waksman and his collaborators in journal articles to describe any substance produced by a microorganism that is antagonistic to the growth of other microorganisms in high dilution. This definition excluded substances that kill bacteria but that are not produced by microorganisms (such as gastric juices and hydrogen peroxide). It also excluded synthetic antibacterial compounds such as the sulfonamides. In current usage, the term ""antibiotic"" is applied to any medication that kills bacteria or inhibits their growth, regardless of whether that medication is produced by a microorganism or not. The term ""antibiotic"" derives from anti + βιωτικός (biōtikos), ""fit for life, lively"", which comes from βίωσις (biōsis), ""way of life"", and that from βίος (bios), ""life"". The term ""antibacterial"" derives from Greek ἀντί (anti), ""against"" + βακτήριον (baktērion), diminutive of βακτηρία (baktēria), ""staff, cane"", because the first bacteria to be discovered were rod-shaped.",0,Wikipedia,Antibiotic,https://en.wikipedia.org/wiki/Antibiotic,,Antibiotic,wikipedia_api
human_wiki_0390,"Usage Medical uses Antibiotics are used to treat or prevent bacterial infections, and sometimes protozoan infections. (Metronidazole is effective against a number of parasitic diseases). When an infection is suspected of being responsible for an illness but the responsible pathogen has not been identified, an empiric therapy is adopted. This involves the administration of a broad-spectrum antibiotic based on the signs and symptoms presented and is initiated pending laboratory results that can take several days. When the responsible pathogenic microorganism is already known or has been identified, definitive therapy can be started. This will usually involve the use of a narrow-spectrum antibiotic. The choice of antibiotic given will also be based on its cost. Identification is critically important as it can reduce the cost and toxicity of the antibiotic therapy and also reduce the possibility of the emergence of antimicrobial resistance. To avoid surgery, antibiotics may be given for non-complicated acute appendicitis. Antibiotics may be given as a preventive measure and this is usually limited to at-risk populations such as those with a weakened immune system (particularly in HIV cases to prevent pneumonia), those taking immunosuppressive drugs, cancer patients, and those having surgery. Their use in surgical procedures is to help prevent infection of incisions. They have an important role in dental antibiotic prophylaxis where their use may prevent bacteremia and consequent infective endocarditis. Antibiotics are also used to prevent infection in cases of neutropenia particularly cancer-related. The use of antibiotics for secondary prevention of coronary heart disease is not supported by current scientific evidence, and may actually increase cardiovascular mortality, all-cause mortality and the occurrence of stroke.",0,Wikipedia,Antibiotic,https://en.wikipedia.org/wiki/Antibiotic,,Antibiotic,wikipedia_api
human_wiki_0391,"Military and naval uses Artillery battery, an organized group of artillery pieces Main battery, the primary weapons of a warship Secondary battery (artillery), the smaller guns on a warship Battery, a ""ready-to-fire"" position of a cartridge in a firearm action",0,Wikipedia,Battery,https://en.wikipedia.org/wiki/Battery,,Battery,wikipedia_api
human_wiki_0392,"Arts and entertainment Music Battery (electro-industrial band) Battery (hardcore punk band) ""Battery"" (song), a song by Metallica from the 1986 album Master of Puppets Drums, which have historically been grouped into ensembles called a battery Drumline, the marching percussion section of a marching ensemble Percussion section, of an orchestra or wind ensemble Battery, a software music sampler by Native Instruments",0,Wikipedia,Battery,https://en.wikipedia.org/wiki/Battery,,Battery,wikipedia_api
human_wiki_0393,"Other uses in arts and entertainment Battery (chess), a formation where two pieces on the same file, rank or diagonal (usually rooks and queens) attack the same square Battery (novel series), by Atsuko Asano Battery Records (disambiguation), the name of several record labels The Battery (disambiguation)#Films, the name of two films",0,Wikipedia,Battery,https://en.wikipedia.org/wiki/Battery,,Battery,wikipedia_api
human_wiki_0394,"Places The Battery (Manhattan), or Battery Park, New York, U.S. The Battery (disambiguation), the name of several places Battery Island, Tasmania, Australia Battery Park (disambiguation), the name of several places Battery Point, Tasmania, Australia",0,Wikipedia,Battery,https://en.wikipedia.org/wiki/Battery,,Battery,wikipedia_api
human_wiki_0395,"Other uses Battery (baseball), the pitcher and catcher collectively Battery, or stamp mill,  a type of mill machine that crushes material by pounding rather than grinding Battery (drink), a brand of energy drinks in Finland Battery Ventures, an investment firm",0,Wikipedia,Battery,https://en.wikipedia.org/wiki/Battery,,Battery,wikipedia_api
human_wiki_0396,"The ocean is the body of salt water that covers approximately 70.8% of Earth. The ocean is conventionally divided into large bodies of water, which are also referred to as oceans (in descending order: the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Antarctic/Southern Ocean, and the Arctic Ocean),  and are themselves mostly divided into seas, gulfs and subsequent bodies of water. The ocean contains 97% of Earth's water and is the primary component of Earth's hydrosphere, acting as a huge reservoir of heat for Earth's energy budget, as well as for its carbon cycle and water cycle, forming the basis for climate and weather patterns worldwide. The ocean is essential to life on Earth, harbouring most of Earth's animals and protist life, originating photosynthesis and therefore Earth's atmospheric oxygen, still supplying half of it. Ocean scientists split the ocean into vertical and horizontal zones based on physical and biological conditions. Horizontally the ocean covers the oceanic crust, which it shapes. Where the ocean meets dry land it covers relatively shallow continental shelfs, which are part of Earth's continental crust. Human activity is mostly coastal with high negative impacts on marine life. Vertically the pelagic zone is the open ocean's water column from the surface to the ocean floor. The water column is further divided into zones based on depth and the amount of light present. The photic zone starts at the surface and is defined to be ""the depth at which light intensity is only 1% of the surface value"" (approximately 200 m in the open ocean). This is the zone where photosynthesis can occur. In this process plants and microscopic algae (free-floating phytoplankton) use light, water, carbon dioxide, and nutrients to produce organic matter. As a result, the photic zone is the most biodiverse and the source of the food supply which sustains most of the ocean ecosystem. Light can only penetrate a few hundred more meters; the rest of the deeper ocean is cold and dark (these zones are called mesopelagic and aphotic zones). Ocean temperatures depend on the amount of solar radiation reaching the ocean surface. In the tropics, surface temperatures can rise to over 30 °C (86 °F). Near the poles where sea ice forms, the temperature in equilibrium is about −2 °C (28 °F). In all parts of the ocean, deep ocean temperatures range between −2 °C (28 °F) and 5 °C (41 °F). Constant circulation of water in the ocean creates ocean currents. Those currents are caused by forces operating on the water, such as temperature and salinity differences, atmospheric circulation (wind), and the Coriolis effect. Tides create tidal currents, while wind and waves cause surface currents. The Gulf Stream, Kuroshio Current, Agulhas Current and Antarctic Circumpolar Current are all major ocean currents. Such currents transport massive amounts of water, gases, pollutants and heat to different parts of the world, and from the surface into the deep ocean.  All this has impacts on the global climate system. Ocean water contains dissolved gases, including oxygen, carbon dioxide and nitrogen. An exchange of these gases occurs at the ocean's surface. The solubility of these gases depends on the temperature and salinity of the water. The carbon dioxide concentration in the atmosphere is rising due to CO2 emissions, mainly from fossil fuel combustion. As the oceans absorb CO2 from the atmosphere, a higher concentration leads to ocean acidification (a drop in pH value). The ocean provides many benefits to humans such as ecosystem services, access to seafood and other marine resources, and a means of transport. The ocean is known to be the habitat of over 230,000 species, but may hold considerably more – perhaps over two million species. Yet, the ocean faces many environmental threats, such as marine pollution, overfishing, and the effects of climate change. Those effects include ocean warming, ocean acidification and sea level rise. The continental shelf and coastal waters are most affected by human activity.",0,Wikipedia,Ocean,https://en.wikipedia.org/wiki/Ocean,,Ocean,wikipedia_api
human_wiki_0397,"Terminology Ocean and sea The terms ""the ocean"" or ""the sea"" used without specification refer to the interconnected body of salt water covering the majority of Earth's surface, i.e., the world ocean. It includes the Pacific, Atlantic, Indian, Antarctic/Southern, and Arctic oceans. As a general term, ""the ocean"" and ""the sea"" are often interchangeable. Strictly speaking, a ""sea"" is a body of water (generally a division of the world ocean) partly or fully enclosed by land. The word ""sea"" can also be used for many specific, much smaller bodies of seawater, such as the North Sea or the Red Sea. There is no sharp distinction between seas and oceans, though generally seas are smaller, and are often partly (as marginal seas) or wholly (as inland seas) bordered by land.",0,Wikipedia,Ocean,https://en.wikipedia.org/wiki/Ocean,,Ocean,wikipedia_api
human_wiki_0398,"World Ocean The contemporary concept of the World Ocean was coined in the early 20th century by the Russian oceanographer Yuly Shokalsky to refer to the continuous ocean that covers and encircles most of Earth. The global, interconnected body of salt water is sometimes referred to as the World Ocean, the global ocean or the great ocean. The concept of a continuous body of water with relatively unrestricted exchange between its components is critical in oceanography.",0,Wikipedia,Ocean,https://en.wikipedia.org/wiki/Ocean,,Ocean,wikipedia_api
human_wiki_0399,"Etymology The word ocean comes from the figure in classical antiquity, Oceanus (; Ancient Greek: Ὠκεανός Ōkeanós, pronounced [ɔːkeanós]), the elder of the Titans in classical Greek mythology. Oceanus was believed by the ancient Greeks and Romans to be the divine personification of an enormous river encircling the world. The concept of Ōkeanós could have an Indo-European connection. Greek Ōkeanós has been compared to the Vedic epithet ā-śáyāna-, predicated of the dragon Vṛtra-, who captured the cows/rivers. Related to this notion, the Okeanos is represented with a dragon-tail on some early Greek vases.",0,Wikipedia,Ocean,https://en.wikipedia.org/wiki/Ocean,,Ocean,wikipedia_api
human_wiki_0400,"Natural history Origin of water Scientists believe that a sizable quantity of water would have been in the material that formed Earth. Water molecules would have escaped Earth's gravity more easily when it was less massive during its formation. This is called atmospheric escape. During planetary formation, Earth possibly had magma oceans. Subsequently, outgassing, volcanic activity and meteorite impacts, produced an early atmosphere of carbon dioxide, nitrogen and water vapor, according to current theories. The gases and the atmosphere are thought to have accumulated over millions of years. After Earth's surface had significantly cooled, the water vapor over time would have condensed, forming Earth's first oceans. The early oceans might have been significantly hotter than today and appeared green due to high iron content. Geological evidence helps constrain the time frame for liquid water existing on Earth. A sample of pillow basalt (a type of rock formed during an underwater eruption) was recovered from the Isua Greenstone Belt and provides evidence that water existed on Earth 3.8 billion years ago. In the Nuvvuagittuq Greenstone Belt, Quebec, Canada, rocks dated at 3.8 billion years old by one study and 4.28 billion years old by another show evidence of the presence of water at these ages. If oceans existed earlier than this, any geological evidence either has yet to be discovered, or has since been destroyed by geological processes like crustal recycling. However, in August 2020, researchers reported that sufficient water to fill the oceans may have always been on the Earth since the beginning of the planet's formation. In this model, atmospheric greenhouse gases kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity.",0,Wikipedia,Ocean,https://en.wikipedia.org/wiki/Ocean,,Ocean,wikipedia_api
human_wiki_0401,"A cryptocurrency (colloquially crypto) is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. However, a type of cryptocurrency called a stablecoin may rely upon government action or legislation to require that a stable value be upheld and maintained.",0,Wikipedia,Cryptocurrency,https://en.wikipedia.org/wiki/Cryptocurrency,,Cryptocurrency,wikipedia_api
human_wiki_0402,"Individual coin ownership records are stored in a digital ledger or blockchain, which is a computerized database that uses a consensus mechanism to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. The two most common consensus mechanisms are proof of work and proof of stake. Despite the name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdictions, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. The first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion. As of April 2025, the cryptocurrency market capitalization was already estimated at $2.76 trillion.",0,Wikipedia,Cryptocurrency,https://en.wikipedia.org/wiki/Cryptocurrency,,Cryptocurrency,wikipedia_api
human_wiki_0403,"History In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash. Later, in 1995, he implemented it through Digicash, an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party. In 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list (October 1996) and later (April 1997) in The American Law Review. In 1998, Wei Dai described ""b-money,"" an anonymous, distributed electronic cash system. Shortly thereafter, Nick Szabo described bit gold. Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published. In January 2009, bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme. In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released, which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake. Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013–2014/15, 2017–2018, and 2021–2023. In August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered. Its final report was published in 2018, and it issued a consultation on cryptoassets and stablecoins in January 2021. In June 2021, El Salvador became the first country to accept bitcoin as legal tender, after the Legislative Assembly had voted 62–22 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such. In August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as bitcoin. In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China. In September 2022, the world's second largest cryptocurrency at that time, Ethereum, transitioned its consensus mechanism from proof-of-work (PoW) to proof-of-stake (PoS) in an upgrade process known as ""the Merge"".  According to the Ethereum Founder, the upgrade would cut both Ethereum's energy use and carbon-dioxide emissions by 99.9%. On 11 November 2022, FTX Trading Ltd., a cryptocurrency exchange, which also operated a crypto hedge fund, and had been valued at $18 billion, filed for bankruptcy. The financial impact of the collapse extended beyond the immediate FTX customer base, as reported, while, at a Reuters conference, financial industry executives said that ""regulators must step in to protect crypto investors."" Technology analyst Avivah Litan commented on the cryptocurrency ecosystem that ""everything...needs to improve dramatically in terms of user experience, controls, safety, customer service."" An October 2024 survey from the Pew Research Center found that 63% of adults in the U.S. ""have little to no confidence that current ways to invest in, trade or use cryptocurrencies are reliable and safe."" The survey also reported that 17% of U.S. adults had directly interacted with cryptocurrency, which was statistically unchanged from 2021.",0,Wikipedia,Cryptocurrency,https://en.wikipedia.org/wiki/Cryptocurrency,,Cryptocurrency,wikipedia_api
human_wiki_0404,"The system does not require a central authority; its state is maintained through distributed consensus. The system keeps an overview of cryptocurrency units and their ownership. The system defines whether new cryptocurrency units can be created. If new cryptocurrency units can be created, the system defines the circumstances of their origin and how to determine the ownership of these new units. Ownership of cryptocurrency units can be proved exclusively cryptographically. The system allows transactions to be performed in which ownership of the cryptographic units is changed. A transaction statement can only be issued by an entity proving the current ownership of these units. If two different instructions for changing the ownership of the same cryptographic units are simultaneously entered, the system performs at most one of them. In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.",0,Wikipedia,Cryptocurrency,https://en.wikipedia.org/wiki/Cryptocurrency,,Cryptocurrency,wikipedia_api
human_wiki_0405,"Altcoins After the early innovation of bitcoin in 2008 and the early network effect gained by bitcoin, tokens, cryptocurrencies, and other digital assets that were not bitcoin became collectively known during the 2010s as alternative cryptocurrencies, or ""altcoins"". Sometimes the term ""alt coins"" was used, or disparagingly, ""shitcoins"". Paul Vigna of The Wall Street Journal described altcoins in 2020 as ""alternative versions of Bitcoin"" given its role as the model protocol for cryptocurrency designers. A Polytechnic University of Catalonia thesis in 2021 used a broader description, including not only alternative versions of bitcoin but every cryptocurrency other than bitcoin. As of early 2020, there were more than 5,000 cryptocurrencies.",0,Wikipedia,Cryptocurrency,https://en.wikipedia.org/wiki/Cryptocurrency,,Cryptocurrency,wikipedia_api
human_wiki_0406,"Renewable energy (also called green energy) is energy made from renewable natural resources that are replenished on a human timescale. The most widely used renewable energy types are solar energy, wind power, and hydropower. Bioenergy and geothermal power are also significant in some countries. Some also consider nuclear power a renewable power source, although this is controversial, as nuclear energy requires mining uranium, a nonrenewable resource. Renewable energy installations can be large or small and are suited for both urban and rural areas. Renewable energy is often deployed together with further electrification. This has several benefits: electricity can move heat and vehicles efficiently and is clean at the point of consumption. Variable renewable energy sources are those that have a fluctuating nature, such as wind power and solar power. In contrast, controllable renewable energy sources include dammed hydroelectricity, bioenergy, or geothermal power.",0,Wikipedia,Renewable energy,https://en.wikipedia.org/wiki/Renewable_energy,,Renewable_energy,wikipedia_api
human_wiki_0407,"Renewable energy systems have rapidly become more efficient and cheaper over the past 30 years. A large majority of worldwide newly installed worldwide electricity capacity is now renewable. Renewable energy sources, such as solar and wind power, have seen significant cost reductions over the past decade, making them more competitive with traditional fossil fuels. In some geographic localities, photovoltaic solar or onshore wind is the cheapest new-build electricity. From 2011 to 2021, renewable energy grew from 20% to 28% of the global electricity supply. Power from the sun and wind accounted for most of this increase, growing from a combined 2% to 10%. Use of fossil energy shrank from 68% to 62%. In 2024, renewables accounted for over 30% of global electricity generation and are projected to reach over 45% by 2030. Many countries already have renewables contributing more than 20% of their total energy supply, with some generating over half or even all their electricity from renewable sources. The main motivation to use renewable energy instead of fossil fuels is to slow and eventually stop climate change, which is mostly caused by their greenhouse gas emissions. In general, renewable energy sources pollute much less than fossil fuels. The International Energy Agency estimates that to achieve net zero emissions by 2050, 90% of global electricity will need to be generated by renewables. Renewables also cause much less air pollution than fossil fuels, improving public health, and are less noisy.",0,Wikipedia,Renewable energy,https://en.wikipedia.org/wiki/Renewable_energy,,Renewable_energy,wikipedia_api
human_wiki_0408,"The deployment of renewable energy still faces obstacles, especially fossil fuel subsidies, lobbying by incumbent power providers, and local opposition to the use of land for renewable installations. Like all mining, the extraction of minerals required for many renewable energy technologies also results in environmental damage. In addition, although most renewable energy sources are sustainable, some are not.",0,Wikipedia,Renewable energy,https://en.wikipedia.org/wiki/Renewable_energy,,Renewable_energy,wikipedia_api
human_wiki_0409,"Overview Definition Renewable energy is usually understood as energy harnessed from continuously occurring natural phenomena. The International Energy Agency defines it as ""energy derived from natural processes that are replenished at a faster rate than they are consumed"". Solar power, wind power, hydroelectricity, geothermal energy, and biomass are widely agreed to be the main types of renewable energy. Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services. Although almost all forms of renewable energy cause much fewer carbon emissions than fossil fuels, the term is not synonymous with low-carbon energy. Some non-renewable sources of energy, such as nuclear power,generate almost no emissions, while some renewable energy sources can be very carbon-intensive, such as the burning of biomass if it is not offset by planting new plants. Renewable energy is also distinct from sustainable energy, a more abstract concept that seeks to group energy sources based on their overall permanent impact on future generations of humans. For example, biomass is often associated with unsustainable deforestation.",0,Wikipedia,Renewable energy,https://en.wikipedia.org/wiki/Renewable_energy,,Renewable_energy,wikipedia_api
human_wiki_0410,"Role in addressing climate change As part of the global effort to limit climate change, most countries have committed to net zero greenhouse gas emissions. In practice, this means phasing out fossil fuels and replacing them with low-emissions energy sources. This much needed process, coined as ""low-carbon substitutions"" in contrast to other transition processes including energy additions, needs to be accelerated multiple times in order to successfully mitigate climate change. At the 2023 United Nations Climate Change Conference, around three-quarters of the world's countries set a goal of tripling renewable energy capacity by 2030. The European Union aims to generate 40% of its electricity from renewables by the same year.",0,Wikipedia,Renewable energy,https://en.wikipedia.org/wiki/Renewable_energy,,Renewable_energy,wikipedia_api
human_wiki_0411,"Political science is the social scientific study of politics. It deals with systems of governance and power, and the analysis of political activities, political thought, political behavior, and associated constitutions and laws. Specialists in the field are political scientists.",0,Wikipedia,Political science,https://en.wikipedia.org/wiki/Political_science,,Political_science,wikipedia_api
human_wiki_0412,"History Origin Political science is a social science dealing with systems of governance and power, and the analysis of political activities, political institutions, political thought and behavior, and associated constitutions and laws. As a social science, contemporary political science started to take shape in the latter half of the 19th century and began to separate itself from political philosophy and history. Into the late 19th century, it was still uncommon for political science to be considered a distinct field from history. The term ""political science"" was not always distinguished from political philosophy, and the modern discipline has a clear set of antecedents including moral philosophy, political economy, political theology, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal state. Generally, while classical political philosophy is primarily defined by a concern for Hellenic and Enlightenment thought, political scientists are also marked by a great concern for ""modernity"" and the contemporary nation state, along with the study of classical thought, and as such share more terminology with sociologists (e.g., structure and agency).",0,Wikipedia,Political science,https://en.wikipedia.org/wiki/Political_science,,Political_science,wikipedia_api
human_wiki_0413,"The advent of political science as a university discipline was marked by the creation of university departments and chairs with the title of political science arising in the late 19th century. The designation ""political scientist"" is commonly used to denote someone with a doctorate or master's degree in the field. Integrating political studies of the past into a unified discipline is ongoing, and the history of political science has provided a rich field for the growth of both normative and positive political science, with each part of the discipline sharing some historical predecessors. The American Political Science Association and the American Political Science Review were founded in 1903 and 1906, respectively, in an effort to distinguish the study of politics from economics and other social phenomena. APSA membership rose from 204 in 1904 to 1,462 in 1915. APSA members played a key role in setting up political science departments that were distinct from history, philosophy, law, sociology, and economics.The journal Political Science Quarterly was established in 1886 by the Academy of Political Science. In the inaugural issue of Political Science Quarterly, Munroe Smith defined political science as ""the science of the state. Taken in this sense, it includes the organization and functions of the state, and the relation of states one to another."" As part of a United Nations Educational, Scientific and Cultural Organization (UNESCO) initiative to promote political science in the late 1940s, the International Political Science Association was founded in 1949, as well as national associations in France in 1949, Britain in 1950, and West Germany in 1951. Founded in 1903, the American Political Science Association (APSA) is the leading professional organization for the study of political science and serves more than 11,000 members in more than 100 countries.",0,Wikipedia,Political science,https://en.wikipedia.org/wiki/Political_science,,Political_science,wikipedia_api
human_wiki_0414,"Behavioral revolution and new institutionalism In the 1950s and the 1960s, a behavioral revolution stressing the systematic and rigorously scientific study of individual and group behavior swept the discipline. A focus on studying political behavior, rather than institutions or interpretation of legal texts, characterized early behavioral political science, including work by Robert Dahl, Philip Converse, and in the collaboration between sociologist Paul Lazarsfeld and public opinion scholar Bernard Berelson. The late 1960s and early 1970s witnessed a takeoff in the use of deductive, game-theoretic formal modelling techniques aimed at generating a more analytical corpus of knowledge in the discipline. This period saw a surge of research that borrowed theory and methods from economics to study political institutions, such as the United States Congress, as well as political behavior, such as voting. William H. Riker and his colleagues and students at the University of Rochester were the main proponents of this shift. Despite considerable research progress in the discipline based on all types of scholarship discussed above, scholars have noted that progress toward systematic theory has been modest and uneven.",0,Wikipedia,Political science,https://en.wikipedia.org/wiki/Political_science,,Political_science,wikipedia_api
human_wiki_0415,"21st century In 2000, the Perestroika Movement in political science was introduced as a reaction against what supporters of the movement called the mathematicization of political science. Those who identified with the movement argued for a plurality of methodologies and approaches in political science and for more relevance of the discipline to those outside of it. Some evolutionary psychology theories argue that humans have evolved a highly developed set of psychological mechanisms for dealing with politics. However, these mechanisms evolved for dealing with the small group politics that characterized the ancestral environment and not the much larger political structures in today's world. This is argued to explain many important features and systematic cognitive biases of current politics.",0,Wikipedia,Political science,https://en.wikipedia.org/wiki/Political_science,,Political_science,wikipedia_api
human_wiki_0416,"Architecture is the art and technique of designing and building, as distinguished from the skills associated with construction. It is both the process and the product of sketching, conceiving, planning, designing, and constructing buildings or other structures. The term comes from Latin  architectura; from Ancient Greek  ἀρχιτέκτων (arkhitéktōn) 'architect'; from  ἀρχι- (arkhi-) 'chief' and  τέκτων (téktōn) 'creator'. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements. The practice, which began in the prehistoric era, has been used as a way of expressing culture by civilizations on all seven continents. For this reason, architecture is considered to be a form of art. Texts on architecture have been written since ancient times. The earliest surviving text on architectural theories is the 1st century BC treatise De architectura by the Roman architect Vitruvius, according to whom a good building embodies firmitas, utilitas, and venustas (durability, utility, and beauty). Centuries later, Leon Battista Alberti developed his ideas further, seeing beauty as an objective quality of buildings to be found in their proportions. In the 19th century, Louis Sullivan declared that ""form follows function"". ""Function"" began to replace the classical ""utility"" and was understood to include not only practical but also aesthetic, psychological, and cultural dimensions. The idea of sustainable architecture was introduced in the late 20th century. Architecture began as rural, oral vernacular architecture that developed from trial and error to successful replication. Ancient urban architecture was preoccupied with building religious structures and buildings symbolizing the political power of rulers until Greek and Roman architecture shifted focus to civic virtues. Indian and Chinese architecture influenced forms all over Asia and Buddhist architecture in particular took diverse local flavors. During the Middle Ages, pan-European styles of Romanesque and Gothic cathedrals and abbeys emerged while the Renaissance favored Classical forms implemented by architects known by name. Later, the roles of architects and engineers became separated. Modern architecture began after World War I as an avant-garde movement that sought to develop a completely new style appropriate for a new post-war social and economic order focused on meeting the needs of the middle and working classes. Emphasis was put on modern techniques, materials, and simplified geometric forms, paving the way for high-rise superstructures. Many architects became disillusioned with modernism which they perceived as ahistorical and anti-aesthetic, and postmodern and contemporary architecture developed. Over the years, the field of architectural construction has branched out to include everything from ship design to interior decorating.",0,Wikipedia,Architecture,https://en.wikipedia.org/wiki/Architecture,,Architecture,wikipedia_api
human_wiki_0417,"A general term to describe buildings and other physical structures. The art and science of designing buildings and (some) nonbuilding structures; sometimes called ""architectonics."" The style of design and method of construction of buildings and other physical structures. A unifying or coherent form or structure. The knowledge of art, science, technology, and humanity. The design activity of the architect, from the macro-level (urban design, landscape architecture) to the micro-level (construction details and furniture). The practice of the architect where architecture means offering or rendering professional services in connection with the design and construction of buildings or built environments.",0,Wikipedia,Architecture,https://en.wikipedia.org/wiki/Architecture,,Architecture,wikipedia_api
human_wiki_0418,"Theory The philosophy of architecture is a branch of the philosophy of art, dealing with aesthetic value of architecture, its semantics and its relation to the development of culture. Many philosophers and theoreticians from Plato to Michel Foucault, Gilles Deleuze, Robert Venturi and Ludwig Wittgenstein have concerned themselves with the nature of architecture and whether or not architecture is distinguished from building.",0,Wikipedia,Architecture,https://en.wikipedia.org/wiki/Architecture,,Architecture,wikipedia_api
human_wiki_0419,"Historic treatises The earliest surviving written work on the subject of architecture is De architectura by the Roman architect Vitruvius in the early 1st century BC. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be:",0,Wikipedia,Architecture,https://en.wikipedia.org/wiki/Architecture,,Architecture,wikipedia_api
human_wiki_0420,"Durability – a building should stand up robustly and remain in good condition Utility – it should be suitable for the purposes for which it is used Beauty – it should be aesthetically pleasing According to Vitruvius, the architect should strive to fulfill each of these three attributes as well as possible. Leon Battista Alberti, who elaborates on the ideas of Vitruvius in his treatise, De re aedificatoria, saw beauty primarily as a matter of proportion, although ornament also played a part. For Alberti, the rules of proportion were those that governed the idealized human figure, the golden mean. The most important aspect of beauty was, therefore, an inherent part of an object, rather than something applied superficially, and was based on universal, recognizable truths. The notion of style in the arts was not developed until the 16th century, with the writing of Giorgio Vasari. By the 18th century, his Lives of the Most Excellent Painters, Sculptors, and Architects had been translated into Italian, French, Spanish, and English. In the 16th century, Italian Mannerist architect, painter and theorist Sebastiano Serlio wrote Tutte L'Opere D'Architettura et Prospetiva (Complete Works on Architecture and Perspective). This treatise exerted immense influence throughout Europe, being the first handbook that emphasized the practical rather than the theoretical aspects of architecture, and it was the first to catalog the five orders. In the early 19th century, Augustus Welby Northmore Pugin wrote Contrasts (1836) that, as the title suggested, contrasted the modern, industrial world, which he disparaged, with an idealized image of neo-medieval world. Gothic architecture, Pugin believed, was the only ""true Christian form of architecture."" The 19th-century English art critic, John Ruskin, in his Seven Lamps of Architecture, published 1849, was much narrower in his view of what constituted architecture. Architecture was the ""art which so disposes and adorns the edifices raised by men ... that the sight of them"" contributes ""to his mental health, power, and pleasure"". For Ruskin, the aesthetic was of overriding significance. His work goes on to state that a building is not truly a work of architecture unless it is in some way ""adorned"". For Ruskin, a well-constructed, well-proportioned, functional building needed string courses or rustication, at the very least. On the difference between the ideals of architecture and mere construction, the 20th-century architect Le Corbusier wrote: ""You employ stone, wood, and concrete, and with these materials you build houses and palaces: that is construction. Ingenuity is at work. But suddenly you touch my heart, you do me good. I am happy and I say: This is beautiful. That is Architecture"". Le Corbusier's contemporary Ludwig Mies van der Rohe is said to have stated in a 1959 interview that ""architecture starts when you carefully put two bricks together. There it begins.""",0,Wikipedia,Architecture,https://en.wikipedia.org/wiki/Architecture,,Architecture,wikipedia_api
human_wiki_0421,"Manufacturing is the creation or production of goods with the help of equipment, labor, machines, tools, and chemical or biological processing or formulation. It is the essence of the secondary sector of the economy. The term may refer to a range of human activity, from handicraft to high-tech, but it is most commonly applied to industrial design, in which raw materials from the primary sector are transformed into finished goods on a large scale. Such goods may be sold to other manufacturers for the production of other more complex products (such as aircraft, household appliances, furniture, sports equipment or automobiles), or distributed via the tertiary industry to end users and consumers (usually through wholesalers, who in turn sell to retailers, who then sell them to individual customers).",0,Wikipedia,Manufacturing,https://en.wikipedia.org/wiki/Manufacturing,,Manufacturing,wikipedia_api
human_wiki_0422,"Manufacturing engineering is the field of engineering that designs and optimizes the manufacturing process, or the steps through which raw materials are transformed into a final product. The manufacturing process begins with product design, and materials specification. These materials are then modified through manufacturing to become the desired product. Contemporary manufacturing encompasses all intermediary stages involved in producing and integrating components of a product. Some industries, such as semiconductor and steel manufacturers, use the term fabrication instead. The manufacturing sector is closely connected with the engineering and industrial design industries.",0,Wikipedia,Manufacturing,https://en.wikipedia.org/wiki/Manufacturing,,Manufacturing,wikipedia_api
human_wiki_0423,"Etymology The Modern English word manufacture is likely derived from the Middle French manufacture (""process of making"") which itself originates from the Classical Latin manū (""hand"") and the Middle French facture (""making""). Alternatively, the English word may have been independently formed from the earlier English manufacture (""made by human hands"") and fracture. Its earliest usage in the English language was recorded in the mid-16th century to refer to the making of products by hand.",0,Wikipedia,Manufacturing,https://en.wikipedia.org/wiki/Manufacturing,,Manufacturing,wikipedia_api
human_wiki_0424,"History and development Prehistory and ancient history Human ancestors manufactured objects using stone and other tools long before the emergence of Homo sapiens about 200,000 years ago. The earliest methods of stone tool making, known as the Oldowan ""industry"", date back to at least 2.3 million years ago, with the earliest direct evidence of tool usage found in Ethiopia within the Great Rift Valley, dating back to 2.5 million years ago. To manufacture a stone tool, a ""core"" of hard stone with specific flaking properties (such as flint) was struck with a hammerstone. This flaking produced sharp edges that could be used as tools, primarily in the form of choppers or scrapers. These tools greatly aided the early humans in their hunter-gatherer lifestyle to form other tools out of softer materials such as bone and wood. The Middle Paleolithic, approximately 300,000 years ago, saw the introduction of the prepared-core technique, where multiple blades could be rapidly formed from a single core stone. Pressure flaking, in which a wood, bone, or antler punch could be used to shape a stone very finely was developed during the Upper Paleolithic, beginning approximately 40,000 years ago. During the Neolithic period, polished stone tools were manufactured from a variety of hard rocks such as flint, jade, jadeite, and greenstone. The polished axes were used alongside other stone tools including projectiles, knives, and scrapers, as well as tools manufactured from organic materials such as wood, bone, and antler. Copper smelting is believed to have originated when the technology of pottery kiln allowed sufficiently high temperatures. The concentration of various elements such as arsenic increases with depth in copper ore deposits and smelting of these ores yields arsenical bronze, which can be sufficiently work-hardened to be suitable for manufacturing tools. Bronze is an alloy of copper with tin; the latter of which being found in relatively few deposits globally delayed true tin bronze from becoming widespread. During the Bronze Age, bronze was a major improvement over stone as a material for making tools, both because of its mechanical properties like strength and ductility and because it could be cast in molds to make intricately shaped objects. Bronze significantly advanced shipbuilding technology with better tools and bronze nails, which replaced the old method of attaching boards of the hull with cord woven through drilled holes. The Iron Age is conventionally defined by the widespread manufacturing of weapons and tools using iron and steel rather than bronze. Iron smelting is more difficult than tin and copper smelting because smelted iron requires hot-working and can be melted only in specially designed furnaces. The place and time for the discovery of iron smelting is not known, partly because of the difficulty of distinguishing metal extracted from nickel-containing ores from hot-worked meteoritic iron. During the growth of the ancient civilizations, many ancient technologies resulted from advances in manufacturing. Several of the six classic simple machines were invented in Mesopotamia. The Mesopotamians have been credited with the invention of the wheel. The wheel and axle mechanism first appeared with the potter's wheel, invented in Mesopotamia (modern Iraq) during the 5th millennium BC. Egyptian paper made from papyrus, as well as pottery, were mass-produced and exported throughout the Mediterranean basin. Early construction techniques used by the Ancient Egyptians made use of bricks composed mainly of clay, sand, silt, and other minerals.",0,Wikipedia,Manufacturing,https://en.wikipedia.org/wiki/Manufacturing,,Manufacturing,wikipedia_api
human_wiki_0425,"Medieval and early modern The Middle Ages witnessed new inventions, innovations in the ways of managing traditional means of production, and economic growth. Papermaking, a 2nd-century Chinese technology, was carried to the Middle East when a group of Chinese papermakers were captured in the 8th century. Papermaking technology was spread to Europe by the Umayyad conquest of Hispania. A paper mill was established in Sicily in the 12th century. In Europe the fiber to make pulp for making paper was obtained from linen and cotton rags. Lynn Townsend White Jr. credited the spinning wheel with increasing the supply of rags, which led to cheap paper, which was a factor in the development of printing. Due to the casting of cannon, the blast furnace came into widespread use in France in the mid 15th century. The blast furnace had been used in China since the 4th century BC. The stocking frame, which was invented in 1598, increased a knitter's number of knots per minute from 100 to 1000.",0,Wikipedia,Manufacturing,https://en.wikipedia.org/wiki/Manufacturing,,Manufacturing,wikipedia_api
human_wiki_0426,"Sustainability (from the latin sustinere - hold up, hold upright; furnish with means of support; bear, undergo, endure) is the ability to continue over a long period of time. In modern usage it generally refers to a state in which the environment, economy, and society will continue to exist over a long period of time. Many definitions emphasize the environmental dimension. This can include addressing key environmental problems, such as climate change and biodiversity loss. The idea of sustainability can guide decisions at the global, national, organizational, and individual levels. A related concept is that of sustainable development, and the terms are often used to mean the same thing. UNESCO distinguishes the two like this: ""Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it."" Details around the economic dimension of sustainability are controversial. Scholars have discussed this under the concept of weak and strong sustainability. For example, there will always be tension between the ideas of ""welfare and prosperity for all"" and environmental conservation, so trade-offs are necessary. It would be desirable to find ways that separate economic growth from harming the environment. This means using fewer resources per unit of output even while growing the economy. This decoupling reduces the environmental impact of economic growth, such as pollution. Doing this is difficult.  It is challenging to measure sustainability as the concept is complex, contextual, and dynamic. Indicators have been developed to cover the environment, society, or the economy but there is no fixed definition of sustainability indicators. The metrics are evolving and include indicators, benchmarks, and audits. They include sustainability standards and certification systems, like Fairtrade and Organic. They also involve indices and accounting systems, such as corporate sustainability reporting and triple Bottom Line accounting.  It is necessary to address many barriers to sustainability to achieve a sustainability transition or sustainability transformation. Some barriers arise from nature and its complexity while others are extrinsic to the concept of sustainability. For example, they can result from the dominant institutional frameworks in countries. Global issues of sustainability are difficult to tackle because they need global solutions. Existing global organizations such as the UN and WTO are seen as inefficient in enforcing current global regulations. One reason for this is the lack of suitable sanctioning mechanisms. Governments are not the only sources of action for sustainability. For example, business groups have tried to integrate ecological concerns with economic activity, seeking sustainable business. Religious leaders have stressed the need for caring for nature and environmental stability. Individuals can also choose to live more sustainably. Some people have criticized the idea of sustainability. One point of criticism is that the concept is vague and only a buzzword. Another is that sustainability might be an impossible goal. Some experts have pointed out that ""no country is delivering what its citizens need without transgressing the biophysical planetary boundaries"".",0,Wikipedia,Sustainability,https://en.wikipedia.org/wiki/Sustainability,,Sustainability,wikipedia_api
human_wiki_0427,"Definitions Current usage Sustainability is regarded as a ""normative concept"". This means it is based on what people value or find desirable: ""The quest for sustainability involves connecting what is known through scientific study to applications in pursuit of what people want for the future."" The 1983 UN Commission on Environment and Development (Brundtland Commission) had a big influence on the use of the term sustainability today. The commission's 1987 Brundtland Report provided a definition of sustainable development. The report, Our Common Future, defines it as development that ""meets the needs of the present without compromising the ability of future generations to meet their own needs"". The report helped bring sustainability into the mainstream of policy discussions. It also popularized the concept of sustainable development. Some other key concepts to illustrate the meaning of sustainability include:",0,Wikipedia,Sustainability,https://en.wikipedia.org/wiki/Sustainability,,Sustainability,wikipedia_api
human_wiki_0428,"It may be a fuzzy concept, but in a positive sense: the goals are more important than the approaches or means applied. It connects with other essential concepts, such as resilience, adaptive capacity, and vulnerability. Choices matter: ""it is not possible to sustain everything, everywhere, forever"". Scale matters in both space and time, and place matters. Limits exist (see planetary boundaries). In everyday usage, sustainability often focuses on the environmental dimension.",0,Wikipedia,Sustainability,https://en.wikipedia.org/wiki/Sustainability,,Sustainability,wikipedia_api
human_wiki_0429,"""Sustainability can be defined as the capacity to maintain or improve the state and availability of desirable materials or conditions over the long term."" ""Sustainability [is] the long-term viability of a community, set of social institutions, or societal practice. In general, sustainability is understood as a form of intergenerational ethics in which the environmental and economic actions taken by present persons do not diminish the opportunities of future persons to enjoy similar levels of wealth, utility, or welfare."" ""Sustainability means meeting our own needs without compromising the ability of future generations to meet their own needs. In addition to natural resources, we also need social and economic resources. Sustainability is not just environmentalism. Embedded in most definitions of sustainability we also find concerns for social equity and economic development."" Some definitions focus on the environmental dimension. The Oxford Dictionary of English defines sustainability as: ""the property of being environmentally sustainable; the degree to which a process or enterprise is able to be maintained or continued while avoiding the long-term depletion of natural resources"".",0,Wikipedia,Sustainability,https://en.wikipedia.org/wiki/Sustainability,,Sustainability,wikipedia_api
human_wiki_0430,"Historical usage The term sustainability is derived from the Latin word sustinere. ""To sustain"" can mean to maintain, support, uphold, or endure. So sustainability is the ability to continue over a long period of time. In the past, sustainability referred to environmental sustainability. It meant using natural resources so that people in the future could continue to rely on them in the long term. The concept of sustainability, or Nachhaltigkeit in German, goes back to Hans Carl von Carlowitz (1645–1714), and applied to forestry. The term for this now would be sustainable forest management. He used this term to mean the long-term responsible use of a natural resource. In his 1713 work Silvicultura oeconomica, he wrote that ""the highest art/science/industriousness [...] will consist in such a conservation and replanting of timber that there can be a continuous, ongoing and sustainable use"". The shift in use of ""sustainability"" from preservation of forests (for future wood production) to broader preservation of environmental resources (to sustain the world for future generations) traces to a 1972 book by Ernst Basler, based on a series of lectures at M.I.T.  The idea itself goes back a long time: Communities have always worried about the capacity of their environment to sustain them in the long term. Many ancient cultures, traditional societies, and indigenous peoples have restricted the use of natural resources.",0,Wikipedia,Sustainability,https://en.wikipedia.org/wiki/Sustainability,,Sustainability,wikipedia_api
human_wiki_0431,"A quantum computer is a (real or theoretical) computer that exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its computation. Quantum computers can be viewed as sampling from quantum systems that evolve in ways that may be described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. By contrast, ordinary (""classical"") computers operate according to deterministic rules. (A classical computer can, in principle, be replicated by a classical mechanical device, with only a simple multiple of time cost. On the other hand (it is believed), a quantum computer would require exponentially more time and energy to be simulated classically.) It is widely believed that a quantum computer could perform some calculations exponentially faster than any classical computer. For example, a large-scale quantum computer could break some widely used public-key cryptographic schemes and aid physicists in performing physical simulations. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks. The basic unit of information in quantum computing, the qubit (or ""quantum bit""), serves the same function as the bit in ordinary or ""classical"" computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a linear combination of two states known as a quantum superposition. The result of measuring a qubit is one of the two states given by a probabilistic rule. If a quantum computer manipulates the qubit in a particular way, wave interference effects amplify the probability of the desired measurement result. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform this amplification. Quantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields). Researchers have claimed, and are widely believed to be correct, that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage or quantum supremacy. These tasks are not necessarily useful for real-world applications.",0,Wikipedia,Quantum computing,https://en.wikipedia.org/wiki/Quantum_computing,,Quantum_computing,wikipedia_api
human_wiki_0432,"History For many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory was developed in the 1920s to explain perplexing physical phenomena observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for nuclear physics used in the Manhattan Project. As physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer. When digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation. In a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security. Quantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein–Vazirani algorithm in 1993, and Simon's algorithm in 1994. These algorithms did not solve practical problems, but demonstrated mathematically that one could obtain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.",0,Wikipedia,Quantum computing,https://en.wikipedia.org/wiki/Quantum_computing,,Quantum_computing,wikipedia_api
human_wiki_0433,"Peter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie–Hellman encryption protocols, which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture. Over the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors. In 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates. In 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that any classical computer would find impossible. This announcement was met with a rebuttal from IBM, which contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its  Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for ""quantum supremacy"".",0,Wikipedia,Quantum computing,https://en.wikipedia.org/wiki/Quantum_computing,,Quantum_computing,wikipedia_api
human_wiki_0434,"Quantum information processing Computer engineers typically describe a modern computer's operation in terms of classical electrodynamics. In these ""classical"" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior; however, because they are not isolated from their environment, any quantum information eventually quickly decoheres. While programmers may depend on probability theory when designing a randomized algorithm, quantum-mechanical notions such as superposition and wave interference are largely irrelevant in program analysis. Quantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice. As physicist Charlie Bennett describes the relationship between quantum and classical computers,",0,Wikipedia,Quantum computing,https://en.wikipedia.org/wiki/Quantum_computing,,Quantum_computing,wikipedia_api
human_wiki_0435,"A classical computer is a quantum computer ... so we shouldn't be asking about ""where do quantum speedups come from?"" We should say, ""Well, all computers are quantum. ... Where do classical slowdowns come from?""",0,Wikipedia,Quantum computing,https://en.wikipedia.org/wiki/Quantum_computing,,Quantum_computing,wikipedia_api
human_wiki_0436,"Science, technology, engineering, and mathematics (STEM) is an umbrella term used to group together the related technical disciplines of science, technology, engineering, and mathematics. It represents a broad and interconnected set of fields that are crucial for innovation and technological advancement. These disciplines are often grouped together because they share a common emphasis on critical thinking, problem-solving, and analytical skills. The term is typically used in the context of education policy or curriculum choices in schools. It has implications for workforce development, national security concerns (as a shortage of STEM-educated citizens can reduce effectiveness in this area), and immigration policy, with regard to admitting foreign students and tech workers. There is no universal agreement on which disciplines are included in STEM; in particular, whether or not the science in STEM includes social sciences, such as psychology, sociology, economics, and political science. In the United States, these are typically included by the National Science Foundation (NSF), the Department of Labor's O*Net online database for job seekers, and the Department of Homeland Security. In the United Kingdom, the social sciences are categorized separately and are instead grouped with humanities and arts to form another counterpart acronym HASS (humanities, arts, and social sciences), rebranded in 2020 as SHAPE (social sciences, humanities and the arts for people and the economy). Some sources also use HEAL (health, education, administration, and literacy) as the counterpart of STEM.",0,Wikipedia,STEM,"https://en.wikipedia.org/wiki/Science,_technology,_engineering,_and_mathematics",,STEM,wikipedia_api
human_wiki_0437,"Terminology History In the early 1990s the acronym STEM was used by a variety of educators. Beverly P. Schwartz developed a STEM mentoring program in the Capital District of New York State, and was using the acronym as early as November, 1991. Charles E. Vela was the founder and director of the Center for the Advancement of Hispanics in Science and Engineering Education (CAHSEE) and started a summer program for talented under-represented students in the Washington, D.C. area called the STEM Institute. Based on the program's recognized success and his expertise in STEM education, Charles Vela was asked to serve on numerous NSF and Congressional panels in science, mathematics, and engineering education. Previously referred to as SMET by the NSF, it is through this manner that NSF was first introduced to the acronym STEM. One of the first NSF projects to use the acronym was STEMTEC, the Science, Technology, Engineering, and Math Teacher Education Collaborative at the University of Massachusetts Amherst, which was founded in 1998. In 2001, at the urging of Dr. Peter Faletra, the Director of Workforce Development for Teachers and Scientists at the Office of Science, the acronym was adopted by Rita Colwell and other science administrators in the National Science Foundation (NSF). The Office of Science was also an early adopter of the STEM acronym.",0,Wikipedia,STEM,"https://en.wikipedia.org/wiki/Science,_technology,_engineering,_and_mathematics",,STEM,wikipedia_api
human_wiki_0438,"Other variations eSTEM (environmental STEM) GEMS (girls in engineering, math, and science); used for programs to encourage women to enter these fields. MINT (mathematics, informatics, natural sciences, and technology) SHTEAM (science, humanities, technology, engineering, arts, and mathematics) SMET (science, mathematics, engineering, and technology); previous name STEAM (science, technology, engineering, arts, and mathematics) STEAM (science, technology, engineering, agriculture, and mathematics); add agriculture STEAM (science, technology, engineering, and applied mathematics); has more focus on applied mathematics STEEM (science, technology, engineering, economics, and mathematics); adds economics as a field STEMIE (science, technology, engineering, mathematics, invention, and entrepreneurship); adds inventing and entrepreneurship as a means to apply STEM to real-world problem-solving and markets. STEMM (science, technology, engineering, mathematics, and medicine) STM (scientific, technical, and mathematics or science, technology, and medicine) STREAM (science, technology, robotics, engineering, arts, and mathematics); adds robotics and arts as fields STREAM (science, technology, reading, engineering, arts, and mathematics); adds reading and arts STREAM (science, technology, recreation, engineering, arts, and mathematics); adds recreation and arts",0,Wikipedia,STEM,"https://en.wikipedia.org/wiki/Science,_technology,_engineering,_and_mathematics",,STEM,wikipedia_api
human_wiki_0439,"By country Australia The Australian Curriculum, Assessment, and Reporting Authority 2015 report entitled, National STEM School Education Strategy, stated that ""A renewed national focus on STEM in school education is critical to ensuring that all young Australians are equipped with the necessary STEM skills and knowledge that they must need to succeed."" Its goals were to:",0,Wikipedia,STEM,"https://en.wikipedia.org/wiki/Science,_technology,_engineering,_and_mathematics",,STEM,wikipedia_api
human_wiki_0440,"""Ensure all students finish school with strong foundational knowledge in STEM and related skills"" ""Ensure that students are inspired to take on more challenging STEM subjects"" Events and programs meant to help develop STEM in Australian schools include the Victorian Model Solar Vehicle Challenge, the Maths Challenge (Australian Mathematics Trust), Go Girl Go Global and the Australian Informatics Olympiad.",0,Wikipedia,STEM,"https://en.wikipedia.org/wiki/Science,_technology,_engineering,_and_mathematics",,STEM,wikipedia_api
human_wiki_0441,"Philosophy (from Ancient Greek philosophía lit. 'love of wisdom') is a systematic study of general and fundamental questions concerning topics like existence, knowledge, mind, reason, language, and value. It is a rational and critical inquiry that reflects on its methods and assumptions. Historically, many of the individual sciences, such as physics and psychology, formed part of philosophy. However, they are considered separate academic disciplines in the modern sense of the term. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Western philosophy originated in Ancient Greece and covers a wide area of philosophical subfields. A central topic in Arabic–Persian philosophy is the relation between reason and revelation. Indian philosophy combines the spiritual problem of how to reach enlightenment with the exploration of the nature of reality and the ways of arriving at knowledge. Chinese philosophy focuses principally on practical issues about right social conduct, government, and self-cultivation. Major branches of philosophy are epistemology, ethics, logic, and metaphysics. Epistemology studies what knowledge is and how to acquire it. Ethics investigates moral principles and what constitutes right conduct. Logic is the study of correct reasoning and explores how good arguments can be distinguished from bad ones. Metaphysics examines the most general features of reality, existence, objects, and properties. Other subfields are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, philosophy of mathematics, philosophy of history, and political philosophy. Within each branch, there are competing schools of philosophy that promote different principles, theories, or methods. Philosophers use a great variety of methods to arrive at philosophical knowledge. They include conceptual analysis, reliance on common sense and intuitions, use of thought experiments, analysis of ordinary language, description of experience, and critical questioning. Philosophy is related to many other fields, such as the natural and social sciences, mathematics, business, law, and journalism. It provides an interdisciplinary perspective and studies the scope and fundamental concepts of these fields. It also investigates their methods and ethical implications.",0,Wikipedia,Philosophy,https://en.wikipedia.org/wiki/Philosophy,,Philosophy,wikipedia_api
human_wiki_0442,"Etymology The word philosophy comes from the Ancient Greek words φίλος (philos) 'love' and σοφία (sophia) 'wisdom'. Some sources say that the term was coined by the pre-Socratic philosopher Pythagoras, but this is not certain.",0,Wikipedia,Philosophy,https://en.wikipedia.org/wiki/Philosophy,,Philosophy,wikipedia_api
human_wiki_0443,"The word entered the English language primarily from Old French and Anglo-Norman starting around 1175 CE. The French philosophie is itself a borrowing from the Latin philosophia. The term philosophy acquired the meanings of ""advanced study of the speculative subjects (logic, ethics, physics, and metaphysics)"", ""deep wisdom consisting of love of truth and virtuous living"", ""profound learning as transmitted by the ancient writers"", and ""the study of the fundamental nature of knowledge, reality, and existence, and the basic limits of human understanding"". Before the modern age, the term philosophy was used in a wide sense. It included most forms of rational inquiry, such as the individual sciences, as its subdisciplines. For instance, natural philosophy was a major branch of philosophy. This branch of philosophy encompassed a wide range of fields, including disciplines like physics, chemistry, and biology. An example of this usage is the 1687 book Philosophiæ Naturalis Principia Mathematica by Isaac Newton. This book referred to natural philosophy in its title, but it is today considered a book of physics. The meaning of philosophy changed toward the end of the modern period when it acquired the more narrow meaning common today. In this new sense, the term is mainly associated with disciplines like metaphysics, epistemology, and ethics. Among other topics, it covers the rational study of reality, knowledge, and values. It is distinguished from other disciplines of rational inquiry such as the empirical sciences and mathematics.",0,Wikipedia,Philosophy,https://en.wikipedia.org/wiki/Philosophy,,Philosophy,wikipedia_api
human_wiki_0444,"Conceptions of philosophy General conception The practice of philosophy is characterized by several general features: it is a form of rational inquiry, it aims to be systematic, and it tends to critically reflect on its own methods and presuppositions. It requires attentively thinking long and carefully about the provocative, vexing, and enduring problems central to the human condition. The philosophical pursuit of wisdom involves asking general and fundamental questions. It often does not result in straightforward answers but may help a person to better understand the topic, examine their life, dispel confusion, and overcome prejudices and self-deceptive ideas associated with common sense. For example, Socrates stated that ""the unexamined life is not worth living"" to highlight the role of philosophical inquiry in understanding one's own existence. And according to Bertrand Russell, ""the man who has no tincture of philosophy goes through life imprisoned in the prejudices derived from common sense, from the habitual beliefs of his age or his nation, and from convictions which have grown up in his mind without the cooperation or consent of his deliberate reason.""",0,Wikipedia,Philosophy,https://en.wikipedia.org/wiki/Philosophy,,Philosophy,wikipedia_api
human_wiki_0445,"Academic definitions Attempts to provide more precise definitions of philosophy are controversial and are studied in metaphilosophy. Some approaches argue that there is a set of essential features shared by all parts of philosophy. Others see only weaker family resemblances or contend that it is merely an empty blanket term. Precise definitions are often only accepted by theorists belonging to a certain philosophical movement and are revisionistic according to Søren Overgaard et al. in that many presumed parts of philosophy would not deserve the title ""philosophy"" if they were true. Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: ""What can I know?""; ""What should I do?""; ""What may I hope?""; and ""What is the human being?"" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as ""the midwife of the sciences"". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a ""rigorous science"" investigating essences. They practice a radical suspension of theoretical assumptions about reality to get back to the ""things themselves"", that is, as originally given in experience. They contend that this base-level of experience provides the foundation for higher-order theoretical knowledge, and that one needs to understand the former to understand the latter. An early approach found in ancient Greek and Roman philosophy is that philosophy is the spiritual practice of developing one's rational capacities. This practice is an expression of the philosopher's love of wisdom and has the aim of improving one's well-being by leading a reflective life. For example, the Stoics saw philosophy as an exercise to train the mind and thereby achieve eudaimonia and flourish in life.",0,Wikipedia,Philosophy,https://en.wikipedia.org/wiki/Philosophy,,Philosophy,wikipedia_api
human_wiki_0446,"Virtual reality (VR) is a simulated experience that employs 3D near-eye displays and pose tracking to give the user an immersive feel of a virtual world. Applications of virtual reality include entertainment (particularly video games), education (such as medical, safety, or military training), research  and business (such as virtual meetings). VR is one of the key technologies in the reality-virtuality continuum. As such, it is different from other digital visualization solutions, such as augmented virtuality and augmented reality. Currently, standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate some realistic images, sounds, and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes but can also be created through specially designed rooms with multiple large screens. Virtual reality typically incorporates auditory and video feedback but may also allow other types of sensory and force feedback through haptic technology.",0,Wikipedia,Virtual reality,https://en.wikipedia.org/wiki/Virtual_reality,,Virtual_reality,wikipedia_api
human_wiki_0447,"Etymology ""Virtual"" has had the meaning of ""being something in essence or effect, though not actually or in fact"" since the mid-1400s. The term ""virtual"" has been used in the computer sense of ""not physically existing but made to appear by software"" since 1959. In 1938, French avant-garde playwright Antonin Artaud described the illusory nature of characters and objects in the theatre as ""la réalité virtuelle"" in a collection of essays, Le Théâtre et son double. The English translation of this book, published in 1958 as The Theater and its Double, is the earliest published use of the term ""virtual reality"". The term ""artificial reality"", coined by Myron Krueger, has been in use since the 1970s. The term ""virtual reality"" was first used in a science fiction context in The Judas Mandala, a 1982 novel by Damien Broderick. Widespread adoption of the term ""virtual reality"" in the popular media is attributed to Jaron Lanier, who in the late 1980s designed some of the first business-grade virtual reality hardware under his firm VPL Research, and the 1992 film Lawnmower Man, which features use of virtual reality systems.",0,Wikipedia,Virtual reality,https://en.wikipedia.org/wiki/Virtual_reality,,Virtual_reality,wikipedia_api
human_wiki_0448,"Forms and methods One method of realizing virtual reality is through simulation-based virtual reality. For example, driving simulators give the driver the impression of actually driving a vehicle by predicting vehicular motion based on the driver's input and providing corresponding visual, motion, and audio cues. With avatar image-based virtual reality, people can join the virtual environment in the form of real video as well as an avatar. One can participate in the 3D distributed virtual environment in the form of either a conventional avatar or a real video. Users can select their own type of participation based on the system capability. In projector-based virtual reality, modeling of the real environment plays a vital role in various virtual reality applications, including robot navigation, construction modeling, and airplane simulation. Image-based virtual reality systems have been gaining popularity in computer graphics and computer vision communities. In generating realistic models, it is essential to accurately register acquired 3D data; usually, a camera is used for modeling small objects at a short distance. Desktop-based virtual reality involves displaying a 3D virtual world on a regular desktop display without use of any specialized VR positional tracking equipment. Many modern first-person video games can be used as an example, using various triggers, responsive characters, and other such interactive devices to make the user feel as though they are in a virtual world. A common criticism of this form of immersion is that there is no sense of peripheral vision, limiting the user's ability to know what is happening around them.",0,Wikipedia,Virtual reality,https://en.wikipedia.org/wiki/Virtual_reality,,Virtual_reality,wikipedia_api
human_wiki_0449,"A head-mounted display (HMD) more fully immerses the user in a virtual world. A virtual reality headset typically includes two small high resolution OLED or LCD monitors which provide separate images for each eye for stereoscopic graphics rendering a 3D virtual world, a binaural audio system, positional and rotational real-time head tracking for six degrees of movement. Options include motion controls with haptic feedback for physically interacting within the virtual world in an intuitive way with little to no abstraction and an omnidirectional treadmill for more freedom of physical movement allowing the user to perform locomotive motion in any direction. Augmented reality (AR) is a type of virtual reality technology that blends what the user sees in their real surroundings with digital content generated by computer software. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way. AR systems layer virtual information over a camera live feed into a headset or smartglasses or through a mobile device giving the user the ability to view three-dimensional images. Mixed reality (MR) is the merging of the real world and virtual worlds to produce new environments and visualizations where physical and digital objects co-exist and interact in real time. A cyberspace is sometimes defined as a networked virtual reality. Simulated reality is a hypothetical virtual reality as truly immersive as the actual reality, enabling an advanced lifelike experience or even virtual eternity.",0,Wikipedia,Virtual reality,https://en.wikipedia.org/wiki/Virtual_reality,,Virtual_reality,wikipedia_api
human_wiki_0450,History The development of perspective in Renaissance European art and the stereoscope invented by Sir Charles Wheatstone were both precursors to virtual reality. The first references to the more modern-day concept of virtual reality came from science fiction.,0,Wikipedia,Virtual reality,https://en.wikipedia.org/wiki/Virtual_reality,,Virtual_reality,wikipedia_api
human_wiki_0451,"Mental health encompasses emotional, psychological, and social well-being, influencing cognition, perception, and behavior. Mental health plays a crucial role in an individual's daily life when managing stress, engaging with others, and contributing to life overall. According to the World Health Organization (WHO), it is a ""state of well-being in which the individual realizes their abilities, can cope with the normal stresses of life, can work productively and fruitfully, and can contribute to their community"". It likewise determines how an individual handles stress, interpersonal relationships, and decision-making. Mental health includes subjective well-being, perceived self-efficacy, autonomy, competence, intergenerational dependence, and self-actualization of one's intellectual and emotional potential, among others. From the perspectives of positive psychology or holism, mental health is thus not merely the absence of mental illness. Rather, it is a broader state of well-being that includes an individual's ability to enjoy life and to create a balance between life activities and efforts to achieve psychological resilience. Cultural differences, personal philosophy, subjective assessments, and competing professional theories all affect how one defines ""mental health"". Some early signs related to mental health difficulties are sleep irritation, lack of energy, lack of appetite, thinking of harming oneself or others, self-isolating (though introversion and isolation are not necessarily unhealthy), and frequently zoning out.",0,Wikipedia,Mental health,https://en.wikipedia.org/wiki/Mental_health,,Mental_health,wikipedia_api
human_wiki_0452,"Mental disorders Mental health, as defined by the Public Health Agency of Canada, is an individual's capacity to feel, think, and act in ways to achieve a better quality of life while respecting personal, social, and cultural boundaries. Impairment of any of these is a risk factor for mental disorders, or mental illnesses, which is a component of mental health. In 2019, about 970 million people worldwide suffered from a mental disorder, with anxiety and depression being the most common. The number of people suffering from mental disorders has risen significantly over the years. Mental disorders are defined as health conditions that affect and alter cognitive functioning, emotional responses, and behavior associated with distress and/or impaired functioning. The ICD-11 is the global standard used to diagnose, treat, research, and report various mental disorders. In the United States, the DSM-5 is used as the classification system of mental disorders. Mental health is associated with a number of lifestyle factors such as diet, exercise, stress, drug abuse, social connections and interactions. Psychiatrists, psychologists, licensed professional clinical counselors, social workers, nurse practitioners, and family physicians can help manage mental illness with treatments such as therapy, counseling, medication, and Trauma-informed care.",0,Wikipedia,Mental health,https://en.wikipedia.org/wiki/Mental_health,,Mental_health,wikipedia_api
human_wiki_0453,"History Early history In the mid-19th century, William Sweetser was the first to coin the term mental hygiene, which can be seen as the precursor to contemporary approaches to work on promoting positive mental health. Isaac Ray, the fourth president of the American Psychiatric Association and one of its founders, further defined mental hygiene as ""the art of preserving the mind against all incidents and influences calculated to deteriorate its qualities, impair its energies, or derange its movements"". In American history, mentally ill patients were thought to be religiously punished. This response persisted through the 1700s, along with the inhumane confinement and stigmatization of such individuals. Dorothea Dix (1802–1887) was an important figure in the development of the ""mental hygiene"" movement. Dix was a school teacher who endeavored to help people with mental disorders and to expose the sub-standard conditions into which they were put. This became known as the ""mental hygiene movement"". Before this movement, it was not uncommon that people affected by mental illness would be considerably neglected, often left alone in deplorable conditions without sufficient clothing. From 1840 to 1880, she won the support of the federal government to set up over 30 state psychiatric hospitals; however, they were understaffed, under-resourced, and were accused of violating human rights. Emil Kraepelin in 1896 developed the taxonomy of mental disorders which has dominated the field for nearly 80 years. Later, the proposed disease model of abnormality was subjected to analysis and considered normality to be relative to the physical, geographical and cultural aspects of the defining group. At the beginning of the 20th century, Clifford Beers founded ""Mental Health America – National Committee for Mental Hygiene"", after publication of his accounts as a patient in several lunatic asylums, A Mind That Found Itself, in 1908 and opened the first outpatient mental health clinic in the United States. The mental hygiene movement, similar to the social hygiene movement, had at times been associated with advocating eugenics and sterilization of those considered too mentally deficient to be assisted into productive work and contented family life. In the post-WWII years, references to mental hygiene were gradually replaced by the term 'mental health' due to its positive aspect that evolves from the treatment of illness to preventive and promotive areas of healthcare.",0,Wikipedia,Mental health,https://en.wikipedia.org/wiki/Mental_health,,Mental_health,wikipedia_api
human_wiki_0454,"Institutionalization and deinstitutionalization When US government-run hospitals were accused of violating human rights, advocates pushed for deinstitutionalization: the replacement of federal mental hospitals with community mental health services. The closure of state-provisioned psychiatric hospitals was enforced by the Community Mental Health Centers Act in 1963 which laid out terms under which only patients who posed an imminent danger to others or themselves could be admitted into state facilities. This was seen as an improvement from previous conditions. However, there remains a debate on the conditions of these community resources. It has been proven that this transition was beneficial for many patients: there was an increase in overall satisfaction, a better quality of life, and more friendships between patients all at an affordable cost. This proved to be true only in the circumstance that treatment facilities had enough funding for staff and equipment as well as proper management. However, this idea is a polarizing issue. Critics of deinstitutionalization argue that poor living conditions prevailed, patients were lonely, and they did not acquire proper medical care in these treatment homes. Additionally, patients that were moved from state psychiatric care to nursing and residential homes had deficits in crucial aspects of their treatment. Some cases result in the shift of care from health workers to patients' families, where they do not have the proper funding or medical expertise to give proper care. On the other hand, patients that are treated in community mental health centers lack sufficient cancer testing, vaccinations, or otherwise regular medical check-ups. Other critics of state deinstitutionalization argue that this was simply a transition to ""transinstitutionalization"", or the idea that prisons and state-provisioned hospitals are interdependent. In other words, patients become inmates. This draws on the Penrose Hypothesis of 1939, which theorized that there was an inverse relationship between prisons' population size and the number of psychiatric hospital beds. This means that populations that require psychiatric mental care will transition between institutions, which in this case, includes state psychiatric hospitals and criminal justice systems. Thus, a decrease in available psychiatric hospital beds occurred at the same time as an increase in inmates. Although some are skeptical that this is due to other external factors, others will reason this conclusion to a lack of empathy for the mentally ill. There is no argument for the social stigmatization of those with mental illnesses, they have been widely marginalized and discriminated against in society. In this source, researchers analyze how most compensation prisoners (detainees who are unable or unwilling to pay a fine for petty crimes) are unemployed, homeless, and with an extraordinarily high degree of mental illnesses and substance use disorders. Compensation prisoners then lose prospective job opportunities, face social marginalization, and lack access to resocialization programs, which ultimately facilitate reoffending. The research sheds light on how the mentally ill—and in this case, the poor—are further punished for certain circumstances that are beyond their control, and that this is a vicious cycle that repeats itself. Thus, prisons embody another state-provisioned mental hospital. Families of patients, advocates, and mental health professionals still call for increase in more well-structured community facilities and treatment programs with a higher quality of long-term inpatient resources and care. With this more structured environment, the United States will continue with more access to mental health care and an increase in the overall treatment of the mentally ill. However, there is still a lack of studies for mental health conditions (MHCs) to raise awareness, knowledge development, and attitudes toward seeking medical treatment for MHCs in Bangladesh. People in rural areas often seek treatment from the traditional healers and MHCs are sometimes considered a spiritual matter.",0,Wikipedia,Mental health,https://en.wikipedia.org/wiki/Mental_health,,Mental_health,wikipedia_api
human_wiki_0455,"Epidemiology Mental illnesses are more common than cancer, diabetes, or heart disease. As of 2021, over 22 percent of all Americans over the age of 18 meet the criteria for having a mental illness. Evidence suggests that 970 million people worldwide have a mental disorder. Major depression ranks third among the top 10 leading causes of disease worldwide. By 2030, it is predicted to become the leading cause of disease worldwide. Over 700 thousand people commit suicide every year and around 14 million attempt it. A World Health Organization (WHO) report estimates the global cost of mental illness at nearly $2.5 trillion (two-thirds in indirect costs) in 2010, with a projected increase to over $6 trillion by 2030. Evidence from the WHO suggests that nearly half of the world's population is affected by mental illness with an impact on their self-esteem, relationships and ability to function in everyday life. An individual's emotional health can impact their physical health. Poor mental health can lead to problems such as the inability to make adequate decisions and substance use disorders. Good mental health can improve life quality whereas poor mental health can worsen it. According to Richards, Campania, & Muse-Burke, ""There is growing evidence that is showing emotional abilities are associated with pro-social behaviors such as stress management and physical health."" Their research also concluded that people who lack emotional expression are inclined to anti-social behaviors (e.g., substance use disorder and alcohol use disorder, physical fights, vandalism), which reflects one's mental health and suppressed emotions. Adults and children who face mental illness may experience social stigma, which can exacerbate the issues.",0,Wikipedia,Mental health,https://en.wikipedia.org/wiki/Mental_health,,Mental_health,wikipedia_api
human_wiki_0456,"Mathematics is a field of study that discovers and organizes methods, theories, and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics). Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms.  Mathematics uses pure reason to prove the properties of objects through proofs, which consist of a succession of applications of deductive rules to already established results. These results, called theorems, include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration. Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications. Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was primarily divided into geometry and arithmetics (the manipulation of natural numbers and fractions) until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics.",0,Wikipedia,Mathematics,https://en.wikipedia.org/wiki/Mathematics,,Mathematics,wikipedia_api
human_wiki_0457,"Areas of mathematics Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes. Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics. During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics. The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the 17th century. At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics. The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas. Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have ""geometry"" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.",0,Wikipedia,Mathematics,https://en.wikipedia.org/wiki/Mathematics,,Mathematics,wikipedia_api
human_wiki_0458,"Number theory Number theory began with the manipulation of numbers, that is, natural numbers                         (                    N                  )         ,                 {\displaystyle (\mathbb {N} ),}     and later expanded to integers                         (                    Z                  )                 {\displaystyle (\mathbb {Z} )}     and rational numbers                         (                    Q                  )         .                 {\displaystyle (\mathbb {Q} ).}     Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations. Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria. The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss. Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra. Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort. Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), Diophantine analysis, and transcendence theory (problem oriented).",0,Wikipedia,Mathematics,https://en.wikipedia.org/wiki/Mathematics,,Mathematics,wikipedia_api
human_wiki_0459,"Geometry Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields. A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements. The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space. Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically. Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions. In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem. In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space. Today's subareas of geometry include:",0,Wikipedia,Mathematics,https://en.wikipedia.org/wiki/Mathematics,,Mathematics,wikipedia_api
human_wiki_0460,"Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines. Affine geometry, the study of properties relative to parallelism and independent from the concept of length. Differential geometry, the study of curves, surfaces, and their generalizations, which are defined using differentiable functions. Manifold theory, the study of shapes that are not necessarily embedded in a larger space. Riemannian geometry, the study of distance properties in curved spaces. Algebraic geometry, the study of curves, surfaces, and their generalizations, which are defined using polynomials. Topology, the study of properties that are kept under continuous deformations. Algebraic topology, the use in topology of algebraic methods, mainly homological algebra. Discrete geometry, the study of finite configurations in geometry. Convex geometry, the study of convex sets, which takes its importance from its applications in optimization. Complex geometry, the geometry obtained by replacing real numbers with complex numbers.",0,Wikipedia,Mathematics,https://en.wikipedia.org/wiki/Mathematics,,Mathematics,wikipedia_api
human_wiki_0461,"Mechanical engineering is the study of physical machines and mechanisms that may involve force and movement. It is an engineering branch that combines engineering physics and mathematics principles with materials science, to design, analyze, manufacture, and maintain mechanical systems. It is one of the oldest and broadest of the engineering branches. Mechanical engineering requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, design, structural analysis, and electricity. In addition to these core principles, mechanical engineers use tools such as computer-aided design (CAD), computer-aided manufacturing (CAM), computer-aided engineering (CAE), and product lifecycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, motor vehicles, aircraft, watercraft, robotics, medical devices, weapons, and others. Mechanical engineering emerged as a field during the Industrial Revolution in Europe in the 18th century; however, its development can be traced back several thousand years around the world. In the 19th century, developments in physics led to the development of mechanical engineering science. The field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology. It also overlaps with aerospace engineering, metallurgical engineering, civil engineering, structural engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts. Mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modelling of biological systems.",0,Wikipedia,Mechanical engineering,https://en.wikipedia.org/wiki/Mechanical_engineering,,Mechanical_engineering,wikipedia_api
human_wiki_0462,"History The application of mechanical engineering can be seen in the archives of various ancient and medieval societies. The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times. Mesopotamian civilization is credited with the invention of the wheel by several, mainly old sources. However, some recent sources either suggest that it was invented independently in both Mesopotamia and Eastern Europe or credit prehistoric Eastern Europeans with the invention of the wheel The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia circa 3000 BC. The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC. The Saqiyah was developed in the Kingdom of Kush during the 4th century BC. It relied on animal power reducing the tow on the requirement of human energy. Reservoirs in the form of Hafirs were developed in Kush to store water and boost irrigation. Bloomeries and blast furnaces were developed during the seventh century BC in Meroe. Kushite sundials applied mathematics in the form of advanced trigonometry. The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC. In ancient Greece, the works of Archimedes (287–212 BC) influenced mechanics in the Western tradition. The geared Antikythera mechanisms was an Analog computer invented around the 2nd century BC. In Roman Egypt, Heron of Alexandria (c. 10–70 AD) created the first steam-powered device (Aeolipile). In China, Zhang Heng (78–139 AD) improved a water clock and invented a seismometer, and Ma Jun (200–265 AD) invented a chariot with differential gears. The medieval Chinese horologist and engineer Su Song (1020–1101 AD) incorporated an escapement mechanism into his astronomical clock tower two centuries before escapement devices were found in medieval European clocks. He also invented the world's first known endless power-transmitting chain drive. The cotton gin was invented in India by the 6th century AD, and the spinning wheel was invented in the Islamic world by the early 11th century, Dual-roller gins appeared in India and China between the 12th and 14th centuries. The worm gear roller gin appeared in the Indian subcontinent during the early Delhi Sultanate era of the 13th to 14th centuries. During the Islamic Golden Age (7th to 15th century), Muslim inventors made remarkable contributions in the field of mechanical technology. Al-Jazari, who was one of them, wrote his famous Book of Knowledge of Ingenious Mechanical Devices in 1206 and presented many mechanical designs. In the 17th century, important breakthroughs in the foundations of mechanical engineering occurred in England and the Continent. The Dutch mathematician and physicist Christiaan Huygens invented the pendulum clock in 1657, which was the first reliable timekeeper for almost 300 years, and published a work dedicated to clock designs and the theory behind them. In England, Isaac Newton formulated his laws of motion and developed calculus, which would become the mathematical basis of physics. Newton was reluctant to publish his works for years, but he was finally persuaded to do so by his colleagues, such as Edmond Halley. Gottfried Wilhelm Leibniz, who earlier designed a mechanical calculator, is also credited with developing the calculus during the same time period. During the early 19th century Industrial Revolution, machine tools were developed in England, Germany, and Scotland. This allowed mechanical engineering to develop as a separate field within engineering. They brought with them manufacturing machines and the engines to power them. The first British professional society of mechanical engineers was formed in 1847 Institution of Mechanical Engineers, thirty years after the civil engineers formed the first such professional society Institution of Civil Engineers. On the European continent, Johann von Zimmermann (1820–1901) founded the first factory for grinding machines in Chemnitz, Germany in 1848. In the United States, the American Society of Mechanical Engineers (ASME) was formed in 1880, becoming the third such professional engineering society, after the American Society of Civil Engineers (1852) and the American Institute of Mining Engineers (1871). The first schools in the United States to offer an engineering education were the United States Military Academy in 1817, an institution now known as Norwich University in 1819, and Rensselaer Polytechnic Institute in 1825. Education in mechanical engineering has historically been based on a strong foundation in mathematics and science.",0,Wikipedia,Mechanical engineering,https://en.wikipedia.org/wiki/Mechanical_engineering,,Mechanical_engineering,wikipedia_api
human_wiki_0463,"Education Degrees in mechanical engineering are offered at various universities worldwide. Mechanical engineering programs typically take four to five years of study depending on the place and university and result in a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Science Engineering (B.Sc.Eng.), Bachelor of Technology (B.Tech.), Bachelor of Mechanical Engineering (B.M.E.), or Bachelor of Applied Science (B.A.Sc.) degree, in or with emphasis in mechanical engineering. In Spain, Portugal and most of South America, where neither B.S. nor B.Tech. programs have been adopted, the formal name for the degree is ""Mechanical Engineer"", and the course work is based on five or six years of training. In Italy the course work is based on five years of education, and training, but in order to qualify as an Engineer one has to pass a state exam at the end of the course. In Greece, the coursework is based on a five-year curriculum. In the United States, most undergraduate mechanical engineering programs are accredited by the Accreditation Board for Engineering and Technology (ABET) to ensure similar course requirements and standards among universities. The ABET web site lists 302 accredited mechanical engineering programs as of 11 March 2014. Mechanical engineering programs in Canada are accredited by the Canadian Engineering Accreditation Board (CEAB), and most other countries offering engineering degrees have similar accreditation societies. In Australia, mechanical engineering degrees are awarded as Bachelor of Engineering (Mechanical) or similar nomenclature, although there are an increasing number of specialisations. The degree takes four years of full-time study to achieve. To ensure quality in engineering degrees, Engineers Australia accredits engineering degrees awarded by Australian universities in accordance with the global Washington Accord. Before the degree can be awarded, the student must complete at least 3 months of on the job work experience in an engineering firm. Similar systems are also present in South Africa and are overseen by the Engineering Council of South Africa (ECSA). In India, to become an engineer, one needs to have an engineering degree like a B.Tech. or B.E., have a diploma in engineering, or by completing a course in an engineering trade like fitter from the Industrial Training Institute (ITIs) to receive a ""ITI Trade Certificate"" and also pass the All India Trade Test (AITT) with an engineering trade conducted by the National Council of Vocational Training (NCVT) by which one is awarded a ""National Trade Certificate"". A similar system is used in Nepal. Some mechanical engineers go on to pursue a postgraduate degree such as a Master of Engineering, Master of Technology, Master of Science, Master of Engineering Management (M.Eng.Mgt. or M.E.M.), a Doctor of Philosophy in engineering (Eng.D. or Ph.D.) or an engineer's degree. The master's and engineer's degrees may or may not include research. The Doctor of Philosophy includes a significant research component and is often viewed as the entry point to academia. The Engineer's degree exists at a few institutions at an intermediate level between the master's degree and the doctorate.",0,Wikipedia,Mechanical engineering,https://en.wikipedia.org/wiki/Mechanical_engineering,,Mechanical_engineering,wikipedia_api
human_wiki_0464,"Coursework Standards set by each country's accreditation society are intended to provide uniformity in fundamental subject material, promote competence among graduating engineers, and to maintain confidence in the engineering profession as a whole. Engineering programs in the U.S., for example, are required by ABET to show that their students can ""work professionally in both thermal and mechanical systems areas."" The specific courses required to graduate, however, may differ from program to program. Universities and institutes of technology will often combine multiple subjects into a single class or split a subject into multiple classes, depending on the faculty available and the university's major area(s) of research. The fundamental subjects required for mechanical engineering usually include:",0,Wikipedia,Mechanical engineering,https://en.wikipedia.org/wiki/Mechanical_engineering,,Mechanical_engineering,wikipedia_api
human_wiki_0465,"Mathematics (in particular, calculus, differential equations, and linear algebra) Basic physical sciences (including physics and chemistry) Statics and dynamics Strength of materials and solid mechanics Materials engineering, composites Thermodynamics, heat transfer, energy conversion, and HVAC Fuels, combustion, internal combustion engine Fluid mechanics (including fluid statics and fluid dynamics) Mechanism and Machine design (including kinematics and dynamics) Instrumentation and measurement Manufacturing engineering, technology, or processes Vibration, control theory and control engineering Hydraulics and Pneumatics Mechatronics and robotics Engineering design and product design Drafting, computer-aided design (CAD) and computer-aided manufacturing (CAM) Mechanical engineers are also expected to understand and be able to apply basic concepts from chemistry, physics, tribology, chemical engineering, civil engineering, and electrical engineering. All mechanical engineering programs include multiple semesters of mathematical classes including calculus, and advanced mathematical concepts including differential equations, partial differential equations, linear algebra, differential geometry, and statistics, among others. In addition to the core mechanical engineering curriculum, many mechanical engineering programs offer more specialized programs and classes, such as control systems, robotics, transport and logistics, cryogenics, fuel technology, automotive engineering, biomechanics, vibration, optics and others, if a separate department does not exist for these subjects. Most mechanical engineering programs also require varying amounts of research or community projects to gain practical problem-solving experience. In the United States it is common for mechanical engineering students to complete one or more internships while studying, though this is not typically mandated by the university. Cooperative education is another option. Research puts demand on study components that feed student's creativity and innovation.",0,Wikipedia,Mechanical engineering,https://en.wikipedia.org/wiki/Mechanical_engineering,,Mechanical_engineering,wikipedia_api
human_wiki_0466,"The Moon is the only natural satellite of Earth. It orbits around Earth at an average distance of 384,399 kilometres (238,854 mi), a distance roughly 30 times the width of Earth. It completes an orbit (lunar month) in relation to Earth and the Sun (synodically) every 29.5 days. The Moon and Earth are bound by gravitational attraction, which is stronger on their facing sides. The resulting tidal forces are the main driver of Earth's tides, and have pulled the Moon to always face Earth with the same near side. This tidal locking effectively synchronizes the Moon's rotation period (lunar day) to its orbital period (lunar month). In geophysical terms, the Moon is a planetary-mass object or satellite planet. Its mass is 1.2% that of the Earth, and its diameter is 3,474 km (2,159 mi), roughly one-quarter of Earth's (about as wide as the contiguous United States). Within the Solar System, it is larger and more massive than any known dwarf planet, and the fifth-largest and fifth-most massive moon, as well as the largest and most massive in relation to its parent planet. Its surface gravity is about one-sixth of Earth's, about half that of Mars, and the second-highest among all moons in the Solar System after Jupiter's moon Io. The body of the Moon is differentiated and terrestrial, with only a minuscule hydrosphere, atmosphere, and magnetic field. The lunar surface is covered in regolith dust, which mainly consists of the fine material ejected from the lunar crust by impact events. The lunar crust is marked by impact craters, with some younger ones featuring bright ray-like streaks. The Moon was volcanically active until 1.2 billion years ago, surfacing lava mostly on the thinner near side of the Moon, filling ancient craters, which through cooling formed the today prominently visible dark plains of basalt called maria ('seas'). The Moon formed  out of material from Earth, ejected by a giant impact into Earth of a hypothesized Mars-sized body named Theia 4.51 billion years ago, not long after Earth's formation. From a distance, the day and night phases of the lunar day are visible as the lunar phases, and when the Moon passes through Earth's shadow a lunar eclipse is observable. The Moon's apparent size in Earth's sky is about the same as that of the Sun, which causes it to cover the Sun completely during a total solar eclipse. The Moon is the brightest celestial object in Earth's night sky because of its large apparent size, while the reflectance (albedo) of its surface is comparable to that of asphalt. About 59% of the surface of the Moon is visible from Earth owing to the different angles at which the Moon can appear in Earth's sky (libration), making parts of the far side of the Moon visible. The Moon has been an important source of inspiration and knowledge in human history, having been crucial to cosmography, mythology, religion, art, time keeping, natural science and spaceflight. The first spaceflights to an extraterrestrial body were to the Moon, starting in 1959 with the flyby of Luna 1 (sent by the Soviet Union), and the intentional impact of Luna 2. In 1966 followed the first soft landing (by Luna 9) and orbital insertion (by Luna 10). Humans first arrived in orbit on December 24, 1968, with Apollo 8 (sent by the United States), and then on the surface on July 20, 1969, with Apollo 11. By 1972, six Apollo missions had landed twelve humans on the Moon and stayed up to three days. Renewed robotic exploration of the Moon, in particular to confirm the presence of water on the Moon, has fueled plans to return humans to the Moon, starting with the Artemis program scheduled for the late 2020s.",0,Wikipedia,Moon,https://en.wikipedia.org/wiki/Moon,,Moon,wikipedia_api
human_wiki_0467,"Names and etymology The English proper name for Earth's natural satellite is typically written as Moon, with a capital M. The noun moon is derived from Old English mōna, which stems from Proto-Germanic *mēnōn, which in turn comes from Proto-Indo-European *mēnsis ('month') – from earlier *mēnōt (genitive *mēneses), which may be related to a verb meaning 'to measure [time]'. The Latin name for the Moon is lūna. The English adjective lunar was ultimately borrowed from Latin, likely through French. In scientific writing and science fiction, the Moon is sometimes referred to as Luna  to distinguish it from other moons. In poetry, Luna may also refer to the personification of the Moon as a woman. The Ancient Greek word selḗnē referred to the Moon as a celestial body, and also to the moon goddess Selene . The rare English adjective selenian  is used to describe the Moon as a world, as opposed to a celestial object. Its cognate selenic, originally a rare synonym, now almost always refers to the chemical element selenium. The corresponding prefix seleno- appears in terms including selenography (the study of the lunar surface). Artemis, the Greek goddess of the wilderness and the hunt, also came to be identified with Selene, and was sometimes called Cynthia after her birthplace on Mount Cynthus. Her Roman equivalent is Diana. The astronomical symbols for the Moon are the crescent  and decrescent , for example in M☾ 'lunar mass'.",0,Wikipedia,Moon,https://en.wikipedia.org/wiki/Moon,,Moon,wikipedia_api
human_wiki_0468,"Natural history Formation Isotope dating of lunar samples suggests the Moon formed around 50 million years after the origin of the Solar System. Historically, several formation mechanisms have been proposed, but none satisfactorily explains the features of the Earth–Moon system. A fission of the Moon from Earth's crust through centrifugal force would require too great an initial rotation rate of Earth. Gravitational capture of a pre-formed Moon depends on an unfeasibly extended atmosphere of Earth to dissipate the energy of the passing Moon. A co-formation of Earth and the Moon together in the primordial accretion disk does not explain the depletion of metals in the Moon. None of these hypotheses can account for the high angular momentum of the Earth–Moon system. The prevailing theory is that the Earth–Moon system formed after a giant impact of a Mars-sized body (named Theia) with the proto-Earth. The oblique impact blasted material into orbit about the Earth and the material accreted and formed the Moon just beyond the Earth's Roche limit of ~2.56 R🜨. Giant impacts are thought to have been common in the early Solar System. Computer simulations of giant impacts have produced results that are consistent with the mass of the lunar core and the angular momentum of the Earth–Moon system. These simulations show that most of the Moon derived from the impactor, rather than the proto-Earth. However, models from 2007 and later suggest a larger fraction of the Moon derived from the proto-Earth. Other bodies of the inner Solar System such as Mars and Vesta have, according to meteorites from them, very different oxygen and tungsten isotopic compositions compared to Earth. However, Earth and the Moon have nearly identical isotopic compositions. The isotopic equalization of the Earth–Moon system might be explained by the post-impact mixing of the vaporized material that formed the two, although this is debated. The impact would have released enough energy to liquefy both the ejecta and the Earth's crust, forming a magma ocean. The liquefied ejecta could have then re-accreted into the Earth–Moon system. The newly formed Moon would have had its own magma ocean; its depth is estimated from about 500 km (300 miles) to 1,737 km (1,079 miles). While the giant-impact theory explains many lines of evidence, some questions are still unresolved, most of which involve the Moon's composition. Models that have the Moon acquiring a significant amount of the proto-Earth are more difficult to reconcile with geochemical data for the isotopes of zirconium, oxygen, silicon, and other elements. A study published in 2022, using high-resolution simulations (up to 108 particles), found that giant impacts can immediately place a satellite with similar mass and iron content to the Moon into orbit far outside Earth's Roche limit. Even satellites that initially pass within the Roche limit can reliably and predictably survive, by being partially stripped and then torqued onto wider, stable orbits. On November 1, 2023, scientists reported that, according to computer simulations, remnants of Theia could still be present inside the Earth.",0,Wikipedia,Moon,https://en.wikipedia.org/wiki/Moon,,Moon,wikipedia_api
human_wiki_0469,"Natural development The newly formed Moon settled into a much closer Earth orbit than it has today. Each body therefore appeared much larger in the sky of the other, eclipses were more frequent, and tidal effects were stronger. Due to tidal acceleration, the Moon's orbit around Earth has become significantly larger, with a longer period. Following formation, the Moon has cooled and most of its atmosphere has been stripped. The lunar surface has since been shaped by large impact events and many small ones, forming a landscape featuring craters of all ages. The Moon was volcanically active until 1.2 billion years ago, which laid down the prominent lunar maria. Most of the mare basalts erupted during the Imbrian period, 3.3–3.7 billion years ago, though some are as young as 1.2 billion years and some as old as 4.2 billion years. The distribution of the mare basalts is uneven, with the basalts predominantly appearing on the Moon's near-side hemisphere. The reasons for this are not yet known, although the relative thinness of the crust on the near side of the Moon is hypothesized to be a factor. Causes of the distribution of the lunar highlands on the far side are also not well understood. Topological measurements show the crust on the near side to be thinner than on the far side. One possible explanation for this is that large impacts on the near side may have made it easier for lava to flow onto the surface.",0,Wikipedia,Moon,https://en.wikipedia.org/wiki/Moon,,Moon,wikipedia_api
human_wiki_0470,"Lunar geologic timescale The lunar geological periods are named after their characteristic features, from most impact craters outside the dark mare, to the mare and later craters, and finally the young, still bright and therefore readily visible craters with ray systems like Copernicus or Tycho.",0,Wikipedia,Moon,https://en.wikipedia.org/wiki/Moon,,Moon,wikipedia_api
human_wiki_0471,"Pollution is the introduction of contaminants into the natural environment that cause harm. Pollution can take the form of any substance (solid, liquid, or gas) or energy (such as radioactivity, heat, sound, or light). Pollutants, the components of pollution, can be either foreign substances/energies or naturally occurring contaminants. Although environmental pollution can be caused by natural events, the word pollution generally implies that the contaminants have a human source, such as manufacturing, extractive industries, poor waste management, transportation or agriculture. Pollution is often classed as point source (coming from a highly concentrated specific site, such as a factory, mine, construction site), or nonpoint source pollution (coming from widespread distributed sources, such as microplastics or agricultural runoff). Many sources of pollution were unregulated parts of industrialization during the 19th and 20th centuries until the emergence of environmental regulation and pollution policy in the later half of the 20th century. Sites where historically polluting industries released persistent pollutants may have legacy pollution long after the source of the pollution is stopped. Major forms of pollution include air pollution, water pollution, litter, noise pollution, plastic pollution, soil contamination, radioactive contamination, thermal pollution, light pollution, and visual pollution. Pollution has widespread consequences on human and environmental health, having systematic impact on social and economic systems. In 2019, pollution killed approximately nine million people worldwide (about one in six deaths that year); about three-quarters of these deaths were caused by air pollution. A 2022 literature review found that levels of anthropogenic chemical pollution have exceeded planetary boundaries and now threaten entire ecosystems around the world. Pollutants frequently have outsized impacts on vulnerable populations, such as children and the elderly, and marginalized communities, because polluting industries and toxic waste sites tend to be collocated with populations with less economic and political power. This outsized impact is a core reason for the formation of the environmental justice movement, and continues to be a core element of environmental conflicts, particularly in the Global South. Because of the impacts of these chemicals, local, country-level, and international policy have increasingly sought to regulate pollutants, resulting in increasing air and water quality standards, alongside regulation of specific waste streams. Regional and national policy is typically supervised by environmental agencies or ministries, while international efforts are coordinated by the UN Environmental Program and other treaty bodies. Pollution mitigation is an important part of all of the Sustainable Development Goals.",0,Wikipedia,Pollution,https://en.wikipedia.org/wiki/Pollution,,Pollution,wikipedia_api
human_wiki_0472,"Definitions and types The term ""pollution"" in the modern environmental sense was rare before the 1860s. The old sense referred to the desecration of something sacred. According to Adam Rome: To describe what we now call air pollution–i.e., the gaseous, chemical, and metallic by-products of combustion and industrial processes–people usually talked of ""the smoke nuisance."" There were several variations of that term–""the smoke problem,"" ""the smoke evil,"" even ""the smoke plague.""  Various definitions of pollution exist, which may or may not recognize certain types, such as noise pollution or greenhouse gases. The United States Environmental Protection Agency defines pollution as ""Any substances in water, soil, or air that degrade the natural quality of the environment, offend the senses of sight, taste, or smell, or cause a health hazard. The usefulness of the natural resource is usually impaired by the presence of pollutants and contaminants."" In contrast, the United Nations considers pollution to be the ""presence of substances and heat in environmental media (air, water, land) whose nature, location, or quantity produces undesirable environmental effects."" The major forms of pollution are listed below along with the particular contaminants relevant to each of them:",0,Wikipedia,Pollution,https://en.wikipedia.org/wiki/Pollution,,Pollution,wikipedia_api
human_wiki_0473,"Air pollution: the release of chemicals and particulates into the atmosphere. Common gaseous pollutants include carbon monoxide, sulfur dioxide, chlorofluorocarbons (CFCs) and nitrogen oxides produced by industry and motor vehicles. Photochemical ozone and smog are created as nitrogen oxides and hydrocarbons react to sunlight. Particulate matter, or fine dust is characterized by their micrometre size PM10 to PM2.5. Chemical pollution: the introduction of novel entities (NEs) in the environment is one of the planetary boundaries. In August 2022, scientists concluded that the (overall transgressed) boundary is a placeholder for multiple different boundaries for NEs that may emerge, reporting that per- and polyfluoroalkyl substances (PFAS) pollution, informally referred to as ""forever chemicals"", is one such new boundary. Electromagnetic pollution: the overabundance of electromagnetic radiation in their non-ionizing form, such as radio and television transmissions, Wi-fi etc. Although there is no demonstrable effect on humans there can be interference with radio-astronomy and effects on safety systems of aircraft and cars. Light pollution: includes light trespass, over-illumination and astronomical interference. Littering: the criminal throwing of inappropriate man-made objects, unremoved, onto public and private properties. Noise pollution: which encompasses roadway noise, aircraft noise, industrial noise as well as high-intensity sonar. Plastic pollution: involves the accumulation of plastic products and microplastics in the environment that adversely affects wildlife, wildlife habitat, or humans. Soil contamination occurs when chemicals are released by spill or underground leakage. Among the most significant soil contaminants are hydrocarbons, heavy metals, MTBE, herbicides, pesticides and chlorinated hydrocarbons. Radioactive contamination, resulting from 20th century activities in atomic physics, such as nuclear power generation and nuclear weapons research, manufacture and deployment. (See alpha emitters and actinides in the environment.) Thermal pollution, is a temperature change in natural water bodies caused by human influence, such as use of water as coolant in a power plant. Visual pollution, which can refer to the presence of overhead power lines, motorway billboards, scarred landforms (as from strip mining), open storage of trash, municipal solid waste or space debris. Water pollution, caused by the discharge of industrial wastewater from commercial and industrial waste (intentionally or through spills) into surface waters; discharges of untreated sewage and chemical contaminants, such as chlorine, from treated sewage; and releases of waste and contaminants into surface runoff flowing to surface waters (including urban runoff and agricultural runoff, which may contain chemical fertilizers and pesticides, as well as human feces from open defecation).",0,Wikipedia,Pollution,https://en.wikipedia.org/wiki/Pollution,,Pollution,wikipedia_api
human_wiki_0474,"Natural causes One of the most significant natural sources of pollution are volcanoes, which during eruptions release large quantities of harmful gases into the atmosphere. Volcanic gases include carbon dioxide, which can be fatal in large concentrations and contributes to climate change, hydrogen halides which can cause acid rain, sulfur dioxide, which is harmful to animals and damages the ozone layer, and hydrogen sulfide, which is capable of killing humans at concentrations of less than 1 part per thousand. Volcanic emissions also include fine and ultrafine particles which may contain toxic chemicals and substances such as arsenic, lead, and mercury. Wildfires, which can be caused naturally by lightning strikes, are also a significant source of air pollution. Wildfire smoke contains significant quantities of both carbon dioxide and carbon monoxide, which can cause suffocation. Large quantities of fine particulates are found within wildfire smoke as well, which pose a health risk to animals.",0,Wikipedia,Pollution,https://en.wikipedia.org/wiki/Pollution,,Pollution,wikipedia_api
human_wiki_0475,"Human generation Motor vehicle emissions are one of the leading causes of air pollution. China, United States, Russia, India, Mexico, and Japan are the world leaders in air pollution emissions. Principal stationary pollution sources include chemical plants, coal-fired power plants, oil refineries, petrochemical plants, nuclear waste disposal activity, incinerators, large livestock farms (dairy cows, pigs, poultry, etc.), PVC factories, metals production factories, plastics factories, and other heavy industry. Agricultural air pollution comes from contemporary practices which include clear felling and burning of natural vegetation as well as spraying of pesticides and herbicides. About 400 million metric tons of hazardous wastes are generated each year. The United States alone produces about 250 million metric tons. Americans constitute less than 5% of the world's population, but produce roughly 25% of the world's CO2, and generate approximately 30% of world's waste. In 2007, China overtook the United States as the world's biggest producer of CO2, while still far behind based on per capita pollution (ranked 78th among the world's nations).",0,Wikipedia,Pollution,https://en.wikipedia.org/wiki/Pollution,,Pollution,wikipedia_api
human_wiki_0476,"Program (American English; also Commonwealth English in terms of computer programming and related activities) or programme (Commonwealth English in all other meanings), programmer, or programming may refer to:",0,Wikipedia,Programming,https://en.wikipedia.org/wiki/Program,,Programming,wikipedia_api
human_wiki_0477,"Business and management Program (management), group of several related projects managed together Program management, the process of managing several related projects Time management Program, a part of planning",0,Wikipedia,Programming,https://en.wikipedia.org/wiki/Program,,Programming,wikipedia_api
human_wiki_0478,"Video or television Broadcast programming, scheduling content for television Program music, a type of art music that attempts to render musically an extra-musical narrative Synthesizer patch or program, a synthesizer setting stored in memory ""Program"", an instrumental song by Linkin Park from LP Underground Eleven Programmer, a film on the lower half of a double feature bill; see B-movie",0,Wikipedia,Programming,https://en.wikipedia.org/wiki/Program,,Programming,wikipedia_api
human_wiki_0479,"Science and technology Computer program, a set of instructions that describes to a computer how to perform a specific task Computer programming, the act of instructing computers to perform tasks Programming language, an artificial language designed to communicate instructions to a machine Game programming, the software development of video games Mathematical programming, or optimization, is the selection of a best element Programmer, a person who writes software Programmer (hardware), a physical device that configures electronic circuits Program (machine), a technical setting stored in the memory of a machine or piece of hardware to be executed, including computers Research program, a professional network of scientists conducting basic research Software engineer, someone who participates in a software development process",0,Wikipedia,Programming,https://en.wikipedia.org/wiki/Program,,Programming,wikipedia_api
human_wiki_0480,"See also Application software Deprogramming Dramatic programming, fictional television content Neuro-linguistic programming, a pseudoscientific method aimed at modifying human behavior Twelve-step program, a set of guiding principles for recovery from addiction, compulsion, or other behavioral problems The Program (disambiguation)",0,Wikipedia,Programming,https://en.wikipedia.org/wiki/Program,,Programming,wikipedia_api
human_wiki_0481,"Automation describes a wide range of technologies that reduce human intervention in processes, mainly by predetermining decision criteria, subprocess relationships, and related actions, as well as embodying those predeterminations in machines. Automation has been achieved by various means including mechanical, hydraulic, pneumatic, electrical, electronic devices, and computers, usually in combination. Complicated systems, such as modern factories, airplanes, and ships typically use combinations of all of these techniques. The benefits of automation includes labor savings, reducing waste, savings in electricity costs, savings in material costs, and improvements to quality, accuracy, and precision. Automation includes the use of various equipment and control systems such as machinery, processes in factories, boilers, and heat-treating ovens, switching on telephone networks, steering, stabilization of ships, aircraft and other applications and vehicles with reduced human intervention. Examples range from a household thermostat controlling a boiler to a large industrial control system with tens of thousands of input measurements and output control signals. Automation has also found a home in the banking industry. It can range from simple on-off control to multi-variable high-level algorithms in terms of control complexity. In the simplest type of an automatic control loop, a controller compares a measured value of a process with a desired set value and processes the resulting error signal to change some input to the process, in such a way that the process stays at its set point despite disturbances. This closed-loop control is an application of negative feedback to a system. The mathematical basis of control theory began in the 18th century and advanced rapidly in the 20th. The term automation, inspired by the earlier word automatic (coming from automaton), was not widely used before 1947, when Ford established an automation department. It was during this time that the industry was rapidly adopting feedback controllers, Technological advancements introduced in the 1930s revolutionized various industries significantly. The World Bank's World Development Report of 2019 shows evidence that the new industries and jobs in the technology sector outweigh the economic effects of workers being displaced by automation. Job losses and downward mobility blamed on automation have been cited as one of many factors in the resurgence of nationalist, protectionist and populist politics in the US, UK and France, among other countries since the 2010s.",0,Wikipedia,Automation,https://en.wikipedia.org/wiki/Automation,,Automation,wikipedia_api
human_wiki_0482,"History Early history It was a preoccupation of the Greeks and Arabs (in the period between about 300 BC and about 1200 AD) to keep an accurate track of time. In Ptolemaic Egypt, about 270 BC, Ctesibius described a float regulator for a water clock, a device not unlike the ball and cock in a modern flush toilet. This was the earliest feedback-controlled mechanism. The appearance of the mechanical clock in the 14th century made the water clock and its feedback control system obsolete. The Persian Banū Mūsā brothers, in their Book of Ingenious Devices (850 AD), described a number of automatic controls. Two-step level controls for fluids, a form of discontinuous variable structure controls, were developed by the Banu Musa brothers. They also described a feedback controller. The design of feedback control systems up through the Industrial Revolution was by trial-and-error, together with a great deal of engineering intuition. It was not until the mid-19th century that the stability of feedback control systems was analyzed using mathematics, the formal language of automatic control theory. The centrifugal governor was invented by Christiaan Huygens in the seventeenth century, and used to adjust the gap between millstones.",0,Wikipedia,Automation,https://en.wikipedia.org/wiki/Automation,,Automation,wikipedia_api
human_wiki_0483,"Industrial Revolution in Western Europe The introduction of prime movers, or self-driven machines advanced grain mills, furnaces, boilers, and the steam engine created a new requirement for automatic control systems including temperature regulators (invented in 1624; see Cornelius Drebbel), pressure regulators (1681), float regulators (1700) and speed control devices. Another control mechanism was used to tent the sails of windmills. It was patented by Edmund Lee in 1745. Also in 1745, Jacques de Vaucanson invented the first automated loom. Around 1800, Joseph Marie Jacquard created a punch-card system to program looms. In 1771 Richard Arkwright invented the first fully automated spinning mill driven by water power, known at the time as the water frame. An automatic flour mill was developed by Oliver Evans in 1785, making it the first completely automated industrial process.",0,Wikipedia,Automation,https://en.wikipedia.org/wiki/Automation,,Automation,wikipedia_api
human_wiki_0484,"A centrifugal governor was used by Mr. Bunce of England in 1784 as part of a model steam crane. The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one at a flour mill Boulton & Watt were building. The governor could not actually hold a set speed; the engine would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped with this governor were not suitable for operations requiring constant speed, such as cotton spinning. Several improvements to the governor, plus improvements to valve cut-off timing on the steam engine, made the engine suitable for most industrial uses before the end of the 19th century. Advances in the steam engine stayed well ahead of science, both thermodynamics and control theory. The governor received relatively little scientific attention until James Clerk Maxwell published a paper that established the beginning of a theoretical basis for understanding control theory.",0,Wikipedia,Automation,https://en.wikipedia.org/wiki/Automation,,Automation,wikipedia_api
human_wiki_0485,"20th century Relay logic was introduced with factory electrification, which underwent rapid adaptation from 1900 through the 1920s. Central electric power stations were also undergoing rapid growth and the operation of new high-pressure boilers, steam turbines and electrical substations created a great demand for instruments and controls. Central control rooms became common in the 1920s, but as late as the early 1930s, most process controls were on-off. Operators typically monitored charts drawn by recorders that plotted data from instruments. To make corrections, operators manually opened or closed valves or turned switches on or off. Control rooms also used color-coded lights to send signals to workers in the plant to manually make certain changes. The development of the electronic amplifier during the 1920s, which was important for long-distance telephony, required a higher signal-to-noise ratio, which was solved by negative feedback noise cancellation. This and other telephony applications contributed to the control theory. In the 1940s and 1950s, German mathematician Irmgard Flügge-Lotz developed the theory of discontinuous automatic controls, which found military applications during the Second World War to fire control systems and aircraft navigation systems. Controllers, which were able to make calculated changes in response to deviations from a set point rather than on-off control, began being introduced in the 1930s. Controllers allowed manufacturing to continue showing productivity gains to offset the declining influence of factory electrification. Factory productivity was greatly increased by electrification in the 1920s. U.S. manufacturing productivity growth fell from 5.2%/yr 1919–29 to 2.76%/yr 1929–41. Alexander Field notes that spending on non-medical instruments increased significantly from 1929 to 1933 and remained strong thereafter. The First and Second World Wars saw major advancements in the field of mass communication and signal processing. Other key advances in automatic controls include differential equations, stability theory and system theory (1938), frequency domain analysis (1940), ship control (1950), and stochastic analysis (1941). Starting in 1958, various systems based on solid-state digital logic modules for hard-wired programmed logic controllers (the predecessors of programmable logic controllers [PLC]) emerged to replace electro-mechanical relay logic in industrial control systems for process control and automation, including early Telefunken/AEG Logistat, Siemens Simatic, Philips/Mullard/Valvo Norbit, BBC Sigmatronic, ACEC Logacec, Akkord Estacord, Krone Mibakron, Bistat, Datapac, Norlog, SSR, or Procontic systems. In 1959 Texaco's Port Arthur Refinery became the first chemical plant to use digital control. Conversion of factories to digital control began to spread rapidly in the 1970s as the price of computer hardware fell.",0,Wikipedia,Automation,https://en.wikipedia.org/wiki/Automation,,Automation,wikipedia_api
human_wiki_0486,"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework.",0,Wikipedia,Machine learning,https://en.wikipedia.org/wiki/Machine_learning,,Machine_learning,wikipedia_api
human_wiki_0487,"History The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period. The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental ""learning machine"" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively ""trained"" by a human operator/teacher to recognise patterns and equipped with a ""goof"" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question, ""Can machines think?"", is replaced with the question, ""Can machines do what we (as thinking entities) can do?"". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.",0,Wikipedia,Machine learning,https://en.wikipedia.org/wiki/Machine_learning,,Machine_learning,wikipedia_api
human_wiki_0488,"Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule. Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques.",0,Wikipedia,Machine learning,https://en.wikipedia.org/wiki/Machine_learning,,Machine_learning,wikipedia_api
human_wiki_0489,"Relationships to other fields Artificial intelligence As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.",0,Wikipedia,Machine learning,https://en.wikipedia.org/wiki/Machine_learning,,Machine_learning,wikipedia_api
human_wiki_0490,"Data compression Data mining Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).",0,Wikipedia,Machine learning,https://en.wikipedia.org/wiki/Machine_learning,,Machine_learning,wikipedia_api
human_wiki_0491,"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, solar thermal energy (including solar water heating) and solar architecture. It is an essential source of renewable energy, and its technologies are broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power, and solar water heating to harness the energy. Passive solar techniques include designing a building for better daylighting, selecting materials with favorable thermal mass or light-dispersing properties, and organizing spaces that naturally circulate air. In 2011, the International Energy Agency said that ""the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible, and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming .... these advantages are global"".",0,Wikipedia,Solar energy,https://en.wikipedia.org/wiki/Solar_energy,,Solar_energy,wikipedia_api
human_wiki_0492,"Potential The Earth receives 174 petawatts (PW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest, 122 PW, is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most of the world's population live in areas with insolation levels of 150–300 watts/m2, or 3.5–7.0 kWh/m2 per day. Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anticyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis, green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived. The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 122 PW·year = 3,850,000 exajoules (EJ) per year. In 2002 (2019), this was more energy in one hour (one hour and 25 minutes) than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass.",0,Wikipedia,Solar energy,https://en.wikipedia.org/wiki/Solar_energy,,Solar_energy,wikipedia_api
human_wiki_0493,"The potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limit the amount of solar energy that we can acquire. In 2021, Carbon Tracker Initiative estimated the land area needed to generate all our energy from solar alone was 450,000 km2 — or about the same as the area of Sweden, or the area of Morocco, or the area of California (0.3% of the Earth's total land area). Solar technologies are categorized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on the distance from the Equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all types of renewable energy, other than geothermal power and tidal power, are derived either directly or indirectly from the Sun. Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful output. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternative resources and are generally considered demand-side technologies. In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year. This took into account factors such as insolation, cloud cover, and the land that is usable by humans. It was stated that solar energy has a global potential of 1,600 to 49,800 exajoules (4.4×1014 to 1.4×1016 kWh) per year (see table below).",0,Wikipedia,Solar energy,https://en.wikipedia.org/wiki/Solar_energy,,Solar_energy,wikipedia_api
human_wiki_0494,"Early commercial adaptation In 1878, at the Universal Exposition in Paris, Augustin Mouchot successfully demonstrated a solar steam engine but could not continue development because of cheap coal and other factors.",0,Wikipedia,Solar energy,https://en.wikipedia.org/wiki/Solar_energy,,Solar_energy,wikipedia_api
human_wiki_0495,"In 1897, Frank Shuman, a US inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912. Shuman built the world's first solar thermal power station in Maadi, Egypt, between 1912 and 1913. His plant used parabolic troughs to power a 45–52 kilowatts (60–70 hp) engine that pumped more than 22,000 litres (4,800 imp gal; 5,800 US gal) of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman's vision, and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:",0,Wikipedia,Solar energy,https://en.wikipedia.org/wiki/Solar_energy,,Solar_energy,wikipedia_api
human_wiki_0496,"A fossil fuel is a flammable carbon compound- or hydrocarbon-containing material formed naturally in the Earth's crust from the buried remains of prehistoric organisms (animals, plants or microplanktons), a process that occurs within geological formations. Reservoirs of such compound mixtures, such as coal, petroleum and natural gas, can be extracted and burnt as fuel for human consumption to provide energy for direct use (such as for cooking, heating or lighting), to power heat engines (such as steam or internal combustion engines) that can propel vehicles, or to generate electricity via steam turbine generators. Some fossil fuels are further refined into derivatives such as kerosene, gasoline and diesel, or converted into petrochemicals such as polyolefins (plastics), aromatics and synthetic resins. The origin of fossil fuels is the anaerobic decomposition of buried dead organisms. The conversion from these organic materials to high-carbon fossil fuels is typically the result of a geological process of millions of years. Due to the length of time it takes for them to form, fossil fuels are considered non-renewable resources. In 2023, 77% of primary energy consumption in the world and over 60% of its electricity supply were from fossil fuels. The large-scale burning of fossil fuels causes serious environmental damage. Over 70% of the greenhouse gas emissions due to human activity in 2022 was carbon dioxide (CO2) released from burning fossil fuels. Natural carbon cycle processes on Earth, mostly absorption by the ocean, can remove only a small part of this, and terrestrial vegetation loss due to deforestation, land degradation and desertification further compounds this deficiency. Therefore, there is a net increase of many billion tonnes of atmospheric CO2 per year. Although methane leaks are significant, the burning of fossil fuels is the main source of greenhouse gas emissions causing global warming and ocean acidification. Additionally, most air pollution deaths are due to fossil fuel particulates and noxious gases, and it is estimated that this costs over 3% of the global gross domestic product and that fossil fuel phase-out will save millions of lives each year. Recognition of the climate crisis, pollution and other negative effects caused by fossil fuels has led to a widespread policy transition and activist movement focused on ending their use in favor of renewable and sustainable energy. Because the fossil-fuel industry is so heavily integrated in the global economy and heavily subsidized, this transition is expected to have significant economic consequences. Many stakeholders argue that this change needs to be a just transition and create policy that addresses the societal burdens created by the stranded assets of the fossil fuel industry. International policy, in the form of United Nations' sustainable development goals for affordable and clean energy and climate action, as well as the Paris Climate Agreement, is designed to facilitate this transition at a global level. In 2021, the International Energy Agency concluded that no new fossil fuel extraction projects could be opened if the global economy and society wants to avoid the worst effects of climate change and meet international goals for climate change mitigation.",0,Wikipedia,Fossil fuel,https://en.wikipedia.org/wiki/Fossil_fuel,,Fossil_fuel,wikipedia_api
human_wiki_0497,"Origin The theory that fossil fuels formed from the fossilized remains of dead plants by exposure to heat and pressure in Earth's crust over millions of years was first introduced by Andreas Libavius ""in his 1597 Alchemia [Alchymia]"" and later by Mikhail Lomonosov ""as early as 1757 and certainly by 1763"". The first recorded use of the term ""fossil fuel"" occurs in the work of the German chemist Caspar Neumann, in English translation in 1759. The Oxford English Dictionary notes that, in the phrase ""fossil fuel"", the adjective ""fossil"" means ""[o]btained by digging; found buried in the earth"", which dates to at least 1652, before the English noun ""fossil"" came to refer primarily to long-dead organisms in the early 18th century. Aquatic phytoplankton and zooplankton that died and sedimented in large quantities under anoxic conditions millions of years ago began forming petroleum and natural gas as a result of anaerobic decomposition. Over geological time this organic matter, mixed with mud, became buried under further heavy layers of inorganic sediment. The resulting high temperature and pressure caused the organic matter to chemically alter, first into a waxy material known as kerogen, which is found in oil shales, and then with more heat into liquid and gaseous hydrocarbons in a process known as catagenesis. Despite these heat-driven transformations, the energy released in combustion is still photosynthetic in origin. Terrestrial plants tend to form coal and methane. Many of the coal fields date to the Carboniferous period of Earth's history. Terrestrial plants also form type III kerogen, a source of natural gas. Although fossil fuels are continually formed by natural processes, they are classified as non-renewable resources because they take millions of years to form and known viable reserves are being depleted much faster than new ones are generated.",0,Wikipedia,Fossil fuel,https://en.wikipedia.org/wiki/Fossil_fuel,,Fossil_fuel,wikipedia_api
human_wiki_0498,"Importance Fossil fuels have been important to human development because they can be readily burned in the open atmosphere to produce heat. The use of peat as a domestic fuel predates recorded history. Coal was burned in some early furnaces for the smelting of metal ore, while semi-solid hydrocarbons from oil seeps were also burned in ancient times, they were mostly used for waterproofing and embalming. Commercial exploitation of petroleum began in the 19th century. Natural gas, once flared-off as an unneeded byproduct of petroleum production, is now considered a very valuable resource. Natural gas deposits are also the main source of helium. Heavy crude oil, which is much more viscous than conventional crude oil, and oil sands, where bitumen is found mixed with sand and clay, began to become more important as sources of fossil fuel in the early 2000s. Oil shale and similar materials are sedimentary rocks containing kerogen, a complex mixture of high-molecular weight organic compounds, which yield synthetic crude oil when heated (pyrolyzed). With additional processing, they can be employed instead of other established fossil fuels. During the 2010s and 2020s there was disinvestment from exploitation of such resources due to their high carbon cost relative to more easily-processed reserves. Prior to the latter half of the 18th century, windmills and watermills provided the energy needed for work such as milling flour, sawing wood or pumping water, while burning wood or peat provided domestic heat. The wide-scale use of fossil fuels, coal at first and petroleum later, in steam engines enabled the Industrial Revolution. At the same time, gas lights using natural gas or coal gas were coming into wide use. The invention of the internal combustion engine and its use in automobiles and trucks greatly increased the demand for gasoline and diesel oil, both made from fossil fuels. Other forms of transportation, railways and aircraft, also require fossil fuels. The other major use for fossil fuels is in generating electricity and as feedstock for the petrochemical industry. Tar, a leftover of petroleum extraction, is used in the construction of roads. The energy for the Green Revolution was provided by fossil fuels in the form of fertilizers (natural gas), pesticides (oil), and hydrocarbon-fueled irrigation. The development of synthetic nitrogen fertilizer has significantly supported global population growth; it has been estimated that almost half of the Earth's population are currently fed as a result of synthetic nitrogen fertilizer use. According to head of a fertilizers commodity price agency, ""50% of the world's food relies on fertilisers.""",0,Wikipedia,Fossil fuel,https://en.wikipedia.org/wiki/Fossil_fuel,,Fossil_fuel,wikipedia_api
human_wiki_0499,"Environmental effects The burning of fossil fuels has a number of negative externalities – harmful environmental consequences where the effects extend beyond the people using the fuel. These effects vary between different fuels. All fossil fuels release CO2 when they burn, thus accelerating climate change. Burning coal, and to a lesser extent oil and its derivatives, contributes to atmospheric particulate matter, smog and acid rain. Air pollution from fossil fuels in 2018 has been estimated to cost US$2.9 trillion, or 3.3% of the global gross domestic product (GDP).",0,Wikipedia,Fossil fuel,https://en.wikipedia.org/wiki/Fossil_fuel,,Fossil_fuel,wikipedia_api
